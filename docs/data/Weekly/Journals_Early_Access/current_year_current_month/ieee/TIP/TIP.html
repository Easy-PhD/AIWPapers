<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip">TIP - 31</h2>
<ul>
<li><details>
<summary>
(2025). Zero-shot image recognition via learning dual prototype accordance across meta-domains. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize unseen classes by transferring semantic knowledge from seen categories. However, existing methods often struggle with the persistent semantic gap caused by limited semantic descriptors and rigid visual feature modeling. In particular, modeling pre-defined class-level attribute descriptions as ground truth hinders effective semantic-to-visual alignment to some extent. To mitigate these issues, we propose the Bilateral-guided Prototype Refinement Network(BPRN), a novel ZSL framework designed to refine dual prototypes across meta-domains of varying scales. Specifically, we first disentangle the relationships among class-level semantics and use them to generate corresponding pseudo-visual prototypes. Then, by leveraging distribution information across dual prototypes in different meta-domains, BPRN achieves bidirectional calibration between visual-to-semantic and semantic-to-visual modalities. Finally, a synthesized class-level representation derived from the refined dual prototypes is employed for inference, instead of relying on a single prototype. Extensive experiments conducted on five widely-used ZSL benchmark datasets demonstrate that BPRN consistently achieves competitive or even superior performance. Specifically, in the GZSL scenario, BPRN shows improvements of 2.1%, 7.3%, 6.1%, and 4.8% on AWA1, AWA2, SUN, and aPY, respectively, compared to existing embedding-based ZSL methods. Ablation studies and visualization analyses further validate the effectiveness of the proposed components.},
  archive      = {J_TIP},
  author       = {Bocheng Ren and Yuanyuan Yi and Qingchen Zhang and Debin Liu},
  doi          = {10.1109/TIP.2025.3607588},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Zero-shot image recognition via learning dual prototype accordance across meta-domains},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uni-ISP: Towards unifying the learning of ISPs from multiple mobile cameras. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607617'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern end-to-end image signal processors (ISPs) can learn complex mappings from RAW/XYZ data to sRGB (and vice versa), opening new possibilities in image processing. However, the growing diversity of camera models, particularly in mobile devices, renders the development of individual ISPs unsustainable due to their limited versatility and adaptability across varied camera systems. In this paper, we introduce Uni-ISP, a novel pipeline that unifies ISP learning for diverse mobile cameras, delivering a highly accurate and adaptable processor. The core of Uni-ISP is leveraging device-aware embeddings through learning forward/inverse ISPs and its special training scheme. By doing so, Uni-ISP not only improves the performance of forward and inverse ISPs but also unlocks new applications previously inaccessible to conventional learned ISPs. To support this work, we construct a real-world 4K dataset, FiveCam, comprising more than 2,400 pairs of sRGB-RAW images synchronously captured by five smartphone cameras. Extensive experiments validate Uni-ISP’s accuracy in learning forward and inverse ISPs (with improvements of +2.4dB/1.5dB PSNR), versatility in enabling new applications, and adaptability to new camera models.},
  archive      = {J_TIP},
  author       = {Lingen Li and Mingde Yao and Xingyu Meng and Muquan Yu and Tianfan Xue and Jinwei Gu},
  doi          = {10.1109/TIP.2025.3607617},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uni-ISP: Towards unifying the learning of ISPs from multiple mobile cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient open vocabulary 3D scene understanding based on semantic probability. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional 3D scene understanding methods heavily depend on 3D annotation and training, which allow for the identification of seen classes but struggle to recognize unseen classes. In this paper, we leverage the open vocabulary inference capabilities of pre-trained models, enabling the encoding of open vocabulary concepts. However, unlike existing open vocabulary 3D scene understanding methods, we propose a framework based on semantic probability. This innovation significantly reduces computational cost and is compatible with state-of-the-art two-stage 2D pre-trained models. Specifically, we align the text features from the CLIP model with the pixel features from the 2D pre-trained models, inferring semantic probability of image pixels based on similarity and projecting it onto 3D points. Subsequently, we introduce a point cloud pairs semantic fusion method to merge the point clouds, reducing the semantic probability of erroneous 3D points. Based on probability scores, we achieve 3D semantic segmentation on open vocabularies without any supervision or training. In addition, the semantic probability of 3D points can serve as pseudo-labels for 3D distillation, and the geometric features of the 3D scene can be exploited to improve the segmentation performance. Experimental results demonstrate that the proposed method exhibits competitive performance on publicly available benchmark datasets, including ScanNet, Matterport3D, and nuScenes.},
  archive      = {J_TIP},
  author       = {Lingfeng Shen and Xiaoyao Wei and Gang Pan and Qian Zheng and Yanlong Cao},
  doi          = {10.1109/TIP.2025.3607643},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cost-efficient open vocabulary 3D scene understanding based on semantic probability},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive anchor-guided representation learning for efficient multi-view subspace clustering. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view Subspace Clustering (MVSC) effectively aggregating multiple data sources to promise clustering performance. Recently, various anchor-based variants have been introduced to effectively alleviate the computation complexity of MVSC. Although satisfactory advancement has been achieved, existing methods either independently learn anchor matrices and their anchor representations or learn a consensus anchor matrix and unified anchor representation, failing to capture both consistency and complementary information simultaneously. In addition, the time complexity of obtaining clustering results by applying Singular Value Decomposition (SVD) on the anchor representation matrix remains high. To tackle the above problems, we propose an Adaptive Anchor-guided Representation Learning for Efficient Multi-view Subspace Clustering (A2RL-EMVSC) framework, which integrates consensus anchors learning, anchor-guided representation learning and matrix factorization to enhance clustering performance and scalability. Technically, the proposed method learns view-specific anchor representation matrices by consensus anchors guidance, which simultaneously exploit consistency and complementary information. Moreover, by applying matrix decomposition to the view-specific anchor representation matrices, clustering results can be achieved with linear time complexity. Extensive experiments on ten challenging multi-view datasets show that the proposed method can improve the effectiveness and superiority of clustering compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Mengjiao Zhang and Xinwang Liu and Tianhao Han and Xiaofeng Qu and Sijie Niu},
  doi          = {10.1109/TIP.2025.3607587},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive anchor-guided representation learning for efficient multi-view subspace clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-category anomaly editing network with correlation exploration and voxel-level attention for unsupervised surface anomaly detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing a unified model for surface anomaly detection remains challenging due to significant variations across product categories. Recent feature editing methods, as a branch of image reconstruction, mitigate the over-generalization of auto-encoders that leads to accurate anomaly reconstruction. However, these methods are only suited for texture-category products and have significant limitations in being generalized to other categories. In this article, we propose a multi-category anomaly editing network with a dual-branch training approach: one branch processes defect-free images (normal branch), while the other handles synthetic anomaly images (anomaly branch). Specifically, the paired samples are first fed into the multi-category anomaly feature editing based auto-encoder (MCAFE-AE) to perform image reconstruction and inpainting. In the normal branch, we propose a dual-entropy constrained deep embedded clustering module (DEC-DECM) to promote a more compact and orderly distribution of normal latent features, while avoiding trivial clustering solutions. Based on the clustering results, we further design a patch-based adaptive thresholding (PAT) strategy to adaptively calculate the threshold representing the central boundary of the cluster center for each local patch, thereby enabling the model to detect anomalies. Then, in the anomaly branch, we propose a multi-category anomaly feature editing module (MCAFEM) to identify anomalies in synthetic images and apply a category-oriented feature editing strategy to transform detected anomaly features into normal ones, thereby suppressing the reconstruction of anomalies. After completing the image reconstruction and inpainting, the input images from both branches and their respective output images are concatenated and fed into the correlation exploration and voxel-level attention based prediction network (CEVA-Net) for anomaly segmentation. The network is integrated with our proposed correlation-dependency exploration and voxel-level attention refinement module (CDE-VARM) and generates precise anomaly maps under the guidance of the bidirectional-path feature fusion (BPFF) and deep supervised learning (DSL). Extensive experiments on three datasets show that our method achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Ruifan Zhang and Hai-Miao Hu},
  doi          = {10.1109/TIP.2025.3607638},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multi-category anomaly editing network with correlation exploration and voxel-level attention for unsupervised surface anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMCFuse: A unified multiple complex scenes infrared and visible image fusion framework. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion has emerged as a prominent research area in computer vision. However, little attention has been paid to complex scenes fusion, leading to sub-optimal results under interference. To fill this gap, we propose a unified framework for infrared and visible images fusion in complex scenes, termed UMCFuse. Specifically, we classify the pixels of visible images from the degree of scattering of light transmission, allowing us to separate fine details from overall intensity. Maintaining a balance between interference removal and detail preservation is essential for the generalization capacity of the proposed method. Therefore, we propose an adaptive denoising strategy for the fusion of detail layers. Meanwhile, we fuse the energy features from different modalities by analyzing them from multiple directions. Extensive fusion experiments on real and synthetic complex scenes datasets cover adverse weather conditions, noise, blur, overexposure, fire, as well as downstream tasks including semantic segmentation, object detection, salient object detection, and depth estimation, consistently indicate the superiority of the proposed method compared with the recent representative methods. Our code is available at https://github.com/ixilai/UMCFuse.},
  archive      = {J_TIP},
  author       = {Xilai Li and Xiaosong Li and Tianshu Tan and Huafeng Li and Tao Ye},
  doi          = {10.1109/TIP.2025.3607623},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UMCFuse: A unified multiple complex scenes infrared and visible image fusion framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cycle translation-based collaborative training for hyperspectral-RGB multimodal change detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image change detection (HSI-CD) benefits from HSIs with continuous spectral bands, which uniquely enables the analysis of more subtle changes. Existing methods have achieved desirable performance relying on multi-temporal homogenous HSIs over the same region, which is generally difficult to obtain in real scenes. HSI-RGB multimodal CD overcomes the constraint of limited HSI availability by incorporating another temporal RGB data, and the combination of advantages within different modalities enhances the robustness of detection results. Nevertheless, due to the different imaging mechanisms between two modalities, existing HSI CD methods cannot be directly applied. In this paper, we propose a cycle translation-based collaborative training (co-training) for HSI-RGB multimodal CD, which achieves cross-modal mutual guidance to collaboratively learn complementary difference information from diverse modalities for identifying changes. Specifically, a cross-modal guided CycleGAN-based image translation module is designed to implement bi-directional image translation, which mitigates modal difference and enables the extraction of information related to land cover changes. Then, a spatial-spectral interactive co-training CD module is proposed to achieve iterative interaction between cross-modal information, which jointly extracts the multimodal difference features to generate the final results. The proposed method outperforms several leading CD methods in extensive experiments carried out on both real and synthetic datasets. In addition, a new public HSI-RGB multimodal dataset along with our code are available at https://github.com/Jiahuiqu/CT2Net.},
  archive      = {J_TIP},
  author       = {Wenqian Dong and Junying Ren and Song Xiao and Leyuan Fang and Jiahui Qu and Yunsong Li},
  doi          = {10.1109/TIP.2025.3607609},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cycle translation-based collaborative training for hyperspectral-RGB multimodal change detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral texture metrology based on distance measures in an information-theoretic framework. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work sought to instil metrology in existing hyperspectral texture feature extraction methods. Specifically, we propose distance-based expressions of graylevel cooccurrence matrix (GLCM), local binary pattern (LBP), and Gabor filtering directly computable for hyperspectral images without any pre- or post-processing. At the core of our proposition is Radical of Extended Mean Information for Discrimination (REID), a novel spectral distance with information-theoretic roots. Respecting the physics of spectrum as continuous function of wavelengths, REID is mathematically decomposable into spectral direction and spectral magnitude distances. The resulted feature calculations are fullband (utilizing all wavelengths), yet lightweight and fully interpretable. A similarity measure based on information theory is also justified. Their efficiency is demonstrated in the context of texture classification, content-based image retrieval, and cancer detection in which they consistently outperform existing computations based on dimensionally reduced space using PCA, ICA, and NMF. The propositions could be potentially integrated into machine/deep learning systems towards explainable AI (XAI).},
  archive      = {J_TIP},
  author       = {Rui Jian Chu and Jie Chen and Susanto Rahardja},
  doi          = {10.1109/TIP.2025.3608667},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral texture metrology based on distance measures in an information-theoretic framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient and structure consistency in multimodal emotion recognition. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608664'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal emotion recognition is a task that integrates text, visual, and audio data to holistically infer an individual’s emotional state. Existing research predominantly focuses on exploiting modality-specific cues for joint learning, often ignoring the differences between multiple modalities under common goal learning. Due to multimodal heterogeneity, common goal learning inadvertently introduces optimization biases and interaction noise. To address above challenges, we propose a novel approach named Gradient and Structure Consistency (GSCon). Our strategy operates at both overall and individual levels to consider balance optimization and effective interaction respectively. At the overall level, to avoid the optimization suppression of a modality on other modalities, we construct a balanced gradient direction that aligns each modality’s optimization direction, ensuring unbiased convergence. Simultaneously, at the individual level, to avoid the interaction noise caused by multimodal alignment, we align the spatial structure of samples in different modalities. The spatial structure of the samples will not differ due to modal heterogeneity, achieving effective inter-modal interaction. Extensive experiments on multimodal emotion recognition and multimodal intention understanding datasets demonstrate the effectiveness of the proposed method. Code is available at https://github.com/ShiQingHongYa/GSCon.},
  archive      = {J_TIP},
  author       = {QingHongYa Shi and Mang Ye and Wenke Huang and Bo Du and Xiaofen Zong},
  doi          = {10.1109/TIP.2025.3608664},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Gradient and structure consistency in multimodal emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-space topological isomorphism and maximization of predictive diversity for unsupervised domain adaptation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing unsupervised domain adaptation methods rely on explicitly or implicitly aligning the features of source and target domains to construct a domain-invariant space, often using entropy minimization to reduce uncertainty and confusion. However, this approach faces two challenges: 1) Explicit alignment reduces discriminability, while implicit alignment risks pseudo-label noise, making it hard to balance structure preservation and alignment. 2) Sole reliance on entropy minimization can lead to trivial solutions in UDA, where all samples collapse into a single class. To address these issues, we propose Dual-Space Topological Isomorphism and Maximization of Predictive Diversity (DTI-MPD). Topological isomorphism is a continuous, bijective mapping that preserves the topological properties of two spaces, ensuring the global structure and relationships of data remain intact during alignment. Our method aligns source and target domain data in two independent spaces while balancing the effects of entropy minimization through predictive diversity maximization. The core of dual-space topological isomorphism lies in establishing a reversible correspondence between the source and target domains, avoiding information loss during alignment and preserving the global structural and topological characteristics of the data. Meanwhile, predictive diversity maximization mitigates the class collapse caused by entropy minimization, ensuring a more balanced predictive distribution across categories. This approach effectively overcomes the aforementioned issues, enabling better adaptation to new data. Extensive experiments demonstrate that our method achieves state-of-the-art performance on multiple benchmark datasets, validating its effectiveness.},
  archive      = {J_TIP},
  author       = {Mengru Wang and Jinglei Liu},
  doi          = {10.1109/TIP.2025.3608670},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-space topological isomorphism and maximization of predictive diversity for unsupervised domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pull pole points to text contour by magnetism: A real-time scene text detector. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text reading plays a crucial role in scene understanding. As its precondition task, scene text detection has garnered increasing interest from researchers. Segmentation-based text detection methods have gained prominence due to their adaptable pixel-level predictions. Many existing methods predict the shrink mask and utilize the Vatti clipping algorithm to reconstruct text contours. However, the shrink mask only focuses on the global geometry feature and shrinks the same distance everywhere, which neglects local contour information and disrupts the instance shape feature. In addition, the post-processing based on the Vatti clipping algorithm heavily relies on the predictions and is relatively complex, causing suboptimal performance in both detection accuracy and efficiency. To address the above problems, we propose an efficient and effective method named Magnetic Text Detector (MTD), inspired by magnetism. It is constructed by a text representation method flexible mask (FM) and a magnetic pull module (MPM). Unlike the shrink mask and concentric mask, the former concerns the local contours and shrinks unfixed distances on different positions, which avoids the truncation issue while preserving distinctiveness from the text regions. The latter generates magnetic fields and pulls pole points of FM to the text contour by magnetism. This allows accurate reconstruction of text contours, even when predictions deviate from the actual text severely, while saving 50% of the post-processing time approximately. Several ablation studies verify the effectiveness of the proposed FM and MPM. Extensive experiments show that our MTD achieves state-of-the-art (SOTA) methods on multiple datasets from different scenes. The code is available at https://github.com/fengmulin/MTD.},
  archive      = {J_TIP},
  author       = {Xu Han and Chuang Yang and Qi Wang},
  doi          = {10.1109/TIP.2025.3609196},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pull pole points to text contour by magnetism: A real-time scene text detector},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-view alignment learning with hierarchical-prompt for class-imbalance multi-label image classification. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world datasets often exhibit class imbalance across multiple categories, manifesting as long-tailed distributions and few-shot scenarios. This is especially challenging in Class-Imbalanced Multi-Label Image Classification (CI-MLIC) tasks, where data imbalance and multi-object recognition present significant obstacles. To address these challenges, we propose a novel method termed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which leverages multi-modal knowledge from vision-language pretrained (VLP) models to mitigate the class-imbalance problem in multi-label settings. Specifically, HP-DVAL employs dual-view alignment learning to transfer the powerful feature representation capabilities from VLP models by extracting complementary features for accurate image-text alignment. To better adapt VLP models for CI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes global and local prompts to learn task-specific and context-related prior knowledge. Additionally, we design a semantic consistency loss during prompt tuning to prevent learned prompts from deviating from general knowledge embedded in VLP models. The effectiveness of our approach is validated on two CI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results demonstrate the superiority of our method over SOTA approaches, achieving mAP improvements of 10.0% and 5.2% on the long-tailed multi-label image classification task, and 6.8% and 2.9% on the multi-label few-shot image classification task.},
  archive      = {J_TIP},
  author       = {Sheng Huang and Jiexuan Yan and Beiyan Liu and Bo Liu and Richang Hong},
  doi          = {10.1109/TIP.2025.3609185},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-view alignment learning with hierarchical-prompt for class-imbalance multi-label image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NanoHTNet: Nano human topology network for efficient 3D human pose estimation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread application of 3D human pose estimation (HPE) is limited by resource-constrained edge devices like Jetson Nano, requiring more efficient models. A key approach to enhancing efficiency involves designing networks based on the structural characteristics of input data. However, effectively utilizing the structural priors in human skeletal inputs remains challenging. To address this, we leverage both explicit and implicit spatio-temporal priors of the human body through innovative model design and a pre-training proxy task. First, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE network with stacked Hierarchical Mixers to capture explicit features. Specifically, the spatial Hierarchical Mixer efficiently learns the human physical topology across multiple semantic levels, while the temporal Hierarchical Mixer with discrete cosine transform and low-pass filtering captures local instantaneous movements and global action coherence. Moreover, Efficient Temporal-Spatial Tokenization (ETST) is introduced to enhance spatio-temporal interaction and reduce computational complexity significantly. Second, PoseCLR is proposed as a general pre-training method based on contrastive learning for 3D HPE, aimed at extracting implicit representations of human topology. By aligning 2D poses from diverse viewpoints in the proxy task, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing the high-dimensional features of the human body, leading to further performance improvements. Extensive experiments verify that NanoHTNet with PoseCLR outperforms other state-of-the-art methods in efficiency, making it ideal for deployment on edge devices like the Jetson Nano. Code and models are available at https://github.com/vefalun/NanoHTNet.},
  archive      = {J_TIP},
  author       = {Jialun Cai and Mengyuan Liu and Hong Liu and Shuheng Zhou and Wenhao Li},
  doi          = {10.1109/TIP.2025.3608662},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NanoHTNet: Nano human topology network for efficient 3D human pose estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-driven global-local fusion transformer for image super-resolution. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image Super-Resolution (SR) has seen remarkable progress with the emergence of transformer-based architectures. However, due to the high computational cost, many existing transformer-based SR methods limit their attention to local windows, which hinders their ability to model long-range dependencies and global structures. To address these challenges, we propose a novel SR framework named Semantic-Driven Global-Local Fusion Transformer (SGLFT). The proposed model enhances the receptive field by combining a Hybrid Window Transformer (HWT) and a Scalable Transformer Module (STM) to jointly capture local textures and global context. To further strengthen the semantic consistency of reconstruction, we introduce a Semantic Extraction Module (SEM) that distills high-level semantic priors from the input. These semantic cues are adaptively integrated with visual features through an Adaptive Feature Fusion Semantic Integration Module (AFFSIM). Extensive experiments on standard benchmarks demonstrate the effectiveness of SGLFT in producing visually faithful and structurally consistent SR results. The code will be available at https://github.com/kbzhang0505/SGLFT.},
  archive      = {J_TIP},
  author       = {Kaibing Zhang and Zhouwei Cheng and Xin He and Jie Li and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3609106},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantic-driven global-local fusion transformer for image super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical color constancy via efficient spectral feature extraction. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an empirical investigation into illuminant estimation using multi-spectral images. Our study emphasizes two key contributions: (1) the utilization of the estimated multi-spectral images and (2) the incorporation of a hierarchical structure. Firstly, exploiting multi-spectral images proves to have a positive influence on illuminant estimation, particularly in scenarios characterized by monochromatic images where conventional color constancy methods face challenges. Our experimental results vividly illustrate the effectiveness of leveraging spectral information in enhancing illuminant estimation. Secondly, the adoption of a hierarchical structure stems from the need for spatial invariance in the task of estimating a global illuminant. To further enhance the performance of the hierarchical structure, we employ a contrastive loss applied to different scaled outputs. This approach demonstrates remarkable effectiveness on our custom dataset, showcasing superior performance compared to the existing methods. In addition, we extend the evaluation to the widely recognized NUS-8 dataset, where the proposed method showcases a notable 26.7% relative improvement over the previous state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Dong-Keun Han and Dong-Hoon Kang and Jong-Ok Kim},
  doi          = {10.1109/TIP.2025.3607631},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical color constancy via efficient spectral feature extraction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRS: Siamese reconstruction-segmentation network based on dynamic-parameter convolution. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic convolution demonstrates outstanding representation capabilities, which are crucial for natural image segmentation. However, it fails when applied to medical image segmentation (MIS) and infrared small target segmentation (IRSTS) due to limited data and limited fitting capacity. In this paper, we propose a new type of dynamic convolution called dynamic parameter convolution (DPConv) which shows superior fitting capacity, and it can efficiently leverage features from deep layers of encoder in reconstruction tasks to generate DPConv kernels that adapt to input variations. Moreover, we observe that DPConv, built upon deep features derived from reconstruction tasks, significantly enhances downstream segmentation performance. We refer to the segmentation network integrated with DPConv generated from reconstruction network as the siamese reconstruction-segmentation network (SRS). We conduct extensive experiments on seven datasets including five medical datasets and two infrared datasets, and the experimental results demonstrate that our method can show superior performance over several recently proposed methods. Furthermore, the zero-shot segmentation under unseen modality demonstrates the generalization of DPConv. The code is available at: https://github.com/fidshu/SRSNet.},
  archive      = {J_TIP},
  author       = {Bingkun Nian and Fenghe Tang and Jianrui Ding and Jie Yang and Zhonglong Zheng and Shaohua Kevin Zhou and Wei Liu},
  doi          = {10.1109/TIP.2025.3607624},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SRS: Siamese reconstruction-segmentation network based on dynamic-parameter convolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). No-reference image quality assessment leveraging GenAI images. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3610238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning-based methods have made significant progress on the image quality assessment problem; however, challenges remain arising from the lack of annotated, real-world training data and consequent poor generalization ability. Towards addressing these challenges, we propose a no-reference image quality assessment (NR-IQA) method based on generative AI (GenAI) images. Specifically, we use GenAI images as reference images, employing a cold diffusion model to generate distorted images of four different distortion types, and we label these distorted images using a full-reference model, thereby making it possible to construct a large-scale pre-training dataset. We use this resource generation method to facilitate NR-IQA model building. We deploy a Multi-scale Cross Attention Block (MCAB) and a Scale Simple Attention Module (SSAM) to enhance feature representation by extracting multi-scale feature information from both the channel and spatial dimensions that are predictive of image quality. Extensive experiments on eight public databases demonstrate that the proposed method achieves state-of-the-art (SOTA) performance. A public release of all the codes associated with this work will be made available on GitHub.},
  archive      = {J_TIP},
  author       = {Qingbing Sang and Qian Li and Lixiong Liu and Zhaohong Deng and Xiaojun Wu and Alan C. Bovik},
  doi          = {10.1109/TIP.2025.3610238},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {No-reference image quality assessment leveraging GenAI images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Volume fusion-based self-supervised pretraining for 3D medical image segmentation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3610249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of deep learning models for medical image segmentation is often limited in scenarios where training data or annotations are limited. Self-Supervised Learning (SSL) is an appealing solution for this dilemma due to its feature learning ability from a large amount of unannotated images. Existing SSL methods have focused on pretraining either an encoder for global feature representation or an encoder-decoder structure for image restoration, where the gap between pretext and downstream tasks limits the usefulness of pretrained decoders in downstream segmentation. In this work, we propose a novel SSL strategy named Volume Fusion (VolF) for pretraining 3D segmentation models. It minimizes the gap between pretext and downstream tasks by introducing a pseudo-segmentation pretext task, where two sub-volumes are fused by a discretized block-wise fusion coefficient map. The model takes the fused result as input and predicts the category of fusion coefficient for each voxel, which can be trained with standard supervised segmentation loss functions without manual annotations. Experiments with an abdominal CT dataset for pretraining and both in-domain and out-domain downstream datasets showed that VolF led to large performance gain from training from scratch with faster convergence speed, and outperformed several state-of-the-art SSL methods. In addition, it is general to different network structures, and the learned features have high generalizability to different body parts and modalities.},
  archive      = {J_TIP},
  author       = {Guotai Wang and Jia Fu and Jianghao Wu and Xiangde Luo and Yubo Zhou and Xinglong Liu and Kang Li and Jingsheng Lin and Baiyong Shen and Shaoting Zhang},
  doi          = {10.1109/TIP.2025.3610249},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Volume fusion-based self-supervised pretraining for 3D medical image segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Privacy-preserving visual localization with event cameras. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607640'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of client-server localization, where edge device users communicate visual data with the service provider for locating oneself against a pre-built 3D map. This localization paradigm is a crucial component for location-based services in AR/VR or mobile applications, as it is not trivial to store large-scale 3D maps and process fast localization on resource-limited edge devices. Nevertheless, conventional client-server localization systems possess numerous challenges in computational efficiency, robustness, and privacy-preservation during data transmission. Our work aims to jointly solve these challenges with a localization pipeline based on event cameras. By using event cameras, our system consumes low energy and maintains small memory bandwidth. Then during localization, we propose applying event-to-image conversion and leverage mature image-based localization, which achieves robustness even in low-light or fast-moving scenes. To further enhance privacy protection, we introduce privacy protection techniques at two levels. Network level protection aims to hide the entire user’s view in private scenes using a novel split inference approach, while sensor level protection aims to hide sensitive user details such as faces with light-weight filtering. Both methods involve small client-side computation and localization performance loss, while significantly mitigating the feeling of insecurity as revealed in our user study. We thus project our method to serve as a building block for practical location-based services using event cameras.},
  archive      = {J_TIP},
  author       = {Junho Kim and Young Min Kim and Ramzi Zahreddine and Weston A. Welge and Gurunandan Krishnan and Sizhuo Ma and Jian Wang},
  doi          = {10.1109/TIP.2025.3607640},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Privacy-preserving visual localization with event cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale segmentation-guided fusion network for hyperspectral image classification. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611146'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolution Neural Networks (CNNs) have demonstrated strong feature extraction capabilities in Euclidean spaces, achieving remarkable success in hyperspectral image (HSI) classification tasks. Meanwhile, Graph convolution networks (GCNs) effectively capture spatial-contextual characteristics by leveraging correlations in non-Euclidean spaces, uncovering hidden relationships to enhance the performance of HSI classification (HSIC). Methods combining GCNs with CNNs have achieved excellent results. However, existing GCN methods primarily rely on single-scale graph structures, limiting their ability to extract features across different spatial ranges. To address this issue, this paper proposes a multiscale segmentation-guided fusion network (MS2FN) for HSIC. This method constructs pixel-level graph structures based on multiscale segmentation data, enabling the GCN to extract features across various spatial ranges. Moreover, effectively utilizing features extracted from different spatial scales is crucial for improving classification performance. This paper adopts distinct processing strategies for different feature types to enhance feature representation. Comparative experiments demonstrate that the proposed method outperforms several state-of-the-art (SOTA) approaches in accuracy. The source code will be released at https://github.com/shengrunhua/MS2FN.},
  archive      = {J_TIP},
  author       = {Hongmin Gao and Runhua Sheng and Yuanchao Su and Zhonghao Chen and Shufang Xu and Lianru Gao},
  doi          = {10.1109/TIP.2025.3611146},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Multiscale segmentation-guided fusion network for hyperspectral image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward better than pseudo-reference in underwater image enhancement. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611138'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since degraded underwater images are not always accompanied with distortion-free counterparts in real-world situations, existing underwater image enhancement (UIE) methods are mostly learned on a paired set consisting of raw underwater images and their corresponding pseudo-reference labels. Although the existing UIE datasets manually select the best model-generated results as pseudo-references, such pseudo-reference labels do not always exhibit perfect visual quality. Therefore, it would be interesting to investigate whether it is possible to break through the performance bottleneck of UIE networks trained with imperfect pseudo-references. Motivated by these facts, this paper focuses on innovating more advanced loss functions rather than designing more complex network architectures. Specifically, a plug-and-play hybrid Performance SurPassing Loss (PSPL), consisting of a Quality Score Comparison Loss (QSCL) and a scene Depth-aware Unpaired Contrastive Loss (DUCL), is formulated to guide the training of UIE network. Functionally, QSCL aims to guide the UIE network to generate enhanced results with better visual quality than pseudo-references by constructing image quality score comparison losses from both image-level and region-level. Nevertheless, only using QSCL cannot guarantee obtaining desired results for those severely degraded distant regions. Therefore, we also design a tailored DUCL to handle this challenging issue from the scene depth perspective, i.e., DUCL encourages the distant regions of the enhanced results to be closer to the high-quality nearby regions (pull) and far away from the low-quality distant regions (push) of the pseudo-references. Extensive experimental results demonstrate the advantage of using PSPL over the state-of-the-arts even with an extremely simple and lightweight UIE network. The source code will be released at https://github.com/lewis081/PSPL.},
  archive      = {J_TIP},
  author       = {Yi Liu and Qiuping Jiang and Xingbo Li and Ting Luo and Wenqi Ren},
  doi          = {10.1109/TIP.2025.3611138},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Toward better than pseudo-reference in underwater image enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HOVER: Hyperbolic video-text retrieval. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video-text retrieval is a crucial task in numerous computer vision applications. In this paper, we focus on video-text retrieval involving complex action compositions, where a single video encompasses multiple primitive actions such as “sitting up”, “opening door”, “cooking food”, and “eating.” Despite the common occurrences in real-world scenarios, such action-compositional videos have received limited research attention, often leading to significant performance degradations in existing retrieval methods. To address this challenge, we present Hyperbolic Video-tExt Retrieval (HOVER), which models the hierarchical semantic relationships between videos and texts by embedding them in a low-dimensional hyperbolic space. Since hyperbolic space provides a geometric prior that naturally aligns with hierarchical data, it allows for more efficient and generalizable representations of video-text semantic hierarchies. HOVER first longitudinally decomposes each video into a hierarchical action tree, where primitive mono-actions are represented as leaf nodes and increasingly complex action compositions as parent nodes. The semantic structures and temporal dependencies of videos/texts are then encoded in hyperbolic space by exploiting hyperbolic distance, norm, and relative cosine similarity. Experimental results show that HOVER significantly outperforms traditional Euclidean-based methods, particularly in scenarios with limited training labels, achieving a notable performance improvement of 28.83%. Additionally, the hyperbolic video-text embeddings learned by HOVER demonstrate strong generalization across new datasets containing videos with varying levels of action complexity. The source code is available at https://github.com/shi-rq/HOVER.},
  archive      = {J_TIP},
  author       = {Jun Wen and Yufeng Chen and Ruiqi Shi and Wei Ji and Menglin Yang and Difei Gao and Junsong Yuan and Roger Zimmermann},
  doi          = {10.1109/TIP.2025.3611174},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HOVER: Hyperbolic video-text retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PCE-GAN: A generative adversarial network for point cloud attribute quality enhancement based on optimal transport. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611178'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud compression significantly reduces data volume but sacrifices reconstruction quality, highlighting the need for advanced quality enhancement techniques. Most existing approaches focus primarily on point-to-point fidelity, often neglecting the importance of perceptual quality as interpreted by the human visual system. To address this issue, we propose a generative adversarial network for point cloud quality enhancement (PCE-GAN), grounded in optimal transport theory, with the goal of simultaneously optimizing both data fidelity and perceptual quality. The generator consists of a local feature extraction (LFE) unit, a global spatial correlation (GSC) unit and a feature squeeze unit. The LFE unit uses dynamic graph construction and a graph attention mechanism to efficiently extract local features, placing greater emphasis on points with severe distortion. The GSC unit uses the geometry information of neighboring patches to construct an extended local neighborhood and introduces a transformer-style structure to capture long-range global correlations. The discriminator computes the deviation between the probability distributions of the enhanced point cloud and the original point cloud, guiding the generator to achieve high quality reconstruction. Experimental results show that the proposed method achieves state-of-the-art performance. Specifically, when applying PCE-GAN to the latest geometry-based point cloud compression (G-PCC) test model, it achieves an average BD-rate of -19.2% compared with the PredLift coding configuration and -18.3% compared with the RAHT coding configuration. Subjective comparisons show a significant improvement in texture clarity and color transitions, revealing finer details and more natural color gradients.},
  archive      = {J_TIP},
  author       = {Tian Guo and Hui Yuan and Qi Liu and Honglei Su and Raouf Hamzaoui and Sam Kwong},
  doi          = {10.1109/TIP.2025.3611178},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {PCE-GAN: A generative adversarial network for point cloud attribute quality enhancement based on optimal transport},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VisionHub: Learning task-plugins for efficient universal vision model. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611645'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building on the success of universal language models in natural language processing (NLP), researchers have recently sought to develop methods capable of tackling a broad spectrum of visual tasks within a unified foundation framework. However, existing universal vision models face significant challenges when adapting to the rapidly expanding scope of downstream tasks. These challenges stem not only from the prohibitive computational and storage expenses associated with training such models but also from the complexity of their workflows, which makes efficient adaptations difficult. Moreover, these models often fail to deliver the required performance and versatility for a broad spectrum of applications, largely due to their incomplete visual generation and perception capabilities, limiting their generalizability and effectiveness in diverse settings. In this paper, we present VisionHub, a novel universal vision model designed to concurrently manage multiple visual restoration and perception tasks, while offering streamlined transferability to downstream tasks. Our model leverages the frozen denoising U-Net architecture from Stable Diffusion as the backbone, fully exploiting its inherent potential for both visual restoration and perception. To further enhance the model’s flexibility, we propose the incorporation of lightweight task-plugins and the task router, which are seamlessly integrated onto the U-Net backbone. This architecture enables VisionHub to efficiently handle various vision tasks according to user-provided natural language instructions, all while maintaining minimal storage costs and operational overhead. Extensive experiments across 11 different vision tasks showcase both the efficiency and effectiveness of our approach. Remarkably, VisionHub achieves competitive performance across a variety of benchmarks, including 53.3% mIoU on ADE20K semantic segmentation, 0.253 RMSE on NYUv2 depth estimation, and 74.2 AP on MS-COCO pose estimation.},
  archive      = {J_TIP},
  author       = {Haolin Wang and Yixuan Zhu and Wenliang Zhao and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TIP.2025.3611645},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {VisionHub: Learning task-plugins for efficient universal vision model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAFS: Masked autoencoder for infrared-visible image fusion and semantic segmentation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611602'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared-visible image fusion methods aim at generating fused images with good visual quality and also facilitate the performance of high-level tasks. Indeed, existing semantic-driven methods have considered semantic information injection for downstream applications. However, none of them investigates the potential for reciprocal promotion between pixel-wise image fusion and cross-modal feature fusion perception tasks from a macroscopic task-level perspective. To address this limitation, we propose a unified network for image fusion and semantic segmentation. MAFS is a parallel structure, containing a fusion sub-network and a segmentation sub-network. On the one hand, We devise a heterogeneous feature fusion strategy to enhance semantic-aware capabilities for image fusion. On the other hand, by cascading the fusion sub-network and a segmentation backbone, segmentation-related knowledge is transferred to promote feature-level fusion-based segmentation. Within the framework, we design a novel multi-stage Transformer decoder to aggregate fine-grained multi-scale fused features efficiently. Additionally, a dynamic factor based on the max-min fairness allocation principle is introduced to generate adaptive weights of two tasks and guarantee smooth training in a multi-task manner. Extensive experiments demonstrate that our approach achieves competitive results compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Liying Wang and Xiaoli Zhang and Chuanmin Jia and Siwei Ma},
  doi          = {10.1109/TIP.2025.3611602},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MAFS: Masked autoencoder for infrared-visible image fusion and semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting RGBT tracking benchmarks from the perspective of modality validity: A new benchmark, problem, and solution. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611687'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGBT tracking draws increasing attention because of its robustness in multi-modal warranting (MMW) scenarios, such as nighttime and adverse weather conditions, where relying on a single sensing modality fails to ensure stable tracking results. However, existing benchmarks predominantly contain videos collected in common scenarios where both RGB and thermal infrared (TIR) information are of sufficient quality. This weakens the representativeness of existing benchmarks in severe imaging conditions, leading to tracking failures in MMW scenarios. To bridge this gap, we present a new benchmark considering the modality validity, MV-RGBT, captured specifically from MMW scenarios where either RGB (extreme illumination) or TIR (thermal truncation) modality is invalid. Hence, it is further divided into two subsets according to the valid modality, offering a new compositional perspective for evaluation and providing valuable insights for future designs. Moreover, MV-RGBT is the most diverse benchmark of its kind, featuring 36 different object categories captured across 19 distinct scenes. Furthermore, considering severe imaging conditions in MMW scenarios, a new problem is posed in RGBT tracking, named ‘when to fuse’, to stimulate the development of fusion strategies for such scenarios. To facilitate its discussion, we propose a new solution with a mixture of experts, named MoETrack, where each expert generates independent tracking results along with a confidence score. Extensive results demonstrate the significant potential of MV-RGBT in advancing RGBT tracking and elicit the conclusion that fusion is not always beneficial, especially in MMW scenarios. Besides, MoETrack achieves state-of-the-art results on several benchmarks, including MV-RGBT, GTOT, and LasHeR. Source codes and benchmarks are available at https://github.com/Zhangyong-Tang/MVRGBT.},
  archive      = {J_TIP},
  author       = {Zhangyong Tang and Tianyang Xu and Xiao-Jun Wu and Xuefeng Zhu and Chunyang Cheng and Zhenhua Feng and Josef Kittler},
  doi          = {10.1109/TIP.2025.3611687},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Revisiting RGBT tracking benchmarks from the perspective of modality validity: A new benchmark, problem, and solution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An explanation method based on interpretable linear model with four key characteristics. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611593'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the interpretability of deep neural networks (DNNs) in visual-related tasks, existing explanation methods commonly generate a saliency map based on the linear relation between output results and input features. However, when the explanation conflicts with a human visual examination, these methods do not provide further evidence to analyze the saliency explanation. Most may fail to provide feature attribution with identifiable semantics or produce misleading explanations due to their insufficient robustness. In this paper, we first propose four key characteristics (richness, adaptivity, exclusiveness, and fairness) to evaluate the existing linear relation-based explanation method, and then construct an interpretable linear model to satisfy them. We formalize the characteristics and develop a novel explanation method based on this. We extract and reconstruct key exclusive semantic features from the feature map using the Nonnegative Matrix Factorization (NMF) algorithm, utilize the information entropy model to determine the number of features adaptively and their richness, and then linearly combine each feature with fairly assigned weights using an approximate Shapley algorithm to generate the saliency map. Compared with the state-of-the-art methods, our explanations of different datasets and DNNs are more convincing and robust in terms of Average drop (AD), Average increase (AI), Deletions (Del), and Insertions (Ins). Our supplementary experiments provide sufficient evidence that the four characteristics guarantee the feasibility of feature attribution analysis and enhance the quality of the resulting explanations.},
  archive      = {J_TIP},
  author       = {Yuecan Yuan and Zhan Ao Huang and Peng Li and Ying Fu and Xuemin Zhao and Canghong Shi and Xiaojie Li and Xi Wu},
  doi          = {10.1109/TIP.2025.3611593},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An explanation method based on interpretable linear model with four key characteristics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Consistent assistant domains transformer for source-free domain adaptation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611799'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-free domain adaptation (SFDA) aims to address the challenge of adapting to a target domain without accessing the source domain directly. However, due to the inaccessibility of source domain data, deterministic invariable features cannot be obtained. Current mainstream methods primarily focus on evaluating invariant features in the target domain that closely resemble those in the source domain, subsequently aligning the target domain with the source domain. However, these methods are susceptible to hard samples and influenced by domain bias. In this paper, we propose a Consistent Assistant Domains Transformer for SFDA, abbreviated as CADTrans, which solves the issue by constructing invariable feature representations of domain consistency. Concretely, we develop an assistant domain module for CADTrans to obtain diversified representations from the intermediate aggregated global attentions, which addresses the limitation of existing methods in adequately representing diversity. Based on assistant and target domains, invariable feature representations are obtained by multiple consistent strategies, which can be used to distinguish easy and hard samples. Finally, to align the hard samples to the corresponding easy samples, we construct a conditional multi-kernel max mean discrepancy (CMK-MMD) strategy to distinguish between samples of the same category and those of different categories. Extensive experiments are conducted on various benchmarks such as Office-31, Office-Home, VISDA-C, and DomainNet-126, proving the significant performance improvements achieved by our proposed approaches. Code is available at https://github.com/RoryShao/CADTrans.git.},
  archive      = {J_TIP},
  author       = {Renrong Shao and Wei Zhang and Kangyang Luo and Qin Li and Jun Wang},
  doi          = {10.1109/TIP.2025.3611799},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Consistent assistant domains transformer for source-free domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An adaptor for triggering semi-supervised learning to out-of-box serve deep image clustering. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3611144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, some works integrate SSL techniques into deep clustering frameworks to enhance image clustering performance. However, they all need pre-training, clustering learning, or a trained clustering model as prerequisites, limiting the flexible and out-of-box application of SSL learners in the image clustering task. This work introduces ASD, an adaptor that enables the cold-start of SSL learners for deep image clustering without any prerequisites. Specifically, we first randomly sample pseudo-labeled data from all unlabeled data, and set an instance-level classifier to learn them with semantically aligned instance-level labels. With the ability of instance-level classification, we track the class transitions of predictions on unlabeled data to extract high-level similarities of instance-level classes, which can be utilized to assign cluster-level labels to pseudo-labeled data. Finally, we use the pseudo-labeled data with assigned cluster-level labels to trigger a general SSL learner trained on the unlabeled data for image clustering. We show the superior performance of ASD across various benchmarks against the latest deep image clustering approaches and very slight accuracy gaps compared to SSL methods using ground-truth, e.g., only 1.33% on CIFAR-10. Moreover, ASD can also further boost the performance of existing SSL-embedded deep image clustering methods.},
  archive      = {J_TIP},
  author       = {Yue Duan and Lei Qi and Yinghuan Shi and Yang Gao},
  doi          = {10.1109/TIP.2025.3611144},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {An adaptor for triggering semi-supervised learning to out-of-box serve deep image clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperbolic self-paced multi-expert network for cross-domain few-shot facial expression recognition. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3612281'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, cross-domain few-shot facial expression recognition (CF-FER), which identifies novel compound expressions with a few images in the target domain by using the model trained only on basic expressions in the source domain, has attracted increasing attention. Generally, existing CF-FER methods leverage the multi-dataset to increase the diversity of the source domain and alleviate the discrepancy between the source and target domains. However, these methods learn feature embeddings in the Euclidean space without considering imbalanced expression categories and imbalanced sample difficulty in the multi-dataset. Such a way makes the model difficult to capture hierarchical relationships of facial expressions, resulting in inferior transferable representations. To address these issues, we propose a hyperbolic self-paced multi-expert network (HSM-Net), which contains multiple mixture-of-experts (MoE) layers located in the hyperbolic space, for CF-FER. Specifically, HSM-Net collaboratively trains multiple experts in a self-distillation manner, where each expert focuses on learning a subset of expression categories from the multi-dataset. Based on this, we introduce a hyperbolic self-paced learning (HSL) strategy that exploits sample difficulty to adaptively train the model from easy-to-hard samples, greatly reducing the influence of imbalanced expression categories and imbalanced sample difficulty. Our HSM-Net can effectively model rich hierarchical relationships of facial expressions and obtain a highly transferable feature space. Extensive experiments on both in-the-lab and in-the-wild compound expression datasets demonstrate the superiority of our proposed method over several state-of-the-art methods. Code will be released at https://github.com/cxtjl/HSM-Net.},
  archive      = {J_TIP},
  author       = {Xueting Chen and Yan Yan and Jing-Hao Xue and Chang Shu and Hanzi Wang},
  doi          = {10.1109/TIP.2025.3612281},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperbolic self-paced multi-expert network for cross-domain few-shot facial expression recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep sparse-to-dense inbetweening for multi-view light fields. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3612257'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field (LF) imaging, which captures both intensity and directional information of light rays, extends the capabilities of traditional imaging techniques. In this paper, we introduce a task in the field of LF imaging, sparse-to-dense inbetweening, which focuses on generating dense novel views from sparse multi-view LFs. By synthesizing intermediate views from sparse inputs, this task enhances LF view synthesis through filling in interperspective gaps within an expanded field of view and increasing data robustness by leveraging complementary information between light rays from different perspectives, which are limited by non-robust single-view synthesis and the inability to handle sparse inputs effectively. To address these challenges, we construct a high-quality multi-view LF dataset, consisting of 60 indoor scenes and 59 outdoor scenes. Building upon this dataset, we propose a baseline method. Specifically, we introduce an adaptive alignment module to dynamically align information by capturing relative displacements. Next, we explore angular consistency and hierarchical information using a multi-level feature decoupling module. Finally, a multi-level feature refinement module is applied to enhance features and facilitate reconstruction. Additionally, we introduce a universally applicable artifact-aware loss function to effectively suppress visual artifacts. Experimental results demonstrate that our method outperforms existing approaches, establishing a benchmark for sparse-to-dense inbetweening. The code is available at https://github.com/Starmao1/MutiLF.},
  archive      = {J_TIP},
  author       = {Yifan Mao and Zeyu Xiao and Ping An and Deyang Liu and Caifeng Shan},
  doi          = {10.1109/TIP.2025.3612257},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep sparse-to-dense inbetweening for multi-view light fields},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
