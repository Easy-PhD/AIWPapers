<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip">TIP - 38</h2>
<ul>
<li><details>
<summary>
(2025). Instance-adaptive spatial-temporal enhancement for efficient video compression. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3602648'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficiently compressing HD/UHD content has long been challenging due to high bitrate costs. Instance-adaptive enhancement methods try to tackle this issue by compressing a video at reduced resolution and enhancing it using a neural model specifically overfitted for this video. However, existing methods focus solely on spatial super-resolution (SR) and under-utilize the videos’ temporal redundancy. Their limited management of the model’s updated parameters also causes excessive overfitting overheads. Therefore, this paper introduces IASTE, the first instance-adaptive enhancement method based on spatial-temporal enhancement (STE), and incorporates low-rank adaptation (LoRA) for efficient model overfitting. Specifically, we downscale videos spatially and temporally to reduce the data volume and achieve efficient video compression. Then, we overfit a specific STE model for each video and use it to enhance the decoded video’s spatiotemporal resolution. Leveraging the video swin transformer’s strong capability in capturing spatiotemporal correlations, we design a lightweight and efficient model to implement video STE. The model is overfitted for each video using LoRA. By freezing the pre-trained model and selectively updating a few low-rank matrices, the bitrate overhead for model storage can be mitigated. Experiments prove that compared to directly compressing high-frame-rate (HFR), high-resolution (HR) videos, our method achieves around 30% BD-Rate gains on the CTC and UVG datasets, about 15% gains on the YoutubeUGC dataset, and about 10% gains on the ultra-long videos in the Xiph dataset.},
  archive      = {J_TIP},
  author       = {Yan Zhao and Zhengxue Cheng and Jiangchuan Li and Donghui Feng and Qunshan Gu and Qi Wang and Guo Lu and Li Song},
  doi          = {10.1109/TIP.2025.3602648},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Instance-adaptive spatial-temporal enhancement for efficient video compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Zero-shot image recognition via learning dual prototype accordance across meta-domains. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot learning (ZSL) aims to recognize unseen classes by transferring semantic knowledge from seen categories. However, existing methods often struggle with the persistent semantic gap caused by limited semantic descriptors and rigid visual feature modeling. In particular, modeling pre-defined class-level attribute descriptions as ground truth hinders effective semantic-to-visual alignment to some extent. To mitigate these issues, we propose the Bilateral-guided Prototype Refinement Network(BPRN), a novel ZSL framework designed to refine dual prototypes across meta-domains of varying scales. Specifically, we first disentangle the relationships among class-level semantics and use them to generate corresponding pseudo-visual prototypes. Then, by leveraging distribution information across dual prototypes in different meta-domains, BPRN achieves bidirectional calibration between visual-to-semantic and semantic-to-visual modalities. Finally, a synthesized class-level representation derived from the refined dual prototypes is employed for inference, instead of relying on a single prototype. Extensive experiments conducted on five widely-used ZSL benchmark datasets demonstrate that BPRN consistently achieves competitive or even superior performance. Specifically, in the GZSL scenario, BPRN shows improvements of 2.1%, 7.3%, 6.1%, and 4.8% on AWA1, AWA2, SUN, and aPY, respectively, compared to existing embedding-based ZSL methods. Ablation studies and visualization analyses further validate the effectiveness of the proposed components.},
  archive      = {J_TIP},
  author       = {Bocheng Ren and Yuanyuan Yi and Qingchen Zhang and Debin Liu},
  doi          = {10.1109/TIP.2025.3607588},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Zero-shot image recognition via learning dual prototype accordance across meta-domains},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Motion and appearance decoupling representation for event cameras. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras, with high temporal resolution and high dynamic range, have shown great potential under extreme scenarios such as high-speed movement and low illumination. However, previous event representation methods typically aggregate event data into a single dense tensor, often overlooking the dynamic changes of events within a given time unit. This limitation can introduce historical artifacts and semantic inconsistencies, ultimately degrading model performance. Inspired by human visual prior, we propose a motion and appearance decoupling (MAD) event representation to disentangle the mixed spatial-temporal event tensor into two independent branches. This bio-inspired design helps the network extract discriminative temporal (i.e., motion) and spatial (i.e., appearance) information, thus reducing the network’s learning burden toward complex high-level interpretation tasks. In our method, the event motion guided attention module (EMGA) is designed to achieve temporal and spatial feature interaction and fusion sequentially. Based on EMGA, three specially designed decoder heads are proposed for several representative event-based tasks (i.e., object detection, semantic segmentation, and human pose estimation). Experimental results demonstrate that our method achieves state-of-the-art performance on the above three tasks, which reveals that our method is an easy-to-implement replacement for currently event-based methods. Our code is available at: https://github.com/ChenYichen9527/MAD-representation.},
  archive      = {J_TIP},
  author       = {Nuo Chen and Boyang Li and Yingqian Wang and Xinyi Ying and Longguang Wang and Chushu Zhang and Yulan Guo and Miao Li and Wei An},
  doi          = {10.1109/TIP.2025.3607632},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Motion and appearance decoupling representation for event cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking generalized zero-shot learning: A synthesized per-instance attribute perspective. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607612'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized zero-shot learning (GZSL) shows great potential for improving generalization to unseen classes in real-world scenarios. However, most GZSL methods depend on benchmark datasets with per-class attribute annotations, which creates a large semantic gap and worsens the domain shift problem in the visual-semantic space. To address these challenges, instance-level attributes offer an intuitive solution, but they require expensive manual annotation. In this paper, we propose a simple yet effective approach called per-instance attribute synthesis (PIAS) to generate diverse semantic representations for each instance. Our method first uses the Vision Transformer (ViT) model to extract visual features and then generates per-instance attributes. The patch splitting, positional embedding, and multi-head self-attention mechanisms in ViT improve the discriminability of both visual and semantic representations. Next, we define the generated attributes of class-average images as class anchor points. These anchor points are calibrated in the semantic space by minimizing the cosine similarity between the anchor points and per-class attribute annotations. Finally, we improve the diversity of generated per-instance attributes by aligning the topological structure between per-class attribute annotations and synthesized per-instance attributes with that between class-average visual features and per-instance visual features. We conduct comprehensive experiments on three challenging ZSL datasets: AWA2, CUB, and SUN. The results show that PIAS significantly outperforms state-of-the-art methods under both ZSL and GZSL settings. We further demonstrate the generalization ability of PIAS by applying it to attribute-based zero-shot image retrieval tasks.},
  archive      = {J_TIP},
  author       = {Chenwei Tang and Ying Wang and Wei Xie and Qianjun Zhang and Rong Xiao and Zhenan He and Jiancheng Lv},
  doi          = {10.1109/TIP.2025.3607612},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking generalized zero-shot learning: A synthesized per-instance attribute perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSP: Multimodal self-attention prompt learning. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal prompt learning has emerged as an effective strategy for adapting vision-language models such as CLIP to downstream tasks. However, conventional approaches typically operate at the input level, forcing learned prompts to propagate through a sequence of frozen Transformer layers. This indirect adaptation introduces cumulative geometric distortions, a limitation that we formalize as the indirect learning dilemma (ILD), leading to overfitting of the base class and reduced generalization to novel classes. To overcome this challenge, we propose the Multimodal Self-Attention Prompt (MSP) framework, which shifts adaptation into the semantic core of the model by injecting learnable prompts directly into the key and value sequences of attention blocks. This direct modulation preserves the pretrained embedding geometry while enabling more precise downstream adaptation. MSP further incorporates distance-aware optimization to maintain semantic consistency with CLIP’s original representation space, and partial prompt learning via stochastic dimension masking to improve robustness and prevent over-specialization. Extensive evaluations across 11 benchmarks demonstrate the effectiveness of MSP. It achieves a state-of-the-art harmonic mean accuracy of 80.67%, with 77.32% accuracy on novel classes—representing a 2.18% absolute improvement over prior methods—while requiring only 0.11M learnable parameters. Notably, MSP surpasses CLIP’s zero-shot performance on 10 out of 11 datasets, establishing a new paradigm for efficient and generalizable prompt-based adaptation. Our implementation is available at https://github.com/laixinyi023/ Multimodal-Self-Attention-Prompt.},
  archive      = {J_TIP},
  author       = {Xinyi Lai and Xiao Ke and Huangbiao Xu and Shanghui Wu and Wenzhong Guo},
  doi          = {10.1109/TIP.2025.3607613},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MSP: Multimodal self-attention prompt learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uni-ISP: Towards unifying the learning of ISPs from multiple mobile cameras. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607617'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern end-to-end image signal processors (ISPs) can learn complex mappings from RAW/XYZ data to sRGB (and vice versa), opening new possibilities in image processing. However, the growing diversity of camera models, particularly in mobile devices, renders the development of individual ISPs unsustainable due to their limited versatility and adaptability across varied camera systems. In this paper, we introduce Uni-ISP, a novel pipeline that unifies ISP learning for diverse mobile cameras, delivering a highly accurate and adaptable processor. The core of Uni-ISP is leveraging device-aware embeddings through learning forward/inverse ISPs and its special training scheme. By doing so, Uni-ISP not only improves the performance of forward and inverse ISPs but also unlocks new applications previously inaccessible to conventional learned ISPs. To support this work, we construct a real-world 4K dataset, FiveCam, comprising more than 2,400 pairs of sRGB-RAW images synchronously captured by five smartphone cameras. Extensive experiments validate Uni-ISP’s accuracy in learning forward and inverse ISPs (with improvements of +2.4dB/1.5dB PSNR), versatility in enabling new applications, and adaptability to new camera models.},
  archive      = {J_TIP},
  author       = {Lingen Li and Mingde Yao and Xingyu Meng and Muquan Yu and Tianfan Xue and Jinwei Gu},
  doi          = {10.1109/TIP.2025.3607617},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Uni-ISP: Towards unifying the learning of ISPs from multiple mobile cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cost-efficient open vocabulary 3D scene understanding based on semantic probability. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607643'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional 3D scene understanding methods heavily depend on 3D annotation and training, which allow for the identification of seen classes but struggle to recognize unseen classes. In this paper, we leverage the open vocabulary inference capabilities of pre-trained models, enabling the encoding of open vocabulary concepts. However, unlike existing open vocabulary 3D scene understanding methods, we propose a framework based on semantic probability. This innovation significantly reduces computational cost and is compatible with state-of-the-art two-stage 2D pre-trained models. Specifically, we align the text features from the CLIP model with the pixel features from the 2D pre-trained models, inferring semantic probability of image pixels based on similarity and projecting it onto 3D points. Subsequently, we introduce a point cloud pairs semantic fusion method to merge the point clouds, reducing the semantic probability of erroneous 3D points. Based on probability scores, we achieve 3D semantic segmentation on open vocabularies without any supervision or training. In addition, the semantic probability of 3D points can serve as pseudo-labels for 3D distillation, and the geometric features of the 3D scene can be exploited to improve the segmentation performance. Experimental results demonstrate that the proposed method exhibits competitive performance on publicly available benchmark datasets, including ScanNet, Matterport3D, and nuScenes.},
  archive      = {J_TIP},
  author       = {Lingfeng Shen and Xiaoyao Wei and Gang Pan and Qian Zheng and Yanlong Cao},
  doi          = {10.1109/TIP.2025.3607643},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cost-efficient open vocabulary 3D scene understanding based on semantic probability},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MambaDiff: Mamba-enhanced diffusion model for 3D medical image segmentation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607615'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate 3D medical image segmentation is crucial for diagnosis and treatment. Diffusion models demonstrate promising performance in medical image segmentation tasks due to the progressive nature of the generation process and the explicit modeling of data distributions. However, the weak guidance of conditional information and insufficient feature extraction in diffusion models lead to the loss of fine-grained features and structural consistency in the segmentation results, thereby affecting the accuracy of medical image segmentation. To address this challenge, we propose a Mamba-Enhanced Diffusion Model for 3D Medical Image Segmentation. We extract multilevel semantic features from the original images using an encoder and tightly integrate them with the denoising process of the diffusion model through a Semantic Hierarchical Embedding (SHE) mechanism, to capture the intricate relationship between the noisy label and image data. Meanwhile, we design a Global-Slice Perception Mamba (GSPM) layer, which integrates multi-dimensional perception mechanisms to endow the model with comprehensive spatial reasoning and feature extraction capabilities. Experimental results show that our proposed MambaDiff achieves more competitive performance compared to prior arts with substantially fewer parameters on four public medical image segmentation datasets including BraTS 2021, BraTS 2024, LiTS and MSD Hippocampus. The source code of our method is available at https://github.com/yuliu316316/MambaDiff.},
  archive      = {J_TIP},
  author       = {Yu Liu and Yan Feng and Juan Cheng and Haolin Zhan and Zhiqin Zhu},
  doi          = {10.1109/TIP.2025.3607615},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MambaDiff: Mamba-enhanced diffusion model for 3D medical image segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3DFACENet: 3D facial attractiveness computation and enhancement network. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607629'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of facial editing, virtual makeup, AR/VR technologies and 3D games applications underscore the need for advanced 3D facial attractiveness research. However, due to the lack of 3D beauty face data and the complexity of handling 3D face data, 3D facial aesthetics research remains largely unexplored. To fill this gap, we propose 3DFACENet, an innovative system designed for the computation and enhancement of 3D facial attractiveness. Our approach employs a 3D facial reconstruction encoder to generate encoded vectors from images and a render module to obtain 3D face models. To minimize computational load, we innovatively propose an attractiveness computation module which leverage 3D shape and texture coefficients rather than 3D mesh models to access facial attractiveness, achieving state-of-the-art results. To balance aesthetic enhancement and identity preservation, we design a controllable beautification decoder. For the first time, we introduce the concept of “attractive centers”, demonstrating that an individual’s distance to these centers is significantly negatively correlated with their beauty scores. Our beautification decoder edits 3D facial coefficients towards these centers, achieving a significant and controllable enhancement in facial attractiveness. Extensive experiments on the SCUT-FBP5500 and MEBeauty dataset validate the effectiveness and feasibility of 3DFACENet.},
  archive      = {J_TIP},
  author       = {Yuan Xie and Tianhao Peng and Mu Li and Baoyuan Wu and David Zhang},
  doi          = {10.1109/TIP.2025.3607629},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {3DFACENet: 3D facial attractiveness computation and enhancement network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized multi-feature-guided progressive fringe order correction for fringe projection profilometry. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607619'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phase unwrapping is a critical step in fringe projection profilometry, essential for achieving accurate and efficient three-dimension (3D) imaging. Temporal phase unwrapping is the most widely utilized to improve robustness and the reconstruction quality. Unfortunately, due to abrupt phase discontinuities at boundaries, misalignment between the wrapped phases, and unreliable shadow regions, fringe order errors may occur. To address these challenges, this study presents a generalized multi-feature-guided progressive order correction algorithm (GMP-OCA) for high-quality 3D imaging. The algorithm integrates global coarse detection, incremental line-wise optimization, and regional precision scanning to progressively correct fringe orders. Static and dynamic experimental results demonstrate that GMP-OCA effectively eliminates the systematic errors inherent in various phase unwrapping methods, producing high-quality 3D imaging results.},
  archive      = {J_TIP},
  author       = {Jiayi Qin and Yansong Jiang and Yiping Cao},
  doi          = {10.1109/TIP.2025.3607619},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Generalized multi-feature-guided progressive fringe order correction for fringe projection profilometry},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring multimodal knowledge for image compression via large foundation models. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607616'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge is an abstraction of factual principles of the physical world. Large foundation models encapsulate extensive multimodal knowledge into the parameters and thus invoke machine intelligence on various tasks. How to invoke the knowledge in these models to facilitate image compression lacks in-depth exploration. In this work, we aim to harness multi-modal knowledge into ultra-low bitrate compression and propose Multimodal Knowledge-aware Image Compression (MKIC). Our key insight is that under the context of ultra-low bitrate compression, where the encoded representation is too sparse to represent enough information of the input signal, knowledge from the physical world is required to be incorporated into the compression. Thus, more shared patterns can be stored in the model together with sparse unique features also embedded into the bitstream. In light of two kinds of knowledge, namely natural visual knowledge and human language knowledge, we propose a novel Alternating Rate-Distortion Optimization to enhance the accuracy and compactness of global semantic text representation extraction, extract the local feature map that captures visual details, and integrate these multimodal representations into a large generative foundation model to achieve high-quality reconstruction. The proposed method relights the path of learned image coding, leveraging decoupled knowledge from large foundation models. Extensive experiments show that our proposed method achieves superior comprehensive performance compared to various methods and shows great potential for ultra-low bitrate image compression.},
  archive      = {J_TIP},
  author       = {Junlong Gao and Zhimeng Huang and Qi Mao and Siwei Ma and Chuanmin Jia},
  doi          = {10.1109/TIP.2025.3607616},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring multimodal knowledge for image compression via large foundation models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Prospective layout-guided multi-modal online hashing. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607626'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real-world scenarios, the data usually appears in a streaming fashion. To achieve remarkable retrieval performance in such scenarios, online multi-modal hashing has drawn great research attention due to its high retrieval speed and low storage cost. However, existing online multi-modal hashing methods still fail to achieve satisfactory retrieval performance in the scenarios where the new streaming datapoints all belong to the new classes. Therefore, to further improve the retrieval performance in these scenarios, we propose a novel Prospective Layout-Guided Multi-modal Online Hashing, termed PLG-MOH. Specifically, PLG-MOH first establishes the layout of the Hamming space by generating a series of hashing centers to split the space. Each hashing center will be gradually assigned to a new appearing class, and these assigned centers correspond one-to-one with the classes. Moreover, we propose a novel prospective layout-guided loss, which leverages all the hashing centers, including those not yet assigned to the classes, to supervise the training of hashing model. As the unassigned hashing centers will be designated to the new classes emerging in the future, it signifies that during each round of training, PLG-MOH has already considered the forthcoming data from new classes in the future rounds. Consequently, PLG-MOH can effectively adapt its hashing functions to address the new arriving samples and learn semantic similarity-preserved hash codes for them, meanwhile it can effectively retain the information learned from the old data. Extensive experiments on two public datasets demonstrate that the proposed PLG-MOH achieves better retrieval performance than state-of-the-art baselines on online scenarios.},
  archive      = {J_TIP},
  author       = {Rong-Cheng Tu and Xian-Ling Mao and Jin-Yu Liu and Zi-Ao Ma and Tian Lan and Heyan Huang},
  doi          = {10.1109/TIP.2025.3607626},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Prospective layout-guided multi-modal online hashing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceptually-guided VR style transfer. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607611'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) makes it possible to provide immersive multimedia content composed of omnidirectional videos (ODVs). Towards enabling more immersive and satisfying VR content, methods are needed to manipulate VR scenes, taking into account perceptual factors related to viewers’ quality of experience (QoE). For example, style transfer methods can be applied to VR content, allowing users to create artistic or surreal effects in their immersive environments. Here, we study perceptual factors that affect the sensation of stylized immersiveness, including color dynamics and spatio-temporal consistency. To do this, we introduce an immersiveness sensitivity model of luminance and color perception, and use it to measure the color dynamics and spatio-temporal consistency of stylized VR contents. We subsequently use this model to construct a perceptually-guided VR style transfer model called VR Style Transfer GAN (VRST-GAN). VRST-GAN learns to transfer a desired style into VR to enhance immersiveness by considering color dynamics while preserving spatio-temporal consistency. We demonstrate the effectiveness of VRST-GAN via qualitative and quantitative experiments. We also develop a VR Immersiveness Predictor (VR-IP) that is able to predict the sensation of immersiveness using the perceptual model. In our experiments, VR-IP predicts immersiveness with an accuracy of 91%.},
  archive      = {J_TIP},
  author       = {Seonghwa Choi and Jungwoo Huh and Sanghoon Lee and Alan Conrad Bovik},
  doi          = {10.1109/TIP.2025.3607611},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Perceptually-guided VR style transfer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive anchor-guided representation learning for efficient multi-view subspace clustering. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607587'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view Subspace Clustering (MVSC) effectively aggregating multiple data sources to promise clustering performance. Recently, various anchor-based variants have been introduced to effectively alleviate the computation complexity of MVSC. Although satisfactory advancement has been achieved, existing methods either independently learn anchor matrices and their anchor representations or learn a consensus anchor matrix and unified anchor representation, failing to capture both consistency and complementary information simultaneously. In addition, the time complexity of obtaining clustering results by applying Singular Value Decomposition (SVD) on the anchor representation matrix remains high. To tackle the above problems, we propose an Adaptive Anchor-guided Representation Learning for Efficient Multi-view Subspace Clustering (A2RL-EMVSC) framework, which integrates consensus anchors learning, anchor-guided representation learning and matrix factorization to enhance clustering performance and scalability. Technically, the proposed method learns view-specific anchor representation matrices by consensus anchors guidance, which simultaneously exploit consistency and complementary information. Moreover, by applying matrix decomposition to the view-specific anchor representation matrices, clustering results can be achieved with linear time complexity. Extensive experiments on ten challenging multi-view datasets show that the proposed method can improve the effectiveness and superiority of clustering compared with state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Mengjiao Zhang and Xinwang Liu and Tianhao Han and Xiaofeng Qu and Sijie Niu},
  doi          = {10.1109/TIP.2025.3607587},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Adaptive anchor-guided representation learning for efficient multi-view subspace clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HOPE: Enhanced position image priors via high-order implicit representations. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607582'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Image Prior (DIP) has shown that networks with stochastic initialization and custom architectures can effectively address inverse imaging challenges. Despite its potential, DIP requires significant computational resources, whereas the lighter Implicit Neural Positional Image Prior (PIP) often yields overly smooth solutions due to exacerbated spectral bias. Research on lightweight, high-performance solutions for inverse imaging remains limited. This paper proposes a novel framework, Enhanced Positional Image Priors through High-Order Implicit Representations (HOPE), incorporating high-order interactions between layers within a conventional cascade structure. This approach reduces the spectral bias commonly seen in PIP, enhancing the model’s ability to capture both low- and high-frequency components for optimal inverse problem performance. We theoretically demonstrate that HOPE’s expanded representational space, narrower convergence range, and improved Neural Tangent Kernel (NTK) diagonal properties enable more precise frequency representations than PIP. Comprehensive experiments across tasks such as signal representation (audio, image, volume) and inverse image processing (denoising, super-resolution, CT reconstruction, inpainting) confirm that HOPE establishes new benchmarks for recovery quality and training efficiency.},
  archive      = {J_TIP},
  author       = {Yang Chen and Ruituo Wu and Junhui Hou and Ce Zhu and Yipeng Liu},
  doi          = {10.1109/TIP.2025.3607582},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HOPE: Enhanced position image priors via high-order implicit representations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Synergistic prompting learning for human-object interaction detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human-Object Interaction (HOI) detection, as a foundational task in human-centric understanding, aims to detect interactive triplets in real-world scenarios. To better distinguish diverse HOIs within an open-world context, current HOI detectors utilize pre-trained Visual-Language Models (VLMs) to extract prior knowledge through textual prompts (i.e., descriptive texts for each HOI instance). However, relying on predetermined descriptive texts, such approaches only acquire a fixed set of textual knowledge for HOI prediction, consequently resulting in inferior performance and limited generalization. To remedy this, we propose a novel VLM-based method, which jointly performs prompting learning from both visual and textual perspectives and synergizes visual-textual prompting for HOI detection. Initially, we design a hierarchical adaptation architecture to perform progressive prompting: visual prompting is facilitated through gradual token migration from VLM’s image encoder, while textual prompting is initialized with progressively leveled interaction descriptions. In addition, to synergize the visual-textual prompting learning, a text-supervising and image-tuning loop is introduced, in which the text-supervising stage guides visual prompting learning through contrastive learning and the image-tuning stage refines textual prompting by modal matching. Finally, we employ an interaction-aware knowledge merging mechanism to effectively transfer visual-textual knowledge encapsulated within synergistic prompting for HOI detection. Extensive experiments on two benchmarks demonstrate that our proposed method outperforms the state-of-the-art ones, under both supervised and zero-shot settings.},
  archive      = {J_TIP},
  author       = {Jinguo Luo and Weihong Ren and Zhiyong Wang and Xi’ai Chen and Huijie Fan and Zhi Han and Honghai Liu},
  doi          = {10.1109/TIP.2025.3607614},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Synergistic prompting learning for human-object interaction detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A multi-category anomaly editing network with correlation exploration and voxel-level attention for unsupervised surface anomaly detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607638'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing a unified model for surface anomaly detection remains challenging due to significant variations across product categories. Recent feature editing methods, as a branch of image reconstruction, mitigate the over-generalization of auto-encoders that leads to accurate anomaly reconstruction. However, these methods are only suited for texture-category products and have significant limitations in being generalized to other categories. In this article, we propose a multi-category anomaly editing network with a dual-branch training approach: one branch processes defect-free images (normal branch), while the other handles synthetic anomaly images (anomaly branch). Specifically, the paired samples are first fed into the multi-category anomaly feature editing based auto-encoder (MCAFE-AE) to perform image reconstruction and inpainting. In the normal branch, we propose a dual-entropy constrained deep embedded clustering module (DEC-DECM) to promote a more compact and orderly distribution of normal latent features, while avoiding trivial clustering solutions. Based on the clustering results, we further design a patch-based adaptive thresholding (PAT) strategy to adaptively calculate the threshold representing the central boundary of the cluster center for each local patch, thereby enabling the model to detect anomalies. Then, in the anomaly branch, we propose a multi-category anomaly feature editing module (MCAFEM) to identify anomalies in synthetic images and apply a category-oriented feature editing strategy to transform detected anomaly features into normal ones, thereby suppressing the reconstruction of anomalies. After completing the image reconstruction and inpainting, the input images from both branches and their respective output images are concatenated and fed into the correlation exploration and voxel-level attention based prediction network (CEVA-Net) for anomaly segmentation. The network is integrated with our proposed correlation-dependency exploration and voxel-level attention refinement module (CDE-VARM) and generates precise anomaly maps under the guidance of the bidirectional-path feature fusion (BPFF) and deep supervised learning (DSL). Extensive experiments on three datasets show that our method achieves state-of-the-art performance.},
  archive      = {J_TIP},
  author       = {Ruifan Zhang and Hai-Miao Hu},
  doi          = {10.1109/TIP.2025.3607638},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {A multi-category anomaly editing network with correlation exploration and voxel-level attention for unsupervised surface anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). URFusion: Unsupervised unified degradation-robust image fusion network. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607628'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When dealing with low-quality source images, existing image fusion methods either fail to handle degradations or are restricted to specific degradations. This study proposes an unsupervised unified degradation-robust image fusion network, termed as URFusion, in which various types of degradations can be uniformly eliminated during the fusion process, leading to high-quality fused images. URFusion is composed of three core modules: intrinsic content extraction, intrinsic content fusion, and appearance representation learning and assignment. It first extracts degradation-free intrinsic content features from images affected by various degradations. These content features then provide feature-level rather than image-level fusion constraints for optimizing the fusion network, effectively eliminating degradation residues and reliance on ground truth. Finally, URFusion learns the appearance representation of images and assign the statistical appearance representation of high-quality images to the content-fused result, producing the final high-quality fused image. Extensive experiments on multi-exposure image fusion and multi-modal image fusion tasks demonstrate the advantages of URFusion in fusion performance and suppression of multiple types of degradations. The code is available at https://github.com/hanna-xu/URFusion.},
  archive      = {J_TIP},
  author       = {Han Xu and Xunpeng Yi and Chen Lu and Guangcan Liu and Jiayi Ma},
  doi          = {10.1109/TIP.2025.3607628},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {URFusion: Unsupervised unified degradation-robust image fusion network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Streaming view classification with noisy label. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607610'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many image processing tasks, e.g., 3D reconstruction of dynamic scenes, different types of descriptions, a.k.a., views, of an object are emerging in a streaming way. Streaming view learning provides an effective solution to this dynamic view problem. In this paradigm, existing streaming view learning methods typically assume that all labels are accurate. However, in many real-world applications, the initial views may be not good enough for characterizing, leading to noisy labels that degrade classification performance. How to learn a model for simultaneous view evolving and label ambiguity is critical yet unexplored. In this paper, we propose a novel method called Streaming View Classification with Noisy Label (SVCNL). We calibrate noisy labels according to the emerging of new views, thereby reflecting the dynamic changes in the data more accurately. Leveraging the sequential and non-revisitable nature of views, the method tunes existing models to inherit information from previous stages by utilizing current-stage data. It reconstructs noisy labels through a label transition matrix and establishes relationships between true labels and samples using a graph embedding strategy, progressively correcting noisy labels. Together with the theoretical analyses about generalization bounds, extensive experiments demonstrate the effectiveness of the proposed approach.},
  archive      = {J_TIP},
  author       = {Xiao Ouyang and Ruidong Fan and Hong Tao and Chenping Hou},
  doi          = {10.1109/TIP.2025.3607610},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Streaming view classification with noisy label},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-free object detection with detection transformer. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607621'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Source-Free Object Detection (SFOD) enables knowledge transfer from a source domain to an unsupervised target domain for object detection without access to source data. Most existing SFOD approaches are either confined to conventional object detection (OD) models like Faster R-CNN or designed as general solutions without tailored adaptations for novel OD architectures, especially Detection Transformer (DETR). In this paper, we introduce Feature Reweighting ANd Contrastive Learning NetworK (FRANCK), a novel SFOD framework specifically designed to perform query-centric feature enhancement for DETRs. FRANCK comprises four key components: (1) an Objectness Score-based Sample Reweighting (OSSR) module that computes attention-based objectness scores on multi-scale encoder feature maps, reweighting the detection loss to emphasize less-recognized regions; (2) a Contrastive Learning with Matching-based Memory Bank (CMMB) module that integrates multi-level features into memory banks, enhancing class-wise contrastive learning; (3) an Uncertainty-weighted Query-fused Feature Distillation (UQFD) module that improves feature distillation through prediction quality reweighting and query feature fusion; and (4) an improved self-training pipeline with a Dynamic Teacher Updating Interval (DTUI) that optimizes pseudo-label quality. By leveraging these components, FRANCK effectively adapts a source-pretrained DETR model to a target domain with enhanced robustness and generalization. Extensive experiments on several widely used benchmarks demonstrate that our method achieves state-of-the-art performance, highlighting its effectiveness and compatibility with DETR-based SFOD models.},
  archive      = {J_TIP},
  author       = {Huizai Yao and Sicheng Zhao and Shuo Lu and Hui Chen and Yangyang Li and Guoping Liu and Tengfei Xing and Chenggang Yan and Jianhua Tao and Guiguang Ding},
  doi          = {10.1109/TIP.2025.3607621},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Source-free object detection with detection transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMCFuse: A unified multiple complex scenes infrared and visible image fusion framework. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607623'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image fusion has emerged as a prominent research area in computer vision. However, little attention has been paid to complex scenes fusion, leading to sub-optimal results under interference. To fill this gap, we propose a unified framework for infrared and visible images fusion in complex scenes, termed UMCFuse. Specifically, we classify the pixels of visible images from the degree of scattering of light transmission, allowing us to separate fine details from overall intensity. Maintaining a balance between interference removal and detail preservation is essential for the generalization capacity of the proposed method. Therefore, we propose an adaptive denoising strategy for the fusion of detail layers. Meanwhile, we fuse the energy features from different modalities by analyzing them from multiple directions. Extensive fusion experiments on real and synthetic complex scenes datasets cover adverse weather conditions, noise, blur, overexposure, fire, as well as downstream tasks including semantic segmentation, object detection, salient object detection, and depth estimation, consistently indicate the superiority of the proposed method compared with the recent representative methods. Our code is available at https://github.com/ixilai/UMCFuse.},
  archive      = {J_TIP},
  author       = {Xilai Li and Xiaosong Li and Tianshu Tan and Huafeng Li and Tao Ye},
  doi          = {10.1109/TIP.2025.3607623},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {UMCFuse: A unified multiple complex scenes infrared and visible image fusion framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cycle translation-based collaborative training for hyperspectral-RGB multimodal change detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607609'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral image change detection (HSI-CD) benefits from HSIs with continuous spectral bands, which uniquely enables the analysis of more subtle changes. Existing methods have achieved desirable performance relying on multi-temporal homogenous HSIs over the same region, which is generally difficult to obtain in real scenes. HSI-RGB multimodal CD overcomes the constraint of limited HSI availability by incorporating another temporal RGB data, and the combination of advantages within different modalities enhances the robustness of detection results. Nevertheless, due to the different imaging mechanisms between two modalities, existing HSI CD methods cannot be directly applied. In this paper, we propose a cycle translation-based collaborative training (co-training) for HSI-RGB multimodal CD, which achieves cross-modal mutual guidance to collaboratively learn complementary difference information from diverse modalities for identifying changes. Specifically, a cross-modal guided CycleGAN-based image translation module is designed to implement bi-directional image translation, which mitigates modal difference and enables the extraction of information related to land cover changes. Then, a spatial-spectral interactive co-training CD module is proposed to achieve iterative interaction between cross-modal information, which jointly extracts the multimodal difference features to generate the final results. The proposed method outperforms several leading CD methods in extensive experiments carried out on both real and synthetic datasets. In addition, a new public HSI-RGB multimodal dataset along with our code are available at https://github.com/Jiahuiqu/CT2Net.},
  archive      = {J_TIP},
  author       = {Wenqian Dong and Junying Ren and Song Xiao and Leyuan Fang and Jiahui Qu and Yunsong Li},
  doi          = {10.1109/TIP.2025.3607609},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Cycle translation-based collaborative training for hyperspectral-RGB multimodal change detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal evolutionary graph learning for brain network analysis using medical imaging. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607633'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic functional brain network (DFBN) can flexibly describe the time-varying topological connectivity patterns of the brain, and show great potential in brain disease diagnosis. However, most of the existing DFBN analysis methods focus on capturing the dynamic interaction at the brain region level, ignoring the spatio-temporal topological evolution across time windows. Moreover, they are difficult to suppress interfering connections in DFBNs, which leads to a diminished capacity for discerning the intrinsic structures that are intimately linked to brain disorders. To address these issues, we propose a topological evolution graph learning model to capture disease-related spatio-temporal topological features in DFBNs. Specifically, we first take the hubness of adjacent DFBN as the source domain and the target domain in turn, and then use Wasserstein distance (WD) and Gromov-Wasserstein distance (GWD) to capture the brain’s evolution law at the node and edge levels, respectively. Furthermore, we introduce the principle of relevant information to guide the topology evolution graph to learn the structures that are most relevant to brain diseases yet least redundant information between adjacent DFBNs. On this basis, we develop a high-order spatio-temporal model with multi-hop graph convolution to collaboratively extract long-range spatial and temporal dependencies from the topological evolution graph. Extensive experiments show that the proposed method outperforms the current state-of-the-art methods, and can effectively reveal the information evolution mechanism between brain regions across windows.},
  archive      = {J_TIP},
  author       = {Shengrong Li and Qi Zhu and Chunwei Tian and Li Zhang and Bo Shen and Chuhang Zheng and Daoqiang Zhang and Wei Shao},
  doi          = {10.1109/TIP.2025.3607633},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Spatio-temporal evolutionary graph learning for brain network analysis using medical imaging},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast video recoloring via curve-based palettes. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607584'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Color grading, as a crucial step in film post-production, plays an important role in emotional expression and artistic enhancement. Recently, a geometric palette-based approach to video recoloring has been introduced with impressive results. It offers an intuitive interface that allows users to alter the color of a video by manipulating a limited set of representative colors. However, this method has two primary limitations. Firstly, palette extraction is computationally expensive, often taking more than one hour to generate palettes even for medium-length videos, which significantly limits the practical application of color editing for longer videos. Secondly, the palette colors are less representative, and some primary colors may be omitted from the resulting palettes during topological simplification, making it less intuitive in color editing. To overcome these limitations, in this paper, we propose a novel approach to video recoloring. The core of our method is a set of Bézier curves that connect the dominant colors throughout the input video. By slicing these Bézier curves in RGBT space, per-frame palette can be naturally derived. During recoloring, users can select several frames of interest and modify their corresponding palettes to change the color of the video. Our method is simple and intuitive, enabling compelling time-varying recoloring results. Compared to existing methods, our approach is more efficient in palette extraction and can effectively capture the dominant colors of the video. Extensive experiments demonstrate the effectiveness of our method.},
  archive      = {J_TIP},
  author       = {Zheng-Jun Du and Jia-Wei Zhou and Kang Li and Jian-Yu Hao and Zi-Kang Huang and Kun Xu},
  doi          = {10.1109/TIP.2025.3607584},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Fast video recoloring via curve-based palettes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Structure-aware generative point cloud compression for visual perception. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, there has been a rapid growth in applications that rely on point clouds to represent the 3D world, driven by the increasing demand for immersive and other related scenarios. However, compressing the large and high-precision point cloud data efficiently while maintaining high perceptual quality for human vision remains a challenge. To solve the problem, we propose a new structure-aware generative point cloud compression framework for human vision. In the encoder, we focus on information that is more sensitive to the human vision and obtain this type of information from different scale. This allows us to capture structural importance information from global scale and local scale, which are more difficult to reconstruct. For the decoder, we introduce a progressive generative reconstruction approach that utilizes acquired information from the encoder to guide the generation of point cloud surfaces. Moreover, we propose a novel probability cloud-based discriminator. Instead of directly assessing the authenticity of the generated point clouds, our discriminator evaluates the probability distribution of the existence of points within the generated point cloud. This approach reduces the difficulty of discrimination while effectively improving the accuracy of the generator in generating probability distributions. According to the correct probability, we can obtain a high accuracy point cloud by pruning the points with low probability. Through comprehensive experiments, we demonstrate the effectiveness and superiority of our proposed framework in terms of encoding efficiency, high perceptual quality, and generation quality.},
  archive      = {J_TIP},
  author       = {Yichen Zhou and Xinfeng Zhang and Yingzhan Xu and Kai Zhang and Li Zhang and Qingming Huang},
  doi          = {10.1109/TIP.2025.3607630},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Structure-aware generative point cloud compression for visual perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semi-supervised text-based person search. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607637'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-based person search (TBPS) aims to retrieve images of a specific person from a large image gallery based on a natural language description. Existing methods rely on massive annotated image-text data to achieve satisfactory performance in fully-supervised learning. This presents a substantial practical challenge, given the difficulty in obtaining annotated texts for person images. This work undertakes a pioneering initiative to explore TBPS under the semi-supervised setting, where only a limited number of person images are annotated with textual descriptions while the majority of images lack annotations. We present a two-stage basic solution based on generation-then-retrieval for semi-supervised TBPS. The generation stage enriches annotated data by applying an image captioning model to generate pseudo-texts for unannotated images. Later, the retrieval stage performs fully-supervised retrieval learning using the augmented data. Crucially, considering the noise interference of the pseudo-texts on retrieval learning, we propose a noise-robust retrieval framework that enhances the ability of the retrieval model to handle noisy data. The framework integrates two key strategies: Hybrid Patch-Channel Masking (PC-Mask) to refine the model architecture, and Noise-Guided Progressive Training (NP-Train) to enhance the training process. PC-Mask performs masking on the input data at both the patch-level and the channel-level to prevent overfitting noisy supervision. NP-Train introduces a progressive training schedule based on the noise level of pseudo-texts to facilitate noise-robust learning. Extensive experiments on multiple TBPS benchmarks show that the proposed framework achieves promising performance under the semi-supervised setting.},
  archive      = {J_TIP},
  author       = {Daming Gao and Yang Bai and Min Cao and Hao Dou and Mang Ye and Min Zhang},
  doi          = {10.1109/TIP.2025.3607637},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semi-supervised text-based person search},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harmonized domain enabled alternate search for infrared and visible image alignment. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrared and visible image alignment is essential and critical to the fusion and multi-modal perception applications. It addresses discrepancies in position and scale caused by spectral properties and environmental variations, ensuring precise pixel correspondence and spatial consistency. Existing manual calibration requires regular maintenance and exhibits poor portability, challenging the adaptability of multi-modal application in dynamic environments. In this paper, we propose a harmonized representation based infrared and visible image alignment, achieving both high accuracy and scene adaptability. Specifically, with regard to the disparity between multi-modal images, we develop an invertible translation process to establish a harmonized representation domain that effectively encapsulates the feature intensity and distribution of both infrared and visible modalities. Building on this, we design a hierarchical framework to correct deformations inferred from the harmonized domain in a coarse-to-fine manner. Our framework leverages advanced perception capabilities alongside residual estimation to enable accurate regression of sparse offsets, while an alternate correlation search mechanism ensures precise correspondence matching. Furthermore, we propose the first ground truth available misaligned infrared and visible image benchmark for evaluation. Extensive experiments validate the effectiveness of the proposed method against the state-of-the-arts, advancing the subsequent applications further.},
  archive      = {J_TIP},
  author       = {Zhiying Jiang and Zengxi Zhang and Jinyuan Liu},
  doi          = {10.1109/TIP.2025.3607585},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Harmonized domain enabled alternate search for infrared and visible image alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hyperspectral texture metrology based on distance measures in an information-theoretic framework. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608667'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The present work sought to instil metrology in existing hyperspectral texture feature extraction methods. Specifically, we propose distance-based expressions of graylevel cooccurrence matrix (GLCM), local binary pattern (LBP), and Gabor filtering directly computable for hyperspectral images without any pre- or post-processing. At the core of our proposition is Radical of Extended Mean Information for Discrimination (REID), a novel spectral distance with information-theoretic roots. Respecting the physics of spectrum as continuous function of wavelengths, REID is mathematically decomposable into spectral direction and spectral magnitude distances. The resulted feature calculations are fullband (utilizing all wavelengths), yet lightweight and fully interpretable. A similarity measure based on information theory is also justified. Their efficiency is demonstrated in the context of texture classification, content-based image retrieval, and cancer detection in which they consistently outperform existing computations based on dimensionally reduced space using PCA, ICA, and NMF. The propositions could be potentially integrated into machine/deep learning systems towards explainable AI (XAI).},
  archive      = {J_TIP},
  author       = {Rui Jian Chu and Jie Chen and Susanto Rahardja},
  doi          = {10.1109/TIP.2025.3608667},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hyperspectral texture metrology based on distance measures in an information-theoretic framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gradient and structure consistency in multimodal emotion recognition. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608664'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal emotion recognition is a task that integrates text, visual, and audio data to holistically infer an individual’s emotional state. Existing research predominantly focuses on exploiting modality-specific cues for joint learning, often ignoring the differences between multiple modalities under common goal learning. Due to multimodal heterogeneity, common goal learning inadvertently introduces optimization biases and interaction noise. To address above challenges, we propose a novel approach named Gradient and Structure Consistency (GSCon). Our strategy operates at both overall and individual levels to consider balance optimization and effective interaction respectively. At the overall level, to avoid the optimization suppression of a modality on other modalities, we construct a balanced gradient direction that aligns each modality’s optimization direction, ensuring unbiased convergence. Simultaneously, at the individual level, to avoid the interaction noise caused by multimodal alignment, we align the spatial structure of samples in different modalities. The spatial structure of the samples will not differ due to modal heterogeneity, achieving effective inter-modal interaction. Extensive experiments on multimodal emotion recognition and multimodal intention understanding datasets demonstrate the effectiveness of the proposed method. Code is available at https://github.com/ShiQingHongYa/GSCon.},
  archive      = {J_TIP},
  author       = {QingHongYa Shi and Mang Ye and Wenke Huang and Bo Du and Xiaofen Zong},
  doi          = {10.1109/TIP.2025.3608664},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Gradient and structure consistency in multimodal emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-space topological isomorphism and maximization of predictive diversity for unsupervised domain adaptation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608670'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing unsupervised domain adaptation methods rely on explicitly or implicitly aligning the features of source and target domains to construct a domain-invariant space, often using entropy minimization to reduce uncertainty and confusion. However, this approach faces two challenges: 1) Explicit alignment reduces discriminability, while implicit alignment risks pseudo-label noise, making it hard to balance structure preservation and alignment. 2) Sole reliance on entropy minimization can lead to trivial solutions in UDA, where all samples collapse into a single class. To address these issues, we propose Dual-Space Topological Isomorphism and Maximization of Predictive Diversity (DTI-MPD). Topological isomorphism is a continuous, bijective mapping that preserves the topological properties of two spaces, ensuring the global structure and relationships of data remain intact during alignment. Our method aligns source and target domain data in two independent spaces while balancing the effects of entropy minimization through predictive diversity maximization. The core of dual-space topological isomorphism lies in establishing a reversible correspondence between the source and target domains, avoiding information loss during alignment and preserving the global structural and topological characteristics of the data. Meanwhile, predictive diversity maximization mitigates the class collapse caused by entropy minimization, ensuring a more balanced predictive distribution across categories. This approach effectively overcomes the aforementioned issues, enabling better adaptation to new data. Extensive experiments demonstrate that our method achieves state-of-the-art performance on multiple benchmark datasets, validating its effectiveness.},
  archive      = {J_TIP},
  author       = {Mengru Wang and Jinglei Liu},
  doi          = {10.1109/TIP.2025.3608670},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-space topological isomorphism and maximization of predictive diversity for unsupervised domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pull pole points to text contour by magnetism: A real-time scene text detector. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text reading plays a crucial role in scene understanding. As its precondition task, scene text detection has garnered increasing interest from researchers. Segmentation-based text detection methods have gained prominence due to their adaptable pixel-level predictions. Many existing methods predict the shrink mask and utilize the Vatti clipping algorithm to reconstruct text contours. However, the shrink mask only focuses on the global geometry feature and shrinks the same distance everywhere, which neglects local contour information and disrupts the instance shape feature. In addition, the post-processing based on the Vatti clipping algorithm heavily relies on the predictions and is relatively complex, causing suboptimal performance in both detection accuracy and efficiency. To address the above problems, we propose an efficient and effective method named Magnetic Text Detector (MTD), inspired by magnetism. It is constructed by a text representation method flexible mask (FM) and a magnetic pull module (MPM). Unlike the shrink mask and concentric mask, the former concerns the local contours and shrinks unfixed distances on different positions, which avoids the truncation issue while preserving distinctiveness from the text regions. The latter generates magnetic fields and pulls pole points of FM to the text contour by magnetism. This allows accurate reconstruction of text contours, even when predictions deviate from the actual text severely, while saving 50% of the post-processing time approximately. Several ablation studies verify the effectiveness of the proposed FM and MPM. Extensive experiments show that our MTD achieves state-of-the-art (SOTA) methods on multiple datasets from different scenes. The code is available at https://github.com/fengmulin/MTD.},
  archive      = {J_TIP},
  author       = {Xu Han and Chuang Yang and Qi Wang},
  doi          = {10.1109/TIP.2025.3609196},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Pull pole points to text contour by magnetism: A real-time scene text detector},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HUNTNet: Homomorphic unified nexus topology for camouflaged object detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607635'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged object detection (COD) is challenging for both human and computer vision, as targets often blend into the background by sharing similar color, texture, or shape. While many feature enhancement techniques exist, single-view methods tend to overemphasize certain Recognizing that camouflaged objects exhibit different concealment strategies under varying observational perspectives, we propose HUNTNet, a network that establishes a dynamic detection mechanism to decouple target features from RGB images and perform topological decamouflage across multiple homomorphic feature spaces through a unified feature focusing architecture. We adopt PVTv2 as the backbone to extract multi-perspective spatial features. Detail representation is enhanced via a feature module that integrates Dual-Channel Recursive (DCR), Wavelet-Gabor Transform (WGT), and Anisotropic Gradient Responding (AGR), which together improve boundary discrimination and edge contour detection. To further boost performance, the Simplicial Feature Integration (SFI) module recursively fuses multi-layer features, enabling high-resolution focus on target regions. Experiments show that HUNTNet surpasses state-of-the-art methods in both accuracy and generalization, offering a robust solution for COD and improving segmentation in complex scenes. Our code is available at https://github.com/HaolinJi817/HUNTNet.},
  archive      = {J_TIP},
  author       = {Haolin Ji and Fengying Xie and Linpeng Pan and Yushan Zheng and Zhenwei Shi},
  doi          = {10.1109/TIP.2025.3607635},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {HUNTNet: Homomorphic unified nexus topology for camouflaged object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-view alignment learning with hierarchical-prompt for class-imbalance multi-label image classification. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world datasets often exhibit class imbalance across multiple categories, manifesting as long-tailed distributions and few-shot scenarios. This is especially challenging in Class-Imbalanced Multi-Label Image Classification (CI-MLIC) tasks, where data imbalance and multi-object recognition present significant obstacles. To address these challenges, we propose a novel method termed Dual-View Alignment Learning with Hierarchical Prompt (HP-DVAL), which leverages multi-modal knowledge from vision-language pretrained (VLP) models to mitigate the class-imbalance problem in multi-label settings. Specifically, HP-DVAL employs dual-view alignment learning to transfer the powerful feature representation capabilities from VLP models by extracting complementary features for accurate image-text alignment. To better adapt VLP models for CI-MLIC tasks, we introduce a hierarchical prompt-tuning strategy that utilizes global and local prompts to learn task-specific and context-related prior knowledge. Additionally, we design a semantic consistency loss during prompt tuning to prevent learned prompts from deviating from general knowledge embedded in VLP models. The effectiveness of our approach is validated on two CI-MLIC benchmarks: MS-COCO and VOC2007. Extensive experimental results demonstrate the superiority of our method over SOTA approaches, achieving mAP improvements of 10.0% and 5.2% on the long-tailed multi-label image classification task, and 6.8% and 2.9% on the multi-label few-shot image classification task.},
  archive      = {J_TIP},
  author       = {Sheng Huang and Jiexuan Yan and Beiyan Liu and Bo Liu and Richang Hong},
  doi          = {10.1109/TIP.2025.3609185},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-view alignment learning with hierarchical-prompt for class-imbalance multi-label image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NanoHTNet: Nano human topology network for efficient 3D human pose estimation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3608662'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The widespread application of 3D human pose estimation (HPE) is limited by resource-constrained edge devices like Jetson Nano, requiring more efficient models. A key approach to enhancing efficiency involves designing networks based on the structural characteristics of input data. However, effectively utilizing the structural priors in human skeletal inputs remains challenging. To address this, we leverage both explicit and implicit spatio-temporal priors of the human body through innovative model design and a pre-training proxy task. First, we propose a Nano Human Topology Network (NanoHTNet), a tiny 3D HPE network with stacked Hierarchical Mixers to capture explicit features. Specifically, the spatial Hierarchical Mixer efficiently learns the human physical topology across multiple semantic levels, while the temporal Hierarchical Mixer with discrete cosine transform and low-pass filtering captures local instantaneous movements and global action coherence. Moreover, Efficient Temporal-Spatial Tokenization (ETST) is introduced to enhance spatio-temporal interaction and reduce computational complexity significantly. Second, PoseCLR is proposed as a general pre-training method based on contrastive learning for 3D HPE, aimed at extracting implicit representations of human topology. By aligning 2D poses from diverse viewpoints in the proxy task, PoseCLR aids 3D HPE encoders like NanoHTNet in more effectively capturing the high-dimensional features of the human body, leading to further performance improvements. Extensive experiments verify that NanoHTNet with PoseCLR outperforms other state-of-the-art methods in efficiency, making it ideal for deployment on edge devices like the Jetson Nano. Code and models are available at https://github.com/vefalun/NanoHTNet.},
  archive      = {J_TIP},
  author       = {Jialun Cai and Mengyuan Liu and Hong Liu and Shuheng Zhou and Wenhao Li},
  doi          = {10.1109/TIP.2025.3608662},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {NanoHTNet: Nano human topology network for efficient 3D human pose estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic-driven global-local fusion transformer for image super-resolution. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3609106'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image Super-Resolution (SR) has seen remarkable progress with the emergence of transformer-based architectures. However, due to the high computational cost, many existing transformer-based SR methods limit their attention to local windows, which hinders their ability to model long-range dependencies and global structures. To address these challenges, we propose a novel SR framework named Semantic-Driven Global-Local Fusion Transformer (SGLFT). The proposed model enhances the receptive field by combining a Hybrid Window Transformer (HWT) and a Scalable Transformer Module (STM) to jointly capture local textures and global context. To further strengthen the semantic consistency of reconstruction, we introduce a Semantic Extraction Module (SEM) that distills high-level semantic priors from the input. These semantic cues are adaptively integrated with visual features through an Adaptive Feature Fusion Semantic Integration Module (AFFSIM). Extensive experiments on standard benchmarks demonstrate the effectiveness of SGLFT in producing visually faithful and structurally consistent SR results. The code will be available at https://github.com/kbzhang0505/SGLFT.},
  archive      = {J_TIP},
  author       = {Kaibing Zhang and Zhouwei Cheng and Xin He and Jie Li and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3609106},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Semantic-driven global-local fusion transformer for image super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical color constancy via efficient spectral feature extraction. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607631'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an empirical investigation into illuminant estimation using multi-spectral images. Our study emphasizes two key contributions: (1) the utilization of the estimated multi-spectral images and (2) the incorporation of a hierarchical structure. Firstly, exploiting multi-spectral images proves to have a positive influence on illuminant estimation, particularly in scenarios characterized by monochromatic images where conventional color constancy methods face challenges. Our experimental results vividly illustrate the effectiveness of leveraging spectral information in enhancing illuminant estimation. Secondly, the adoption of a hierarchical structure stems from the need for spatial invariance in the task of estimating a global illuminant. To further enhance the performance of the hierarchical structure, we employ a contrastive loss applied to different scaled outputs. This approach demonstrates remarkable effectiveness on our custom dataset, showcasing superior performance compared to the existing methods. In addition, we extend the evaluation to the widely recognized NUS-8 dataset, where the proposed method showcases a notable 26.7% relative improvement over the previous state-of-the-art methods.},
  archive      = {J_TIP},
  author       = {Dong-Keun Han and Dong-Hoon Kang and Jong-Ok Kim},
  doi          = {10.1109/TIP.2025.3607631},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hierarchical color constancy via efficient spectral feature extraction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SRS: Siamese reconstruction-segmentation network based on dynamic-parameter convolution. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607624'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic convolution demonstrates outstanding representation capabilities, which are crucial for natural image segmentation. However, it fails when applied to medical image segmentation (MIS) and infrared small target segmentation (IRSTS) due to limited data and limited fitting capacity. In this paper, we propose a new type of dynamic convolution called dynamic parameter convolution (DPConv) which shows superior fitting capacity, and it can efficiently leverage features from deep layers of encoder in reconstruction tasks to generate DPConv kernels that adapt to input variations. Moreover, we observe that DPConv, built upon deep features derived from reconstruction tasks, significantly enhances downstream segmentation performance. We refer to the segmentation network integrated with DPConv generated from reconstruction network as the siamese reconstruction-segmentation network (SRS). We conduct extensive experiments on seven datasets including five medical datasets and two infrared datasets, and the experimental results demonstrate that our method can show superior performance over several recently proposed methods. Furthermore, the zero-shot segmentation under unseen modality demonstrates the generalization of DPConv. The code is available at: https://github.com/fidshu/SRSNet.},
  archive      = {J_TIP},
  author       = {Bingkun Nian and Fenghe Tang and Jianrui Ding and Jie Yang and Zhonglong Zheng and Shaohua Kevin Zhou and Wei Liu},
  doi          = {10.1109/TIP.2025.3607624},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {SRS: Siamese reconstruction-segmentation network based on dynamic-parameter convolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). COLA: Context-aware language-driven test-time adaptation. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607634'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Test-time adaptation (TTA) has gained increasing popularity due to its efficacy in addressing “distribution shift” issue while simultaneously protecting data privacy. However, most prior methods assume that a paired source domain model and target domain sharing the same label space coexist, heavily limiting their applicability. In this paper, we investigate a more general source model capable of adaptation to multiple target domains without needing shared labels. This is achieved by using a pre-trained vision-language model (VLM), e.g., CLIP, that can recognize images through matching with class descriptions. While the zero-shot performance of VLMs is impressive, they struggle to effectively capture the distinctive attributes of a target domain. To that end, we propose a novel method – Context-aware Language-driven TTA (COLA). The proposed method incorporates a lightweight context-aware module that consists of three key components: a task-aware adapter, a context-aware unit, and a residual connection unit for exploring task-specific knowledge, domain-specific knowledge from the VLM and prior knowledge of the VLM, respectively. It is worth noting that the context-aware module can be seamlessly integrated into a frozen VLM, ensuring both minimal effort and parameter efficiency. Additionally, we introduce a Class-Balanced Pseudo-labeling (CBPL) strategy to mitigate the adverse effects caused by class imbalance. We demonstrate the effectiveness of our method not only in TTA scenarios but also in class generalisation tasks. The source code is available at https://github.com/NUDT-Bai-Group/COLA-TTA.},
  archive      = {J_TIP},
  author       = {Aiming Zhang and Tianyuan Yu and Liang Bai and Jun Tang and Yanming Guo and Yirun Ruan and Yun Zhou and Zhihe Lu},
  doi          = {10.1109/TIP.2025.3607634},
  journal      = {IEEE Transactions on Image Processing},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {COLA: Context-aware language-driven test-time adaptation},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
