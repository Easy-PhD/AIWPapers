<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip">TIP - 8</h2>
<ul>
<li><details>
<summary>
(2025). Action quality assessment via hierarchical pose-guided multi-stage contrastive regression. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613952'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action Quality Assessment (AQA), which aims at the automatic and fair evaluation of athletic performance, has gained increasing attention in recent years. However, athletes are often in rapid movement and the corresponding visual appearance variances are subtle, making it challenging to capture fine-grained pose differences and leading to poor estimation performance. Furthermore, most common AQA tasks, such as diving in sports, are usually divided into multiple sub-actions, each of which contains different durations. However, existing methods focus on segmenting the video into fixed frames, which disrupts the temporal continuity of sub-actions resulting in unavoidable prediction errors. To address these challenges, we propose a novel action quality assessment method through hierarchically pose-guided multi-stage contrastive regression. Firstly, we introduce a multi-scale dynamic visual-skeleton encoder to capture fine-grained spatio-temporal visual and skeletal features. Compared to mask or auxiliary visual features, skeletal features provide a more accurate representation during athletic movements. Then, a procedure segmentation network is introduced to separate different sub-actions and obtain segmented features. Afterwards, the segmented visual and skeletal features are both fed into a multi-modal fusion module as physics structural priors, to guide the model in learning refined activity similarities and variances. Finally, a multi-stage contrastive learning regression approach is employed to learn discriminative representations and output prediction results. In addition, we introduce a newly-annotated FineDiving-Pose Dataset to improve the current low-quality human pose labels. In experiments, the results on FineDiving and MTL-AQA datasets demonstrate the effectiveness and superiority of our proposed approach. Our source code and dataset are available at https://github.com/Lumos0507/HP-MCoRe.},
  archive      = {J_TIP},
  author       = {Mengshi Qi and Hao Ye and Jiaxuan Peng and Huadong Ma},
  doi          = {10.1109/TIP.2025.3613952},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Action quality assessment via hierarchical pose-guided multi-stage contrastive regression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AGHL: Anchor-guided point cloud registration network with hybrid local feature perception. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613987'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud registration, which estimates a rigid transformation matrix between two point clouds, is a fundamental process in numerous applications. While existing detector-free techniques present exceptional performance, they overlook the extraction of hybrid local features that capture correlations between points and their neighbours, thereby limiting the quality of point cloud recognition. Moreover, these approaches typically treat point clouds as sequential data and employ the transformer to integrate global context from all points, which inevitably introduces interference from irrelevant regions, hence affecting the registration accuracy. In this work, we propose a novel detector-free approach AGHL to address these challenges. For the first issue, AGHL introduces a hybrid local feature perception module that designs two parallel branches to concurrently extract low-level and high-level local features, which effectively encode the correlations between each point and its neighborhood points in both Euclidean space and high-dimensional feature space. For the second issue, AGHL develops an anchor-guided cross attention that adheres to the local geometric consistency to constrain the network’s attention on reliable anchors, thereby effectively suppressing interference from irrelevant regions. Benefiting from these techniques, AGHL achieves impressive point cloud registration accuracy across all synthetic, indoor, and outdoor datasets. Furthermore, we build an experimental platform and conduct a real-world robot localization experiment, with results showing the strong generalization ability of AGHL.},
  archive      = {J_TIP},
  author       = {Kun Dai and Tao Xie and Zhiqiang Jiang and Ke Wang and Ruifeng Li and Lijun Zhao and Chuqing Cao},
  doi          = {10.1109/TIP.2025.3613987},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {AGHL: Anchor-guided point cloud registration network with hybrid local feature perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BITS: Bit-extendable incremental hashing in open environments. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613924'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing is an effective technique for large-scale image retrieval. However, traditional hashing models typically follow a closed-set assumption, which fails to satisfy the practicality of real-world tasks. In this paper, we explore a meaningful yet overlooked question: is there a hashing paradigm that not only supports rehearsal-free online incremental coding for single-pass data streams but also adapts to potentially expanding concept spaces in open environments? Instead of presetting fixed bit lengths, we suggest adjusting the bit length dynamically based on the number of encountered categories, meanwhile enabling bit extension of existing hash codes to match the adaptive code lengths without knowledge forgetting. Therefore, we propose a Bit-extendable IncremenTal haShing (BITS) method for image retrieval in open environments. Specifically, we identify a blurry incremental setup to better simulate realistic scenarios, revisiting the widely-used data-incremental and class-incremental settings. With this challenging setup, a three-phase framework is designed to efficiently perform incremental hashing, which jointly solves online continual coding and bit extension with adaptive code lengths. Through the well-designed hashing paradigm, BITS achieves comparable performance to offline hashing methods while significantly saving computational resources. Comprehensive experiments on six benchmarks demonstrate the superiority of our BITS in dynamic scenarios. The source code is available at https://github.com/yxinwang/BITS.},
  archive      = {J_TIP},
  author       = {Yongxin Wang and Zhen-Duo Chen and Xin Luo and Xin-Shun Xu},
  doi          = {10.1109/TIP.2025.3613924},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BITS: Bit-extendable incremental hashing in open environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust reversible watermarking with invisible distortion against VAE watermark removal. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613958'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal Moment-based Robust Reversible Watermarking (OM-RRW) is crucial for intellectual property protection, providing the dual benefits of robustness and reversibility. However, OM-RRW embeds watermarks into visually sensitive global low-frequency features, which easily leads to ring-like distortions that expose watermark locations, making them vulnerable to removal through image inpainting. To address this issue, this paper makes the first attempt to introduce an innovative strategy to eliminate these visible distortions, thereby overcoming OM-RRW’s inherent limitations. The strategy innovates on two fronts: first, it customizes varying embedding step sizes based on the stability differences of moment values to minimize distortion; second, it designs a texture-aware adaptive basis function fine-tuning strategy. This strategy adjusts the representation capability of the basis functions in different regions based on the human eye’s sensitivity to various texture areas, helping to avoid visible ring-like distortions. The performance of the proposed method is evaluated using Polar Harmonic Transform (PHT) moments, comprising three moments that exhibit remarkable performance in existing OM-RRW methods. Extensive experiments show that the proposed method can embed 128-bit watermarks with no visible distortions while minimizing the loss of robustness. In addition, this paper finds that OM-RRW demonstrates satisfactory robustness against VAE watermark removal attacks.},
  archive      = {J_TIP},
  author       = {Bobiao Guo and Ping Ping and Fan Liu and Feng Xu},
  doi          = {10.1109/TIP.2025.3613958},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Robust reversible watermarking with invisible distortion against VAE watermark removal},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimized vessel segmentation: A structure-agnostic approach with small vessel enhancement and morphological correction. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3607583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate segmentation of blood vessels is essential for various clinical assessments and postoperative analyses. However, the inherent challenges of vascular imaging—such as sparsity, fine granularity, low contrast, data distribution variability, and the critical need for preserving topological integrity—make generalized vessel segmentation particularly complex. While specialized segmentation methods have been developed for specific anatomical regions, their over-reliance on tailored models hinders broader applicability and generalization. General-purpose segmentation models introduced in medical imaging often fail to address critical vascular characteristics, including the connectivity of segmentation results. In this study, we propose OVS-Net, an optimized vessel segmentation framework designed to generalize across diverse vessel structures and imaging modalities. It introduces a dual-branch architecture design for improving small vessel segmentation and a morphology-aware correction module to preserve vascular topology and connectivity. We compiled a comprehensive multi-modality dataset from 17 sources to train and benchmark the proposed OVS-Net against 6 SAM-based methods and 17 expert models under various conditions. The results demonstrate that our approach achieves superior segmentation accuracy, generalization, and a 34.6% improvement in connectivity, underscoring its potential for clinical applications. The code and dataset will be released at github.com.},
  archive      = {J_TIP},
  author       = {Dongning Song and Weijian Huang and Jiarun Liu and Md Jahidul Islam and Hao Yang and Shuqiang Wang and Hairong Zheng and Shanshan Wang},
  doi          = {10.1109/TIP.2025.3607583},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Optimized vessel segmentation: A structure-agnostic approach with small vessel enhancement and morphological correction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-space normalizing flow for unsupervised video anomaly detection. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3614006'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional reconstruction-based video anomaly detection (VAD) methods implicitly model normality in latent spaces, which is limited by the generalization ability of latent features. Normalizing Flow (NF)-based methods have been introduced to address this issue, as they explicitly model the distribution of input data and achieve significant performance in VAD. However, existing NF-based methods are confined to Euclidean space, limiting their ability to model action hierarchies. While effective at capturing local joint dynamics and short-term temporal variations, they fail to encode kinematic dependencies and long-term pose evolution, ultimately struggling to discern ambiguous anomalies that deviate minimally from normal motion. In contrast, hyperbolic representation learning, with its ability to model hierarchical and complex relationships among actions, offers a promising solution to enhance the discriminative power between similar skeletal actions. Motivated by this, we propose a novel Dual-Space Normalizing Flow (DSNF) method. Specifically, we design a Dual-Space Parallel Graph Convolutional Network (DSPGCN) that synergistically integrates the strengths of both Euclidean and hyperbolic geometries to simultaneously capture local detail features of poses and intrinsic hierarchical relationships of actions. To enhance the model’s focus on discriminative features, we design an Adaptive Weighted Approximation Mass (AWAM) loss that dynamically adjusts weights to impose stronger constraints on regions with low discriminability in the dual space, encouraging the model to focus more on key discriminative features in hyperbolic space that reflect complex relationships between actions. Extensive experiments on public datasets demonstrate the effectiveness and robustness of our method in various VAD scenarios.},
  archive      = {J_TIP},
  author       = {Jiaxu Leng and Yumeng Zhang and Mingpi Tan and Changjiang Kuang and Zhanjie Wu and Ji Gan and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3614006},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dual-space normalizing flow for unsupervised video anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring vision-based active 3D object detection by informativeness characterization. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3613927'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based 3D object detection (3DOD) gains lots of attention due to its low cost for deployment compared to Lidar-based tasks, while it suffers from labor-expensive data annotations. At the same time, active learning (AL) has shown great potential in reducing annotation costs in related tasks, which can maximize model performance within very limited labeled data. In this paper, we explore active learning for vision-based 3DOD for the first time. Inspired by the entropy analysis, we involve three concerns to characterize the sample informativeness: sample diversity in input space, feature informativeness in BEV space, and result distribution in prediction space. Based on these concerns, we propose a novel AL framework named HMAD, which utilizes Height Modeling and Adaptive Diversity-based sampling for comprehensive informativeness characterization. In HMAD, we first propose a novel height-guided adversarial module in BEV space, which measures the informativeness of height modeling for 2D-to-3D mapping in an adversarial manner. Furthermore, Budget-aware SpatioTemporal diversity Sampling (BSTS) and Class Balance Sampling (CBS) are proposed to adaptively measure the sample informativeness in input and prediction space, respectively. Finally, the three components are integrated into a two-stage sampling strategy, with which the most informative samples can be selected and annotated for the next iteration. Experiments evidence that HMAD achieves comparable performances by only using 50% annotated training data, and can generalize well on different conditions.},
  archive      = {J_TIP},
  author       = {Ruixiang Li and Yiming Wu and Yehao Lu and Xuewei Li and Xian Wang and Xiubo Liang and Xi Li},
  doi          = {10.1109/TIP.2025.3613927},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Exploring vision-based active 3D object detection by informativeness characterization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised discovery of cross-lingual shared knowledge for continual text recognition. <em>TIP</em>, 1. (<a href='https://doi.org/10.1109/TIP.2025.3614773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental multilingual text recognition (IMLTR) aims to advance continual learning by retaining knowledge from previously learned languages while adapting to new ones. Existing methods typically perform under a constrained assumption that each text instance originates from a specific single-language domain. However, this assumption is inaccurate in multilingual scenarios, as it overlooks the inherent cross-lingual knowledge, i.e., the incremental sharing problem. To address this issue, we propose a novel self-supervised cross-lingual knowledge discovery framework, CrossKnow, tailored for IMLTR tasks. Specifically, an innovative shared knowledge discovery strategy is developed to identify potential shared knowledge by leveraging prediction consistency across multiple recognizers, thus eliminating the reliance on language labels of all characters. Building upon this shared knowledge, we further design a multi-granularity, multi-task language domain discriminator to capture dependency relationships among incremental languages, which could adequately guide the hierarchical sequence decoding. By mining shared knowledge, CrossKnow can not only mitigate the forgetting of old knowledge but also efficiently achieve cross-lingual knowledge transfer, thereby promoting the continual learning of incremental multilingual text recognition models. Experiments on two widely used datasets, MLT17 and MLT19, demonstrate the superiority of CrossKnow. Compared to methods that leverage additional language supervision of characters, CrossKnow achieves competitive performance while eliminating storage overhead and improving computation efficiency.},
  archive      = {J_TIP},
  author       = {Xiao-Qian Liu and Zhen-Duo Chen and Xin Luo and Xin-Shun Xu},
  doi          = {10.1109/TIP.2025.3614773},
  journal      = {IEEE Transactions on Image Processing},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Self-supervised discovery of cross-lingual shared knowledge for continual text recognition},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
