<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TBD</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tbd">TBD - 11</h2>
<ul>
<li><details>
<summary>
(2025). Fast convergent federated learning via decaying SGD updates. <em>TBD</em>, 1-14. (<a href='https://doi.org/10.1109/TBDATA.2025.3618454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated Learning (FL), a groundbreaking approach for collaborative model training across decentralized devices, maintains data privacy while constructing a decent global machine learning model. Conventional FL methods typically demand more communication rounds to achieve convergence in non-Independent and non-Identically Distributed (non-IID) data scenarios due to their reliance on fixed Stochastic Gradient Descent (SGD) updates at each Communication Round (CR). In this paper, we introduce a novel strategy to expedite the convergence of FL models, inspired by the insights from McMahan etal.'s seminal work. We focus on FL convergence via traditional SGD decay by introducing a dynamic adjusting mechanism for local epochs and local batch size. Our method adapts the decay of SGD updates during the training process, akin to decaying learning rates in classical optimization. Particularly, by adaptively reducing local epochs and increasing local batch size using their ongoing values and the CR as the model progresses, our method enhances convergence speed without compromising accuracy, specifically by effectively addressing challenges posed by non-IID data. We provide theoretical results of the benefits of the dynamic decay of SGD updates in FL scenarios. We demonstrate our method's consistent outperformance regarding the global model's communication speedup and convergence behavior through comprehensive experiments.},
  archive  = {J},
  author   = {Md Palash Uddin and Yong Xiang and Mahmudul Hasan and Yao Zhao and Youyang Qu and Longxiang Gao},
  doi      = {10.1109/TBDATA.2025.3618454},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  pages    = {1-14},
  title    = {Fast convergent federated learning via decaying SGD updates},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavior habits enhanced intention learning for session based recommendation. <em>TBD</em>, 1-13. (<a href='https://doi.org/10.1109/TBDATA.2025.3618463'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multi-behavior Session Based Recommendations (MBSBRs) have achieved remarkable results due to considering behavioral heterogeneity in sessions. Yet most existing works only consider binary or continuous behavior dependencies and aim to predict the next item under the target behavior, neglecting users' inherent behavior habits, resulting in learning inaccurate intentions. To tackle the above issues, we propose a novel Behavior Habits Enhanced Intention Learning framework for Session Based Recommendation (BHSBR). Specifically, we focus on the next item recommendation and design a global item transition graph to learn the behavior-aware semantic relationships between items, in order to mine the underlying similarity between items beyond the session. In addition, we construct a hypergraph to extract the diverse behavior habits of users and break through the limitations of temporal relationships in the session. Compared to the existing works, our behavior habit learning method learns behavior dependencies at the user level, which could capture the user's more accurate long-term intentions and reduce the impact of noise behaviors. Extensive experiments on three datasets demonstrate that the performance of our proposed BHSBR is superior to SOTA. Further ablation experiments fully illustrate the effectiveness of our various modules.},
  archive  = {J},
  author   = {Zhida Qin and Wenhao Xue and Haotian He and Haoyao Zhang and Shixiao Yang and Enjun Du and Tianyu Huang and John C.S. Lui},
  doi      = {10.1109/TBDATA.2025.3618463},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  pages    = {1-13},
  title    = {Behavior habits enhanced intention learning for session based recommendation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parallel core decomposition of temporal graphs. <em>TBD</em>, 1-12. (<a href='https://doi.org/10.1109/TBDATA.2025.3618443'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {To underscore the significance of the interactive frequency among diverse vertices in each snapshot, prior research has extended the $k$-core of general graphs to the $(k,h)$-core of temporal graphs, in which each vertex has at least $k$ neighbors and is connected by at least $h$ edges to each of these neighbors. Due to the numerous combinations of $k$ and $h$, the quantity of $(k,h)$-cores is substantial, which necessitates considerable time and space for querying and decomposition. As a temporal graph evolves, for instance, with edges being inserted or removed from the previous snapshot, the affected $(k,h)$-cores must also be updated to reflect the latest structure. To address these challenges, we initially develop a novel $(k,h)$-core storage index that exhibits excellent query performance while consuming linear space regarding the graph size. Subsequently, we design an efficient decomposition algorithm to extract $(k,h)$-cores from a snapshot. Following this, we offer two maintenance algorithms to manage temporal graph evolution. Finally, we validate the effectiveness of our proposed methods on actual temporal graphs. Experimental results indicate that our methods surpass existing techniques by two orders of magnitude.},
  archive  = {J},
  author   = {Wen Bai and Yufeng Wang and Yuncheng Jiang and Di Wu},
  doi      = {10.1109/TBDATA.2025.3618443},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  pages    = {1-12},
  title    = {Parallel core decomposition of temporal graphs},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing recommendations with knowledge-guided interest contrast. <em>TBD</em>, 1-12. (<a href='https://doi.org/10.1109/TBDATA.2025.3618449'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the digital age, the overwhelming amount of information necessitates advanced recommendation systems to deliver personalized content. However, these systems face significant challenges, such as sparse user-item interactions and long-tail bias. Recent studies construct structural learning or self-supervised learning on the interaction graph achieving a positive impact on alleviating the problems, but the interaction data itself may be far too little to solve the problems. While knowledge graphs (KGs) offer a promising solution by providing semantic depth to recommendations, their integration often introduces noise from redundant knowledge. Addressing these critical gaps, this study proposes a knowledge-guided interest contrast (KGIC) to enhance recommendations, which innovatively harmonizes collaborative filtering with semantic insights from KG. The KGIC model introduces three key innovations: (1) a knowledge filtering mechanism that selectively leverages interest-relevant signals from the knowledge graph to encode interest and avoid redundant knowledge interference; (2) an adaptive graph augmentation strategy that enhances the interaction graph based on semantic-aware interest propagation and interaction intensity estimation; and (3) a self-supervised contrastive learning task that mitigates long-tail bias and sparsity issues by homogenizing the embedding distribution between augmented views. The extensive evaluation reveals the superiority of KGIC with knowledge filtering and graph augmentation for recommendation.},
  archive  = {J},
  author   = {Meng Jian and Ruoxi Li and Yulong Bai and Ge Shi},
  doi      = {10.1109/TBDATA.2025.3618449},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  pages    = {1-12},
  title    = {Enhancing recommendations with knowledge-guided interest contrast},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLIP2LE: A label enhancement fair representation method via CLIP. <em>TBD</em>, 1-12. (<a href='https://doi.org/10.1109/TBDATA.2025.3618450'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Label enhancement is a novel label shift strategy that aims to integrate the feature space with the logical label space to obtain a high-quality label distribution. This label distribution can serve as a soft target for algorithmic learning, akin to label smoothing, thereby enhancing the performance of various learning paradigms including multi-label learning, single positive multi-label learning, and partial-label learning. However, limited by dataset type and annotation inaccuracy, the same label enhancement algorithm on different datasets struggles to achieve consistent performance, for reasons derived from the following two insights: 1) Differential Contribution of Feature Space and Logical Label Space: The feature space and logical label space of different datasets contribute differently to generating an accurate label distribution; 2) Presence of Noise and Incorrect Labels: Some datasets contain noise and inaccurately labeled samples, leading to divergent outputs for similar inputs. To address these challenges, we propose leveraging CLIP (Contrastive Language-Image Pre-training) as a foundational strategy, treating the feature space and the logical label space as two distinct modalities. By recoding these modalities before applying the label enhancement algorithm, we aim to achieve a fair and robust representation. In addition, we further explained the reasonableness of our motives in the discussion session. Extensive experimental results demonstrate the effectiveness of our approach to help existing label enhancement algorithms improve their performance on several benchmarks.},
  archive  = {J},
  author   = {Pu Wang and YinSong Xiong and Zhuoran Zheng},
  doi      = {10.1109/TBDATA.2025.3618450},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  pages    = {1-12},
  title    = {CLIP2LE: A label enhancement fair representation method via CLIP},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STGym: A modular benchmark for spatio-temporal networks with a survey and case study on traffic forecasting. <em>TBD</em>, 1-20. (<a href='https://doi.org/10.1109/TBDATA.2025.3618482'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The rapid advancement of spatio-temporal domain has led to a surge of novel models. These models can typically be decomposed into different modules, such as various types of graph neural networks and temporal networks. Notably, many of these models share identical or similar modules. However, the existing literature often relies on fragmented and self-constructed experimental frameworks. This fragmentation hinders a comprehensive understanding of model interrelationships and makes fair comparisons difficult due to inconsistent training and evaluation processes. To address these issues, we introduce Spatio-Temporal Gym (STGym), an innovative modular benchmark that provides a platform for exploring various spatio-temporal models and supports research for developers. The modular design of STGym facilitates an in-depth analysis of model components and promotes the seamless adoption and extension of existing methods. By standardizing the training and evaluation processes, STGym ensures reproducibility and scalability, enabling fair comparisons across different models. In this paper, we use traffic forecasting, a popular research topic in the spatio-temporal domain, as a case to demonstrate the capabilities of the STGym. Our detailed survey systematically utilizes the modular framework of STGym to organize key modules into various models, thereby facilitating deeper insights into their structures and mechanisms. We also evaluate 18 models on six widely used traffic forecasting datasets and analyze critical hyperparameters to reveal their impact on performance. This study provides valuable resources and insights for developers and researchers. The source codes are available at https://github.com/JW-Shen/STGym.},
  archive  = {J},
  author   = {Chun-Wei Shen and Jia-Wei Jiang and Hsun-Ping Hsieh},
  doi      = {10.1109/TBDATA.2025.3618482},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  pages    = {1-20},
  title    = {STGym: A modular benchmark for spatio-temporal networks with a survey and case study on traffic forecasting},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Differential encoding for improved representation learning over graphs. <em>TBD</em>, 1-12. (<a href='https://doi.org/10.1109/TBDATA.2025.3618447'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Combining the message-passing paradigm with the global attention mechanism has emerged as an effective framework for learning over graphs. The message-passing paradigm and the global attention mechanism basically generate embeddings of nodes by taking the sum of information from a node's local neighbourhood and from the entire graph, respectively. However, this simple summation aggregation approach fails to distinguish between the information from a node itself or from the node's neighbours. Therefore, there exists information lost at each layer of embedding generation, and this information lost could be accumulated and become more serious in deeper model layers. In this paper, we present a differential encoding method to address the issue of information lost. Instead of simply taking the sum to aggregate local or global information, we explicitly encode the difference between the information from a node itself and that from the node's local neighbours (or from the rest of the entire graph nodes). The obtained differential encoding is then combined with the original aggregated representation to generate the updated node embedding. By combining differential encodings, the representational ability of generated node embeddings is improved, and therefore the model performance is improved. The differential encoding method is empirically evaluated on different graph tasks on seven benchmark datasets. The results show that it is a general method that improves the message-passing update and the global attention update, advancing the state-of-the-art performance for graph representation learning on these benchmark datasets.},
  archive  = {J},
  author   = {Haimin Zhang and Jiaohao Xia and Min Xu},
  doi      = {10.1109/TBDATA.2025.3618447},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  pages    = {1-12},
  title    = {Differential encoding for improved representation learning over graphs},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging user dynamic preferences: A unified bridge-based diffusion model for next POI recommendation. <em>TBD</em>, 1-15. (<a href='https://doi.org/10.1109/TBDATA.2025.3618453'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Next POI recommendation plays a crucial role in delivering personalized location-based services, but it faces significant challenges in capturing complex user behavior and adapting to dynamic interest distributions. Most methods often provide insufficient modeling of implicit features in user trajectories, such as directional transitions and latent edge relationships, which are essential for understanding user behavior. Moreover, existing diffusion models, constrained by Gaussian priors, struggle to handle the diverse and evolving nature of user preferences. The lack of a unified scheduling for noise and sampling also limits the flexibility of diffusion models. In this paper, we propose a Unified Bridge-based Diffusion model (UB-Diff) for the next POI recommendation. UB-Diff incorporates a direction-aware POI transition graph learning, which jointly captures spatio-temporal and directional features. To overcome the limitations of Gaussian priors, we introduce a bridge-based diffusion POI generative model. It can achieve distribution translation from the user's historical distribution to the target distribution by learning a bridge to associate user behavior with POI recommendation, adapting to dynamic user interests. In the end, we design a novel intermediate function to unify the diffusion process, enabling precise control over noise scheduling and modular optimization. Extensive experiments on five real-world datasets demonstrate the superiority of UB-Diff over advanced baseline methods. Our code is available at https://github.com/JKZuo/UBDiff.},
  archive  = {J},
  author   = {Jiankai Zuo and Zihao Yao and Yaying Zhang},
  doi      = {10.1109/TBDATA.2025.3618453},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  pages    = {1-15},
  title    = {Bridging user dynamic preferences: A unified bridge-based diffusion model for next POI recommendation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ACJoin: A low-latency multi-table join order selection model with minimum cost using asynchronous advantage actor-critic. <em>TBD</em>, 1-16. (<a href='https://doi.org/10.1109/TBDATA.2025.3618478'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Join order selection is one of the most challenging problems in query optimization and plays an essential role in providing high query performance in Big Data management. Currently, researchers have applied deep reinforcement learning methods, for example, Rejoin and DQ, to join order selection in order to obtain high query performance. However, Rejoin and DQ cannot capture the structural characteristic of the join tree, which may lead to similar encoding structure for different execution plans. To tackle these challenges, we propose a new learning optimizer called ACJoin (asynchronous advantage Actor-Critic for multi-table Join order selection). ACJoin employs a new encoding method to capture the structural characteristics of the join tree through integrating GRU (Gated Recurrent Unit). In particular, ACJoin can distinguish different execution plans. It uses A3C (Asynchronous Advantage Actor-Critic) to guide the join order selection and reduce the time taken to find the best query plan with the minimum cost. Compared with existing search strategies, ACJoin can find the globally optimal solution with efficient and stable query performance. Extensive experiments are conducted on the real JOB and the synthetic TPC-H datasets. The results show that ACJoin outperforms the state-of-the-art join order selection methods and DRL Deep Reinforcement Learning)-based methods in cost and latency.},
  archive  = {J},
  author   = {Shaojie Qiao and Chenxu Liu and Nan Han and Kanglei Xu and Ruiwei Gao and Xindong Wu},
  doi      = {10.1109/TBDATA.2025.3618478},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  pages    = {1-16},
  title    = {ACJoin: A low-latency multi-table join order selection model with minimum cost using asynchronous advantage actor-critic},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Two-step nyström sampling for large-scale kernel approximation. <em>TBD</em>, 1-13. (<a href='https://doi.org/10.1109/TBDATA.2025.3618472'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Nystrom approximation is one of the most popular approximation methods to accelerate kernel analysis on largescale data sets. Nystrom employs one single landmark set to ¨ obtain eigenvectors (low-rank decomposition) and projects the entire data set to the eigenvectors (embedding). Most existing methods focus on accelerating landmark selection. For extremely large-scale data sets, however, the embedding time cost, rather than that of low-rank decomposition, is critical. In addition, both accuracy and embedding time cost are dominated by the landmark set size. As a result, using more landmarks is the only way to improve accuracy at the cost of extremely high embedding costs. In this paper, we propose a method for the first time to decouple embedding cost from that of low-rank decomposition. We first obtain the eigenvectors from a large landmark set for a low error, and then optimize a small landmark set that minimizes the landmark-set-embedding error to ensure a low embedding cost. In return, our accuracy is close to that of the large landmark set but the small one dominates the embedding time cost. Our method can deal with popular kernels and be plugged into most existing methods. Experimental results demonstrate the superiority of the proposed method.},
  archive  = {J},
  author   = {Li He and Hong Zhang},
  doi      = {10.1109/TBDATA.2025.3618472},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  pages    = {1-13},
  title    = {Two-step nyström sampling for large-scale kernel approximation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lightweight scope integration network for rail surface defect detection. <em>TBD</em>, 1-9. (<a href='https://doi.org/10.1109/TBDATA.2025.3618444'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Rail-surface defect detection (RSDD) is a key technology for ensuring the safety and efficiency of railroad transportation. Existing models enhance the robustness in complex scenarios by using complementary information from visible light (RGB) image and depth data. However, incorporating depth data increases the computational complexity, which consumes more resources, increases the risk of overfitting, and decreases the model efficiency. Optimizing and streamlining multimodal data processing remains challenging. To address this issue and enable fast and accurate RSDD, this study introduces a novel lightweight scope integration network (LSINet). This method efficiently and precisely fuses the features using a lightweight double-context-aware module and refines the image quality using an adaptive Markov field smoothing module in a hierarchical decoding process. A lightweight two-dimensional scanning method that captures long-range dependencies and improves computational efficiency. We integrate this approach with local range extremes to enhance the multimodal feature fusion. Furthermore, to refine the defect edges and improve the detection accuracy, we integrate the Markov random field module into the defect segmentation and optimization using its ability to model interpixel spatial correlation, particularly for small and ambiguous defects. In designing the model, we focused on optimizing the number of parameters (e.g., computational resources). Experimental evaluations using various rail surface images demonstrated that the proposed method enhanced the defect detection accuracy and recall while reaching fast detection speeds. According to extensive experiments conducted using the industrial RGB-D dataset (i.e., NEU RSDDS-AUG), LSINet outperformed 15 state-of-the-art methods with only 27.6 million parameters. Code and results are publicly available at https://github.com/Wuyue15/LSINet.},
  archive  = {J},
  author   = {Wujie Zhou and Yue Wu and Fangfang Qiang and Weiqing Yan},
  doi      = {10.1109/TBDATA.2025.3618444},
  journal  = {IEEE Transactions on Big Data},
  month    = {10},
  pages    = {1-9},
  title    = {Lightweight scope integration network for rail surface defect detection},
  year     = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
