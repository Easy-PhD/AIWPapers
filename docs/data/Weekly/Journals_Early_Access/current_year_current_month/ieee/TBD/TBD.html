<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TBD</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tbd">TBD - 20</h2>
<ul>
<li><details>
<summary>
(2025). FinMem: A performance-enhanced LLM trading agent with layered memory and character design. <em>TBD</em>, 1-18. (<a href='https://doi.org/10.1109/TBDATA.2025.3593370'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We introduce FinMem, a novel Large Language Models (LLM)-based agent framework for financial trading, designed to address the need for automated systems that can transform real-time data into executable decisions. FinMem comprises three core modules: Profile for customizing agent characteristics, Memory for hierarchical financial data assimilation, and Decision-making for converting insights into investment choices. The Memory module, which mimics human traders' cognitive structure, offers interpretability and real-time tuning while handling the critical timing of various information types. It employs a layered approach to process and prioritize data based on its timeliness and relevance, ensuring that the most recent and impactful information is given appropriate weight in decision-making. FinMem's adjustable cognitive span allows retention of critical information beyond human limits, enabling it to balance historical patterns with current market dynamics. This framework facilitates self-evolution of professional knowledge, agile reactions to investment cues, and continuous refinement of trading decisions in financial environments. When compared against advanced algorithmic agents using a large-scale real-world financial dataset, FinMem demonstrates superior performance across classic metrics like Cumulative Return and Sharpe ratio. Further tuning of the agent's perceptual span and character setting enhances its trading performance, positioning FinMem as a cutting-edge solution for automated trading.},
  archive  = {J},
  author   = {Yangyang Yu and Haohang Li and Zhi Chen and Yuechen Jiang and Yang Li and Jordan W. Suchow and Denghui Zhang and Khaldoun Khashanah},
  doi      = {10.1109/TBDATA.2025.3593370},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-18},
  title    = {FinMem: A performance-enhanced LLM trading agent with layered memory and character design},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Document-image perceptual hashing for content authentication. <em>TBD</em>, 1-15. (<a href='https://doi.org/10.1109/TBDATA.2025.3596854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper proposes an end-to-end two-branch network for a document image perceptual hashing scheme, where the two branches focus on image visual features and text features, respectively. Existing perceptual hashing schemes cannot solve the problem of the tiny proportion of text tampering detection, while simple text detection is unable to solve the problem of background region-aware matching. To address these issues, we extract text information via optical character recognition (OCR) and then generate the text features using the bidirectional encoder representations from Transformers (BERT). Visual features of the image are extracted from the local and global features of the image using ResNet and Vision Transformer cascades, and then fused to generate the final hash sequence through the fully connected layer. The proposed network considers both image visual features and textual information to verify that the document image has not been tampered with. In our network, the OCR module enables accurate and intelligent text detection and recognition, particularly for dealing with text tampering that has only been conducted in tiny portions. It also provides more efficient and robust text recognition services. Experimental results show that the proposed hashing scheme is robust and discriminative in document images.},
  archive  = {J},
  author   = {Xiaotong Situ and Tong Liu and Heng Yao and Chuan Qin and Xinpeng Zhang},
  doi      = {10.1109/TBDATA.2025.3596854},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-15},
  title    = {Document-image perceptual hashing for content authentication},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PatchAD: A lightweight patch-based MLP-mixer for time series anomaly detection. <em>TBD</em>, 1-15. (<a href='https://doi.org/10.1109/TBDATA.2025.3596745'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Time series anomaly detection is a pivotal task in data analysis, yet it poses the challenge of discerning normal and abnormal patterns in label-deficient scenarios. While prior studies have largely employed reconstruction-based approaches, which limit the models' representational capacities. Moreover, existing deep learning-based methods are not sufficiently lightweight. Addressing these issues, we present PatchAD, our novel, highly efficient multiscale patch-based MLP-Mixer architecture that utilizes contrastive learning for representation extraction and anomaly detection. With its four distinct MLP Mixers and innovative dual project constraint module, PatchAD mitigates potential model degradation and offers a lightweight solution, requiring only 0.403M parameters. Its efficacy is demonstrated by state-of-the-art results across 8 datasets sourced from different application scenarios, outperforming over 30 comparative algorithms. PatchAD significantly improves the classical F1 score by 6.84%, the Aff-F1 score by 4.27%, and the V-ROC by 2.49%. Simultaneously, an in-depth analysis of the mechanisms underlying PatchAD has been conducted from both theoretical and experimental perspectives, validating the design motivations of the model. The code is publicly available at https://github.com/EmorZz1G/PatchAD.},
  archive  = {J},
  author   = {Zhijie Zhong and Zhiwen Yu and Yiyuan Yang and Weizheng Wang and Kaixiang Yang and C. L. Philip Chen},
  doi      = {10.1109/TBDATA.2025.3596745},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-15},
  title    = {PatchAD: A lightweight patch-based MLP-mixer for time series anomaly detection},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Identifying covert channels in blockchain: A case study on bitcoin, zcash, monero and ethereum. <em>TBD</em>, 1-11. (<a href='https://doi.org/10.1109/TBDATA.2025.3594241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Blockchain has been an attractive platform both for covert channel and covert communication. Recent years have witnessed growing researches on achieving covert communication with blockchain applications. Identifying covert channels in blockchain is necessary and important to design blockchain based covert communication schemes. However, there still lacks a systematical method to analyze all possible covert channels in blockchain applications. In the paper, we propose a layer based covert channel identification method to analyze both covert storage channels and covert timing channels in blockchain applications. 11, 15, 14 and 19 new covert channels are identified in Bitcoin, Zcash, Monero and Ethereum with the proposed method, which proves the effectiveness of the method. The method is general and can be applied to identify covert channels in other blockchain applications, which lays foundation for both practical covert communication with blockchain applications and covert communication detection research.},
  archive  = {J},
  author   = {Tao Zhang},
  doi      = {10.1109/TBDATA.2025.3594241},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-11},
  title    = {Identifying covert channels in blockchain: A case study on bitcoin, zcash, monero and ethereum},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comprehensive privacy analysis on recommendation with causal embedding against model inversion attacks. <em>TBD</em>, 1-13. (<a href='https://doi.org/10.1109/TBDATA.2025.3594291'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recommendation systems, the interactions between users and items are influenced by two factors: the user's conformity towards popular items and the user's real interest. Training individual user embeddings and item embeddings to capture these two factors can effectively improve the accuracy of recommendations. However, recommendation systems often exchange item embeddings with third-party servers, which may expose sensitive information to malicious attackers. Specifically, attackers can infer sensitive user information based on published item embeddings and partial public user information. In this paper, we first design a model inversion attack to analyze the influence of conformity item embeddings and interest item embeddings on privacy. This analysis reveals that different item embeddings have varying resistances against inversion attack. Based on the resistance levels of the two item embeddings, we propose a novel adaptive differential privacy protection method that enhances resistance against model inversion attacks while ensuring recommendation accuracy. We conduct experiments on three real datasets, and the results demonstrate the outstanding performance of our method in terms of both recommendation accuracy and resistance to inversion attack.},
  archive  = {J},
  author   = {Hanyang Liu and Yong Wang and Zhiqiang Zhang and Jiangzhou Deng and Yongdong Wang},
  doi      = {10.1109/TBDATA.2025.3594291},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-13},
  title    = {Comprehensive privacy analysis on recommendation with causal embedding against model inversion attacks},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PB-TABL: Task incremental learning strategy via applying piggyback architecture on temporal attention-augmented bilinear networks for financial time-series classification. <em>TBD</em>, 1-13. (<a href='https://doi.org/10.1109/TBDATA.2025.3598730'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {As financial markets continue to grow and generate vast volumes of data, there is a growing need for models capable of being updated incrementally. In this paper, we present PB-TABL, a novel method for financial time-series forecasting that integrates the piggyback (PB) architecture with Temporal Attention-Augmented Bilinear Networks (TABL). The proposed PB-TABL architecture addresses the challenges of task-incremental learning, including catastrophic forgetting, concept drift, and computational efficiency. By adapting pre-trained models through binary masks, PB-TABL enables efficient reuse of models for new tasks, significantly reducing training time and computational costs. We demonstrate the effectiveness of this approach through extensive experiments on large-scale Limit Order Book (LOB) data, where PB-TABL outperforms baseline models in terms of accuracy, F1 score, and overall computational efficiency. Our contributions include formulating the problem of adapting pre-trained neural networks to new financial data, introducing the PB-TABL architecture, and showing its advantages in handling time-series data with temporal dependencies while mitigating the risks of catastrophic forgetting and model degradation. All the data and implemented code are available here: https://github.com/rezapaki1376/PB-TABL.},
  archive  = {J},
  author   = {Reza Paki and Hossein Abbasimehr},
  doi      = {10.1109/TBDATA.2025.3598730},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-13},
  title    = {PB-TABL: Task incremental learning strategy via applying piggyback architecture on temporal attention-augmented bilinear networks for financial time-series classification},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DucDiff: Dual-consistent diffusion for uncertainty-aware information diffusion prediction. <em>TBD</em>, 1-15. (<a href='https://doi.org/10.1109/TBDATA.2025.3598713'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Information diffusion prediction is a vital component for a wide range of social applications, including viral marketing identification and personal recommendation. Prior methods primarily focus on learning target user representation by modeling contextual information from the historical retweet user sequence of a single cascade, overlooking the uncertainties that exist in both historical propagation trajectory and future diffusion trends. In this work, we propose DucDiff, a novel dual-consistent diffusion model for enhancing target user representation used for information diffusion prediction. DucDiff harnesses the distribution generation capability of the diffusion model to generate target user representations from a distributional perspective rather than a fixed vector. Specifically, it captures the multi-latent aspects (i.e., uncertainties) of target user representation from historical and future user sequences, respectively, using disentangled dual denoising modules. Additionally, a shared information bottleneck is designed for the cross-distillation of knowledge between the historical and future denoising modules, eliminating the performance gap between training and inference, while ensuring that future information can be implicitly introduced during the inference phase. Extensive experiments conducted on five datasets demonstrate that DucDiff significantly outperforms state-of-the-art baselines.},
  archive  = {J},
  author   = {Ting Zhong and Wenxue Ye and Shichong Li and Yang Liu and Zhangtao Cheng and Fan Zhou and Xueqin Chen},
  doi      = {10.1109/TBDATA.2025.3598713},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-15},
  title    = {DucDiff: Dual-consistent diffusion for uncertainty-aware information diffusion prediction},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VDDFormer: A variable dependency discrepancy-based transformer for multivariate time series anomaly detection. <em>TBD</em>, 1-14. (<a href='https://doi.org/10.1109/TBDATA.2025.3600004'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The dynamics of multivariate time series (MTS) data are jointly characterized by its nonlinear temporal dependencies and complex variable dependencies, making unsupervised time series anomaly detection a challenging task. Existing methods primarily rely on prediction or reconstruction errors, neglecting the valuable information within the variable dependencies. In this paper, we propose a variable dependency discrepancy-based Transformer (VDDFormer) for unsupervised MTS anomaly detection. VDDFormer comprises a variable correlation encoder, a temporal dependency encoder, and a reconstruction decoder. The variable correlation encoder capitalizes on a variable dependency attention mechanism, which employs self-attention to learn the global variable dependencies; meanwhile, the local variable dependencies are captured by the adaptive correlation matrix. The global and local variable dependencies are then used to compute the variable dependency discrepancy as a new intrinsic property to distinguish between normal and abnormal patterns. By integrating this new discrepancy with the reconstruction error, the model effectively enhances its anomaly differentiation capability. Extensive experiments on five real-world anomaly detection datasets demonstrate that VDDFormer effectively and robustly detects group anomaly patterns by leveraging the variable dependency discrepancy and achieves state-of-the-art performance on four out of the five datasets.},
  archive  = {J},
  author   = {Bo Liu and Lingling Tao and Xiaodan Chen and Zhijun Li},
  doi      = {10.1109/TBDATA.2025.3600004},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-14},
  title    = {VDDFormer: A variable dependency discrepancy-based transformer for multivariate time series anomaly detection},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intent-driven semantic query: An effective approach for temporal knowledge graph query. <em>TBD</em>, 1-13. (<a href='https://doi.org/10.1109/TBDATA.2025.3600035'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The temporal knowledge graph (TKG) query facilitates the retrieval of potential answers by parsing questions that incorporate temporal constraints, regarded as a vital downstream task in the broader spectrum of the TKG applications. Currently, enhancing the accuracy of the queries and the user experience has become a focal point for researchers. Existing query methods of the TKG aim to execute unambiguous standard query statements to return query results while neglecting the potential ambiguity in user input queries. To overcome this problem, in this paper, we propose a semantic query model for temporal knowledge graphs, TKGSQ-PM (Temporal Knowledge Graph Semantic Query based on Pre-trained Model). This model first identifies and extracts entity and temporal information from temporal knowledge graph queries and obtains corresponding temporal knowledge graph embedding information based on embedding methods. Then, it utilizes the pre-trained model DistilBERT to infer the true query intent from user input queries. Finally, it performs comprehensive sorting to return highquality query results. We conduct multiple experiments on three different datasets to demonstrate the efficiency and effectiveness of the proposed methods. Experimental results indicate that the TKGSQ-PM model has an overall advantage over baseline models in terms of query effectiveness and efficiency.},
  archive  = {J},
  author   = {Luyi Bai and Jixuan Dong and Lin Zhu},
  doi      = {10.1109/TBDATA.2025.3600035},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-13},
  title    = {Intent-driven semantic query: An effective approach for temporal knowledge graph query},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentences based adversarial attack on AI-generated text detectors. <em>TBD</em>, 1-12. (<a href='https://doi.org/10.1109/TBDATA.2025.3600034'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The widespread use of AI-generated text has introduced significant security concerns, driving the need for reliable detection systems. However, recent studies reveal that neural network-based detectors are vulnerable to adversarial examples. To improve the robustness of such classifiers, a number of adversarial attack strategies have been developed, particularly in the context of text sentiment classification. Most existing adversarial attack methods focus on the semantics of individual words or sentences, often neglecting the broader contextual semantics of the entire text-particularly in the case of long AI-generated text. This limitation frequently results in adversarial examples that lack fluency and coherence. In this paper, we propose a novel method called Sentence-based Adversarial attack on AI-Generated Text detectors (SAGT), which generates linguistically fluent adversarial examples by inserting model-generated sentences into the original text. To ensure contextual semantic consistency, we extract important keywords from the original text-selected based on changes in the detector's confidence score-and incorporate them into the generated sentences. Extensive experimental results demonstrate that adversarial examples crafted by SAGT can effectively evade AI-generated text detectors.},
  archive  = {J},
  author   = {Rongxin Tu and Xiangui Kang and Chee Wei Tan and Chi-Hung Chi and Kwok-Yan Lam},
  doi      = {10.1109/TBDATA.2025.3600034},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-12},
  title    = {Sentences based adversarial attack on AI-generated text detectors},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can GNNs learn link heuristics? a concise review and evaluation of link prediction methods. <em>TBD</em>, 1-15. (<a href='https://doi.org/10.1109/TBDATA.2025.3600031'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper explores the ability of Graph Neural Networks (GNNs) in learning various forms of information for link prediction, alongside a brief review of existing link prediction methods. Our analysis reveals that GNNs cannot effectively learn structural information related to the number of common neighbors between two nodes, primarily due to the nature of set-based pooling of the neighborhood aggregation scheme. Also, our extensive experiments indicate that trainable node embeddings can improve the performance of GNN-based link prediction models. Importantly, we observe that the denser the graph, the greater such the improvement. We attribute this to the characteristics of node embeddings, where the link state of each link sample could be encoded into the embeddings of nodes that are involved in the neighborhood aggregation of the two nodes in that link sample. In denser graphs, every node could have more opportunities to attend the neighborhood aggregation of other nodes and encode states of more link samples to its embedding, thus learning better node embeddings for link prediction. Lastly, we demonstrate that the insights gained from our research carry important implications in identifying the limitations of existing link prediction methods, which could guide the future development of more robust algorithms.},
  archive  = {J},
  author   = {Shuming Liang and Yu Ding and Zhidong Li and Bin Liang and Siqi Zhang and Yang Wang and Fang Chen},
  doi      = {10.1109/TBDATA.2025.3600031},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-15},
  title    = {Can GNNs learn link heuristics? a concise review and evaluation of link prediction methods},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ST-DDGAN: A traffic data compensation model based on image restoration technology. <em>TBD</em>, 1-18. (<a href='https://doi.org/10.1109/TBDATA.2025.3600037'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In Intelligent Transportation Systems (ITS), the accuracy of compensating for missing traffic data is critical. This directly impacts the effectiveness of traffic flow prediction and road condition monitoring. Inspired by image restoration techniques, this study introduces the Generative Adversarial Network (GAN) to enhance traffic data compensation. Firstly, to address the problem of converting traffic data into the traffic flow matrix of the road network, we propose the RoadNetIMatrix algorithm to generate the traffic flow matrix of the road network. This algorithm precisely captures traffic flow dynamics in road networks and provides a holistic representation of traffic states. Secondly, given the inherent spatio-temporal correlation in traffic data, we proposed a spatio-temporal collaborative mining component (STSSM). This component integrates the hidden temporal dependencies and spatial features of the mined traffic data into the GAN generator to improve the authenticity of the generated content and ensure the consistency of data compensation. Finally, addressing the influence of external characteristics of traffic data on data compensation results, an external information module based on a multi-head attention mechanism is constructed, which can effectively mine the influence of external factors of traffic data. Furthermore, spatio-temporal and external features are fused to further improve the accuracy of data compensation. Experiments show that the model has a higher accuracy of data compensation and a better generalization of the system in the case of multiple types or a high data loss rate.},
  archive  = {J},
  author   = {Rong Wang and Na Lv and Xing Huang and Qingwang Guo and Yunpeng Xiao and Chaolong Jia and Haofei Xie},
  doi      = {10.1109/TBDATA.2025.3600037},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-18},
  title    = {ST-DDGAN: A traffic data compensation model based on image restoration technology},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TAG: Triple alignment with rationale generation for knowledge-based visual question answering. <em>TBD</em>, 1-16. (<a href='https://doi.org/10.1109/TBDATA.2025.3600012'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Knowledge-based Visual Question Answering (VQA) involves answering questions based not only on the given image, but also on external knowledge. Existing methods for knowledge-based VQA can be classified into two main categories: those that rely on external knowledge bases, and those that use Large Language Models (LLMs) as implicit knowledge engines. However, the former approach heavily relies on the quality of information retrieval, introducing additional information bias to the entire system. And the latter approach suffers from the extremely high computational cost and the loss of image information. To address these issues, we propose a novel framework called TAG that reformulates knowledge-based VQA as a contrastive learning problem. We innovatively propose a triple asymmetric paradigm, which aligns a lightweight text encoder to the image space with an extremely low training cost (0.0152B trainable parameters), and enhance its understanding ability on semantic granularity. TAG is both computation-efficient and effective, and we evaluate it on the knowledge-based VQA datasets, A-OKVQA, OK-VQA and VCR. The results show that TAG (0.387B) achieves the state-of-the-art performance when compared to methods using less than 1B parameters. Besides, TAG still shows competitive performance when compared to methods with LLM.},
  archive  = {J},
  author   = {Sihang Cai and Xuan Lin and Wenqiang Xu and Jingtong Wu and Tao Jin and Zhou Zhao and Fei Wu and Jun Yu},
  doi      = {10.1109/TBDATA.2025.3600012},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-16},
  title    = {TAG: Triple alignment with rationale generation for knowledge-based visual question answering},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal entity in one word: Aligning multi-level semantics for multi-modal knowledge graph completion. <em>TBD</em>, 1-14. (<a href='https://doi.org/10.1109/TBDATA.2025.3600014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Current multi-modal knowledge graph completion often incorporates simple fusion neural networks to achieve multi-modal alignment and knowledge completion tasks, which face three major challenges: 1) Inconsistent semantics between images and texts corresponding to the same entity; 2) Discrepancies in semantic spaces resulting from the use of diverse uni-modal feature extractors;3) Inadequate evaluation of semantic alignment using only energy functions or basic contrastive learning losses. To address these challenges, we propose the Multi-modal Entity in One Word (MEOW) model. This model ensures alignment at various levels, including text-image match alignment, feature alignment and distribution alignment. Specificially, the entity image filtering module utilizes a visual-language model to exclude unrelated images by aligning their captions with corresponding text descriptions. A pre-trained CLIP-based encoder is utilized for encoding dense semantic relationships, while a graph attention network based structure encoder handles sparse semantic relationships, yielding a comprehensive semantic representation and enhancing convergence speed. Additionally, a diffusion model is integrated to enhance denoising capabilities. The proposed MEOW further includes a distribution alignment module equipped with dense alignment constraint, integrity alignment constraint, and fusion fidelity constraint to effectively align multi-modal representations. Experiments on two public multi-modal knowledge graph datasets show that MEOW significantly improves link prediction performance. The code of the proposed model is available at https://github.com/yuyuyuger/MEOW.},
  archive  = {J},
  author   = {Lan Zhao and Boyue Wang and Junbin Gao and Xiaoyan Li and Yongli Hu and Baocai Yin},
  doi      = {10.1109/TBDATA.2025.3600014},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-14},
  title    = {Multi-modal entity in one word: Aligning multi-level semantics for multi-modal knowledge graph completion},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Community-imbalanced graph sampling. <em>TBD</em>, 1-14. (<a href='https://doi.org/10.1109/TBDATA.2025.3600032'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {A community-imbalanced graph refers to a graph containing multiple communities with large differences in node and edge scales. Graph sampling is a widely used graph reduction technique to accelerate graph computations and simplify graph visualizations. However, existing graph sampling algorithms may encounter several problems, including the loss of small communities, disconnections between communities, and distortions of community scale distribution, on maintaining the community structures in a community-imbalanced graph. In this work, a new quality indicator is proposed to determine if a graph can be regarded as a community-imbalanced graph. A community-imbalanced graph sampling (CIGS) algorithm is proposed to address the community-imbalanced graph sampling problems. Three new evaluation metrics are proposed to assess the performance of community structure maintenance of graph sampling. An algorithm performance experiment and a user study are conducted to evaluate the effectiveness of the proposed CIGS.},
  archive  = {J},
  author   = {Ying Zhao and Genghuai Bai and Yusheng Qiu and Yiwen Liu and Chuhan Zhang and Chi Han and Yitao Wu and Kehua Guo and Jian Zhang and Fangfang Zhou},
  doi      = {10.1109/TBDATA.2025.3600032},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-14},
  title    = {Community-imbalanced graph sampling},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDEC: Semantic deep embedded clustering. <em>TBD</em>, 1-16. (<a href='https://doi.org/10.1109/TBDATA.2025.3603433'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The high dimensional and semantically complex nature of textual Big data presents significant challenges for text clustering, which frequently lead to suboptimal groupings when using conventional techniques like k-means or hierarchical clustering. This work presents Semantic Deep Embedded Clustering (SDEC), an unsupervised text clustering framework that combines an improved autoencoder with transformer-based embeddings to overcome these challenges. This novel method preserves semantic relationships during data reconstruction by combining Mean Squared Error (MSE) and Cosine Similarity Loss (CSL) within an autoencoder. Furthermore, a semantic refinement stage that takes advantage of the contextual richness of transformer embeddings is used by SDEC to further improve a clustering layer with soft cluster assignments and distributional loss. The capabilities of SDEC are demonstrated by extensive testing on five benchmark datasets: AG News, Yahoo! Answers, DBPedia, Reuters 2, and Reuters 5. The framework not only outperformed existing methods with a clustering accuracy of 85.7% on AG News and set a new benchmark of 53.63% on Yahoo! Answers, but also showed robust performance across other diverse text corpora. These findings highlight the significant improvements in accuracy and semantic comprehension of text data provided by SDEC's advances in unsupervised text clustering.},
  archive  = {J},
  author   = {Mohammad Wali Ur Rahman and Ric Nevarez and Lamia Tasnim Mim and Salim Hariri},
  doi      = {10.1109/TBDATA.2025.3603433},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-16},
  title    = {SDEC: Semantic deep embedded clustering},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing deduplication parameters via a change-estimation analytical model. <em>TBD</em>, 1-12. (<a href='https://doi.org/10.1109/TBDATA.2025.3604171'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Variable-sized, content-defined deduplication is a technique to find and eliminate redundant chunks of data for efficient data backups, reduced data transfers, and reduced data-storage overheads. For big datasets, especially with incremental updates over time such as backups and gathered data, deduplication makes data management faster and more efficient. While many existing deduplication systems use default expected chunk lengths such as 4 KB or 8 KB, they are suboptimal. Poorly optimized deduplication systems can significantly increase storage costs and network usage, making large datasets prohibitively expensive to manage. We present the design, implementation, and an empirical validation of our Deduplication Change-Estimation Analytical Model (DCAM) which predicts the performance of sliding window-based deduplication parameters on any given dataset, to be used for parameter optimization. Our empirical evaluation includes workloads based on source code (Linux kernel, Kubernetes, TensorFlow), open-research datasets (CORD-19), and articles (Wikipedia). Validated using both our system and the Destor deduplication system, a DCAM-based search finds deduplication parameters that require up to 3.8× less storage relative to a common baseline. DCAM Search optimizes parameters up to 19.8× faster than previously possible, and the size of the resulting deduplicated datasets are all within 5.15% of the best results found by searching using actual deduplication.},
  archive  = {J},
  author   = {Owen Randall and Luke Schultz and Paul Lu},
  doi      = {10.1109/TBDATA.2025.3604171},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-12},
  title    = {Optimizing deduplication parameters via a change-estimation analytical model},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ScProGraph: A cell bagging strategy for cell type annotation with gene interaction-aware explainability. <em>TBD</em>, 1-12. (<a href='https://doi.org/10.1109/TBDATA.2025.3604169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The rapid advancement of scRNA-seq has generated massive data for cell type annotation. However, current automated annotation methods remain limited: most approaches separately model either cell-cell similarities or gene-gene relationships, neglecting their synergistic effects, which leads to suboptimal accuracy and poor biological interpretability. To address this, we propose scProGraph, a prototype-guided graph neural network that jointly models cell type classification and functional gene subgraph discovery. By constructing a cell similarity graph and incorporating cell-type prototypes as prior anchors, our method simultaneously optimizes classification boundaries and the interpretability of gene subgraphs. Experiments on seven independent datasets spanning three disease categories demonstrate that scProGraph achieves over 90% accuracy on four datasets and exceeds 80% on six datasets, outperforming state-of-the-art methods. Further analysis reveals that the gene subgraphs extracted by scProGraph for Macrophage, Fibroblast, and Monocyte cover 26.92%, 26.83%, and 22.22% of a protein-protein interaction networks dataset, respectively, validating the biological relevance of the identified gene modules. This study not only provides a high-accuracy tool for single-cell annotation but also opens new avenues for discovering novel biomarkers and regulatory mechanisms through gene relationship mining.},
  archive  = {J},
  author   = {Xinyuan Li and Yue-Chao Li and Hai-Ru You and Xuequn Shang and Leon Wong and Zhi-An Huang and Zhu-Hong You and Yu-An Huang},
  doi      = {10.1109/TBDATA.2025.3604169},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-12},
  title    = {ScProGraph: A cell bagging strategy for cell type annotation with gene interaction-aware explainability},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal transport barycentric aggregation for byzantine-resilient federated learning. <em>TBD</em>, 1-12. (<a href='https://doi.org/10.1109/TBDATA.2025.3604177'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Federated learning (FL) has emerged as a promising solution to enable distributed learning without sharing sensitive data. However, FL is vulnerable to data poisoning attacks, where malicious clients inject malicious data during training to compromise the global model. Existing FL defenses suffer from the assumptions of independent and identically distributed (IID) model updates, asymptotic optimal error rate bounds, and strong convexity in the optimization problem. Hence, we propose a novel framework called Federated Learning Optimal Transport (FLOT) that leverages the Wasserstein barycentric technique to obtain a global model from a set of locally trained non-IID models on client devices. In addition, we introduce a loss function-based rejection (LFR) mechanism to suppress malicious updates and a dynamic weighting scheme to optimize the Wasserstein barycentric aggregation function. We provide the theoretical proof of the Byzantine resilience and convergence of FLOT to highlight its efficacy. We evaluate FLOT on four benchmark datasets: GTSRB, KBTS, CIFAR10, and EMNIST. The experimental results underscore the practical significance of FLOT as an effective defense mechanism against data poisoning attacks in FL while maintaining high accuracy and scalability. Also, we observe that FLOT serves as a robust client selection technique under no attack, which demonstrates its effectiveness.},
  archive  = {J},
  author   = {K Naveen Kumar and Srinivasa Rao Chalamala and Ajeet Kumar Singh and C Krishna Mohan},
  doi      = {10.1109/TBDATA.2025.3604177},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-12},
  title    = {Optimal transport barycentric aggregation for byzantine-resilient federated learning},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MuGNet-CMI: Multi-head hybrid graph neural network for predicting circRNA-miRNA interactions with global high-order and local low-order information. <em>TBD</em>, 1-15. (<a href='https://doi.org/10.1109/TBDATA.2025.3604175'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Circular RNAs (circRNAs) are non-coding RNA molecules that play a crucial role in regulating genes and contributing to disease progression. CircRNAs can function as sponges for microRNAs (miRNAs), thereby regulating gene expression and influencing disease outcomes. Identifying associations between circRNAs and miRNAs through computational methods enhances the understanding of complex disease mechanisms and offers a reliable tool for pre-selecting candidates for experimental validation. Existing models, however, are limited in their ability to capture either global or local node information, the prediction of circRNA and miRNA interactions is still challenging. In order to effectively deal with this problem, we propose a novel framework for predicting circRNA-miRNA interactions (CMIs), known as MuGNet-CMI, which leverages multi-head hybrid graph neural network and global high-order and local low-order information. The model employs the MetaPath2Vec algorithm to generate high-quality node embeddings within the circRNA-miRNA heterogeneous matrix. The multi-head dynamic attention mechanism, combined with GraphSAGE, is incorporated to efficiently capture both global high-order and local low-order node information. Additionally, we integrate neural aggregators into the multi-head dynamic attention mechanism to aggregate feature information from the captured nodes. Validation using three real datasets demonstrates that MuGNet-CMI delivers good performance in predicting CMIs, offering valuable insights to guide experimental research in gene regulation.},
  archive  = {J},
  author   = {Chen Jiang and Lei Wang and Changqing Yu and Zhuhong You and Xinfei Wang and Mengmeng Wei and Mianshuo Lu},
  doi      = {10.1109/TBDATA.2025.3604175},
  journal  = {IEEE Transactions on Big Data},
  month    = {8},
  pages    = {1-15},
  title    = {MuGNet-CMI: Multi-head hybrid graph neural network for predicting circRNA-miRNA interactions with global high-order and local low-order information},
  year     = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
