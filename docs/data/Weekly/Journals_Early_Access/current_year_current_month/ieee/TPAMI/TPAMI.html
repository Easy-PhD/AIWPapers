<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPAMI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpami">TPAMI - 23</h2>
<ul>
<li><details>
<summary>
(2025). $\beta$-DARTS++: Bi-level regularization for proxy-robust differentiable architecture search. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3616249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Architecture Search (NAS) has attracted increasing attention in recent years because of its capability to design neural networks automatically. Among them, differential NAS approaches such as DARTS, have gained popularity for search efficiency. However, they still suffer from three main issues, that are, the weak stability due to the performance collapse, the poor generalization ability of the searched architectures, and the inferior robustness to different kinds of proxies (i.e., computationally reduced search configurations). To solve the search stability and searched architecture's generalization problems, a simple-but-effective regularization method, termed as Beta-Decay, is proposed to regularize the DARTS-based NAS searching process (referred as $\beta$-DARTS). Specifically, Beta-Decay regularization can impose constraints to keep the value and variance of activated architecture parameters from being too large, thereby ensuring fair competition among architecture parameters and making the supernet less sensitive to the impact of input on the operation set. In-depth theoretical analyses on how it works and why it works are provided, and comprehensive experiments on a variety of search spaces and datasets validate that Beta-Decay regularization can help to stabilize the searching process and make the searched network more transferable across different datasets. To address the proxy robustness problem, we first benchmark differentiable NAS methods under a wide range of proxy data, proxy channels, proxy layers, and proxy epochs, since the robustness of NAS under different kinds of proxies has not been explored before. We then conclude some interesting findings and find that $\beta$-DARTS always achieves the best result among all compared NAS methods under almost all proxy settings. We further introduce the novel flooding regularization to the weight optimization of $\beta$-DARTS (termed as Bi-level regularization), and experimentally and theoretically verify its effectiveness for improving the proxy robustness of differentiable NAS. In summary, our search scheme shows lots of outstanding properties for practical applications,},
  archive      = {J_TPAMI},
  author       = {Peng Ye and Tong He and Baopu Li and Tao Chen and Lei Bai and Wanli Ouyang},
  doi          = {10.1109/TPAMI.2025.3616249},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {$\beta$-DARTS++: Bi-level regularization for proxy-robust differentiable architecture search},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ID-guard: A universal framework for combating facial manipulation via breaking identification. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3616232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The misuse of deep learning-based facial manipulation poses a serious threat to civil rights. To prevent such fraud at its source, proactive defense methods have been proposed that embed invisible adversarial perturbations into images, disrupting the manipulation process and rendering the forged output unconvincing to observers. However, non-targeted disruption of the output may leave identifiable facial features intact, potentially leading to the stigmatization of individuals. In this work, we propose a universal framework for combating facial manipulation, termed ID-Guard. The framework employs a single forward pass of an encoder-decoder network to generate cross-model transferable adversarial perturbations. We introduce a novel Identity Destruction Module (IDM) to suppress identifiable features in manipulated faces. The perturbation generation is optimized by formulating the disruption of various manipulation types as a multi-task learning problem, with a dynamic weighting strategy designed to enhance cross-model performance. Experimental results show that ID-Guard effectively defends against diverse facial manipulation models while degrading identifiable regions in manipulated images. It also enables disrupted images to evade facial inpainting and facial recognition systems. Moreover, ID-Guard can be seamlessly integrated as a plug-and-play component into other tasks, such as adversarial training.},
  archive      = {J_TPAMI},
  author       = {Zuomin Qu and Wei Lu and Xiangyang Luo and Qian Wang and Xiaochun Cao},
  doi          = {10.1109/TPAMI.2025.3616232},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ID-guard: A universal framework for combating facial manipulation via breaking identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DTL: Parameter- and memory-efficient disentangled vision learning. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3616318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cost of finetuning a pretrained model on downstream tasks steadily increases as they grow larger. Parameter-efficient transfer learning (PETL) is proposed to reduce this cost by changing only a tiny subset of trainable parameters. But, the GPU memory footprint during training is not effectively reduced in PETL. This issue happens because trainable parameters from these methods are generally tightly entangled with the backbone, such that a lot of intermediate states have to be stored for back propagation. To alleviate this issue, we introduce Disentangled Transfer Learning (DTL), which disentangles the trainable parameters from the backbone using a lightweight Compact Side Network (CSN). By progressively extracting task-specific information with a few low-rank linear mappings and appropriately adding the information back to the backbone, CSN effectively realizes knowledge transfer in various downstream recognition tasks. We further extend DTL to more difficult tasks such as object detection and semantic segmentation by employing a more sparse architectural design. Extensive experiments validate the effectiveness of DTL, which not only reduces a large amount of GPU memory usage and trainable parameters, but also outperforms existing PETL methods by a significant margin in accuracy.},
  archive      = {J_TPAMI},
  author       = {Minghao Fu and Ke Zhu and Zonghao Ding and Jianxin Wu},
  doi          = {10.1109/TPAMI.2025.3616318},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DTL: Parameter- and memory-efficient disentangled vision learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-matrix completion: A novel framework for structurally missing elements. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3616607'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common assumption in matrix completion (MC) and tensor completion (TC) is that the missing locations are sampled randomly. However, in real-world scenarios, the unobserved elements are often not arbitrarily located, and may concentrate within entire rows or columns. We refer to this missing mechanism as structural missingness, and traditional MC and TC schemes suffer from drastic degradation under these circumstances. This work addresses the challenge of restoring structural missingness by introducing a novel framework for simultaneously reconstructing multiple matrices, called multi-matrix completion (MMC). In MMC, tri-factorization across matrices captures the correlation between matrices, and Tikhonov regularization on each matrix exploits its correlation. This design enables MMC to efficiently handle both random and structural missingness. In addition, MMC is not affected by the smoothness along matrices which makes it suitable for a wider variety of data compared to Fourier transform based TC methods. The alternating direction method of multipliers is utilized to solve the resultant optimization problem. The global convergence of the algorithm is supported by comprehensive theoretical analyses. We demonstrate the versatility of MMC through extensive experiments in image and video restoration, and showcase its superior performance in comparison to traditional MC and TC methods. The code is available at https://github.com/ShuDun23/MMC.},
  archive      = {J_TPAMI},
  author       = {Hao Nan Sheng and Zhi-Yong Wang and Hing Cheung So and Abdelhak M. Zoubir},
  doi          = {10.1109/TPAMI.2025.3616607},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-matrix completion: A novel framework for structurally missing elements},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deeper insights into deep graph convolutional networks: Stability and generalization. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3616350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional networks (GCNs) have emerged as powerful models for graph learning tasks, exhibiting promising performance in various domains. While their empirical success is evident, there is a growing need to understand their essential ability from a theoretical perspective. Existing theoretical research has primarily focused on the analysis of single-layer GCNs, while a comprehensive theoretical exploration of the stability and generalization of deep GCNs remains limited. In this paper, we bridge this gap by delving into the stability and generalization properties of deep GCNs, aiming to provide valuable insights by characterizing rigorously the associated upper bounds. Our theoretical results reveal that the stability and generalization of deep GCNs are influenced by certain key factors, such as the maximum absolute eigenvalue of the graph filter operators and the depth of the network. Our theoretical studies contribute to a deeper understanding of the stability and generalization properties of deep GCNs, potentially paving the way for developing more reliable and well-performing models.},
  archive      = {J_TPAMI},
  author       = {Guangrui Yang and Ming Li and Han Feng and Xiaosheng Zhuang},
  doi          = {10.1109/TPAMI.2025.3616350},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deeper insights into deep graph convolutional networks: Stability and generalization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAIT: Triple-win compression towards high accuracy, fast inference, and favorable transferability for ViTs. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3616854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers (ViTs) have emerged as state-of-the-art models for various vision tasks recently. However, their heavy computation costs remain daunting for resource-limited devices. To address this, researchers have dedicated themselves to compressing redundant information in ViTs for acceleration. However, existing approaches generally sparsely drop redundant image tokens by token pruning or brutally remove channels by channel pruning, leading to a sub-optimal balance between model performance and inference speed. Moreover, they struggle when transferring compressed models to downstream vision tasks that require the spatial structure of images, such as semantic segmentation. To tackle these issues, we propose CAIT, a joint compression method for ViTs that achieves a harmonious blend of high accuracy, fast inference speed, and favorable transferability to downstream tasks. Specifically, we introduce an asymmetric token merging (ATME) strategy to effectively integrate neighboring tokens. It can successfully compress redundant token information while preserving the spatial structure of images. On top of it, we further design a consistent dynamic channel pruning (CDCP) strategy to dynamically prune unimportant channels in ViTs. Thanks to CDCP, insignificant channels in multi-head self-attention modules of ViTs can be pruned uniformly, significantly enhancing the model compression. Extensive experiments on multiple benchmark datasets show that our proposed method can achieve state-of-the-art performance across various ViTs.},
  archive      = {J_TPAMI},
  author       = {Ao Wang and Hui Chen and Zijia Lin and Sicheng Zhao and Jungong Han and Guiguang Ding},
  doi          = {10.1109/TPAMI.2025.3616854},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CAIT: Triple-win compression towards high accuracy, fast inference, and favorable transferability for ViTs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LargeAD: Large-scale cross-sensor data pretraining for autonomous driving. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3617126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in vision foundation models (VFMs) have revolutionized visual perception in 2D, yet their potential for 3D scene understanding, particularly in autonomous driving applications, remains underexplored. In this paper, we introduce LargeAD, a versatile and scalable framework designed for large-scale 3D pretraining across diverse real-world driving datasets. Our framework leverages VFMs to extract semantically rich superpixels from 2D images, which are aligned with LiDAR point clouds to generate high-quality contrastive samples. This alignment facilitates cross-modal representation learning, enhancing the semantic consistency between 2D and 3D data. We introduce several key innovations: (i) VFM-driven superpixel generation for detailed semantic representation, (ii) a VFM-assisted contrastive learning strategy to align multimodal features, (iii) superpoint temporal consistency to maintain stable representations across time, and (iv) multi-source data pretraining to generalize across various LiDAR configurations. Our approach achieves substantial gains over state-of-the-art methods in linear probing and fine-tuning for LiDAR-based segmentation and object detection. Extensive experiments on 11 large-scale multi-sensor datasets highlight our superior performance, demonstrating adaptability, efficiency, and robustness in real-world autonomous driving scenarios.},
  archive      = {J_TPAMI},
  author       = {Lingdong Kong and Xiang Xu and Youquan Liu and Jun Cen and Runnan Chen and Wenwei Zhang and Liang Pan and Kai Chen and Ziwei Liu},
  doi          = {10.1109/TPAMI.2025.3617126},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LargeAD: Large-scale cross-sensor data pretraining for autonomous driving},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient 3D surface super-resolution via normal-based multimodal restoration. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3614184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-fidelity 3D surface is essential for vision tasks across various domains such as medical imaging, cultural heritage preservation, quality inspection, virtual reality, and autonomous navigation. However, the intricate nature of 3D data representations poses significant challenges in restoring diverse 3D surfaces while capturing fine-grained geometric details at a low cost. This paper introduces an efficient multimodal normal-based 3D surface super-resolution (mn3DSSR) framework, designed to address the challenges of microgeometry enhancement and computational overhead. Specifically, we have constructed one of the largest normalbased multimodal dataset, ensuring superior data quality and diversity through meticulous subjective selection. Furthermore, we explore a new two-branch multimodal alignment approach along with a multimodal split fusion module to mitigate computational complexity while improving restoration performances. To address the limitations associated with normal-based multimodal learning, we develop novel normal-induced loss functions that facilitate geometric consistency and improve feature alignment. Extensive experiments conducted on seven benchmark datasets across four different 3D data representations demonstrate that mn3DSSR consistently outperforms state-ofthe-art super-resolution methods in terms of restoration accuracy with high computational efficiency.},
  archive      = {J_TPAMI},
  author       = {Miaohui Wang and Yunheng Liu and Wuyuan Xie and Boxin Shi and Jianmin Jiang},
  doi          = {10.1109/TPAMI.2025.3614184},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient 3D surface super-resolution via normal-based multimodal restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Schedule-robust continual learning. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3614868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning (CL) tackles a fundamental challenge in machine learning, aiming to continuously learn novel data from non-stationary data streams while mitigating forgetting of previously learned data. Although existing CL algorithms have introduced various practical techniques for combating forgetting, little attention has been devoted to studying how data schedules – which dictate how the sample distribution of a data stream evolves over time – affect the CL problem. Empirically, most CL methods are susceptible to schedule changes: they exhibit markedly lower accuracy when dealing with more “difficult schedules over the same underlying training data. In practical scenarios, data schedules are often unknown and a key challenge is thus to design CL methods that are robust to diverse schedules to ensure model reliability. In this work, we introduce the novel concept of schedule robustness for CL and propose Schedule-Robust Continual Learning (SCROLL), a strong baseline satisfying this desirable property. SCROLL trains a linear classifier on a suitably pre-trained representation, followed by model adaptation using replay data only. We connect SCROLL to a meta-learning formulation of CL with provable guarantees on schedule robustness. Empirically, the proposed method significantly outperforms existing CL methods and we provide extensive ablations to highlight its properties.},
  archive      = {J_TPAMI},
  author       = {Ruohan Wang and Marco Ciccone and Massimiliano Pontil and Carlo Ciliberto},
  doi          = {10.1109/TPAMI.2025.3614868},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Schedule-robust continual learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PiercingEye: Dual-space video violence detection with hyperbolic vision-language guidance. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3617460'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing weakly supervised video violence detection (VVD) methods primarily rely on Euclidean representation learning, which often struggles to distinguish visually similar yet semantically distinct events due to limited hierarchical modeling and insufficient ambiguous training samples. To address this challenge, we propose PiercingEye, a novel dual-space learning framework that synergizes Euclidean and hyperbolic geometries to enhance discriminative feature representation. Specifically, PiercingEye introduces a layer-sensitive hyperbolic aggregation strategy with hyperbolic Dirichlet energy constraints to progressively model event hierarchies, and a cross-space attention mechanism to facilitate complementary feature interactions between Euclidean and hyperbolic spaces. Furthermore, to mitigate the scarcity of ambiguous samples, we leverage large language models to generate logic-guided ambiguous event descriptions, enabling explicit supervision through a hyperbolic vision-language contrastive loss that prioritizes high-confusion samples via dynamic similarity-aware weighting. Extensive experiments on XD-Violence and UCF-Crime benchmarks demonstrate that PiercingEye achieves state-of-the-art performance, with particularly strong results on a newly curated ambiguous event subset, validating its superior capability in fine-grained violence detection.},
  archive      = {J_TPAMI},
  author       = {Jiaxu Leng and Zhanjie Wu and Mingpi Tan and Mengjingcheng Mo and Jiankang Zheng and Qingqing Li and Ji Gan and Xinbo Gao},
  doi          = {10.1109/TPAMI.2025.3617460},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PiercingEye: Dual-space video violence detection with hyperbolic vision-language guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning dynamic graph embeddings with neural controlled differential equations. <em>TPAMI</em>, 1-10. (<a href='https://doi.org/10.1109/TPAMI.2025.3617660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on representation learning for dynamic graphs with temporal interactions. A fundamental issue is that both the graph structure and the nodes own their own dynamics, and their blending induces intractable complexity in the temporal evolution over graphs. Drawing inspiration from the recent progress of physical dynamic models in deep neural networks, we propose Graph Neural Controlled Differential Equations (GN-CDEs), a continuous-time framework that jointly models node embeddings and structural dynamics by incorporating a graph enhanced neural network vector field with a time-varying graph path as the control signal. Our framework exhibits several desirable characteristics, including the ability to express dynamics on evolving graphs without piecewise integration, the capability to calibrate trajectories with subsequent data, and robustness to missing observations. Empirical evaluation on a range of dynamic graph representation learning tasks demonstrates the effectiveness of our proposed approach in capturing the complex dynamics of dynamic graphs.},
  archive      = {J_TPAMI},
  author       = {Tiexin Qin and Benjamin Walker and Terry Lyons and Hong Yan and Haoliang Li},
  doi          = {10.1109/TPAMI.2025.3617660},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning dynamic graph embeddings with neural controlled differential equations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EvLight++: Low-light video enhancement with an event camera: A large-scale real-world dataset, novel method, and more. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3617801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras offer significant advantages for low-light video enhancement, primarily due to their high dynamic range. Current research, however, is severely limited by the absence of large-scale, real-world, and spatio-temporally aligned event-video datasets. To address this, we introduce a large-scale dataset with over 30,000 pairs of frames and events captured under varying illumination. This dataset was curated using a robotic arm that traces a consistent non-linear trajectory, achieving spatial alignment precision under 0.03mm and temporal alignment with errors under 0.01s for 90% of the dataset. Based on the dataset, we propose EvLight++, a novel event-guided low-light video enhancement approach designed for robust performance in real-world scenarios. Firstly, we design a multi-scale holistic fusion branch to integrate structural and textural information from both images and events. To counteract variations in regional illumination and noise, we introduce Signal-to-Noise Ratio (SNR)-guided regional feature selection, enhancing features from high SNR regions and augmenting those from low SNR regions by extracting structural information from events. To incorporate temporal information and ensure temporal coherence, we further introduce a recurrent module and temporal loss in the whole pipeline. Extensive experiments on ours and the synthetic SDSD dataset demonstrate that EvLight++ significantly outperforms both single image- and video-based methods by 1.37 dB and 3.71 dB, respectively. To further explore its potential in downstream tasks like semantic segmentation and monocular depth estimation, we extend our datasets by adding pseudo segmentation and depth labels via meticulous annotation efforts with foundation models. Experiments under diverse low-light scenes show that the enhanced results achieve a 15.97% improvement in mIoU for semantic segmentation.},
  archive      = {J_TPAMI},
  author       = {Kanghao Chen and Guoqiang Liang and Yunfan Lu and Hangyu Li and Lin Wang},
  doi          = {10.1109/TPAMI.2025.3617801},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EvLight++: Low-light video enhancement with an event camera: A large-scale real-world dataset, novel method, and more},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lodge++: High-quality and long dance generation with robust choreography patterns. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3618016'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose Lodge++, a choreography framework to generate high-quality, ultra-long dances given the music and desired genre. To handle the challenges in computational efficiency, the complex global choreography patterns across various dance genres, and the physical quality of local dance movements, Lodge++ adopts a two-stage strategy to produce dances from coarse to fine. In the first stage, a global choreography network is designed to generate coarse-grained dance primitives that capture complex global choreography patterns. In the second stage, guided by these dance primitives, a primitive-based dance diffusion model is proposed to further generate high-quality, long-sequence dances in parallel, faithfully adhering to the complex choreography patterns. Additionally, to improve the physical plausibility, Lodge++ employs a penetration guidance module to resolve character self-penetration, a foot refinement module to optimize foot-ground contact, and a multi-genre discriminator to maintain genre consistency throughout the dance. Lodge++ is validated by extensive experiments, which show that our method can rapidly generate ultra-long dances suitable for various dance genres, ensuring well-organized global choreography patterns and high-quality local motion. Code, model and demonstrative video results will be available at https://li-ronghui.github.io/lodgepp.},
  archive      = {J_TPAMI},
  author       = {Ronghui Li and Hongwen Zhang and Yachao Zhang and Yuxiang Zhang and Youliang Zhang and Jie Guo and Yan Zhang and Xiu Li and Yebin Liu},
  doi          = {10.1109/TPAMI.2025.3618016},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Lodge++: High-quality and long dance generation with robust choreography patterns},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-free test time adaptation for out-of-distribution detection. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3615192'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Out-of-distribution (OOD) detection is essential for the reliability of ML models. Most existing methods for OOD detection learn a fixed decision criterion from a given in-distribution dataset and apply it universally to decide if a data point is OOD. Recent work [1] shows that given only indistribution data, it is impossible to reliably detect OOD data without extra assumptions. Motivated by the theoretical result and recent exploration of test-time adaptation methods, we propose a Non-Parametric Test Time Adaptation framework for Out-OfDistribution Detection (AdaODD). Unlike conventional methods, AdaODD utilizes online test samples for model adaptation during testing, enhancing adaptability to changing data distributions. The framework incorporates detected OOD instances into decisionmaking, reducing false positive rates, particularly when ID and OOD distributions overlap significantly. We demonstrate the effectiveness of AdaODD through comprehensive experiments on multiple OOD detection benchmarks, extensive empirical studies show that AdaODD significantly improves the performance of OOD detection over state-of-the-art methods. Specifically, AdaODD reduces the false positive rate (FPR95) by 23.23% on the CIFAR-10 benchmarks and 38% on the ImageNet-1k benchmarks compared to the advanced methods. Lastly, we theoretically verify the effectiveness of AdaODD.},
  archive      = {J_TPAMI},
  author       = {YiFan Zhang and Xue Wang and Tian Zhou and Kun Yuan and Zhang Zhang and Liang Wang and Rong Jin},
  doi          = {10.1109/TPAMI.2025.3615192},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Model-free test time adaptation for out-of-distribution detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Data-dependent rectangular bounding processes. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3618585'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic partition processes divide a multi-dimensional space into a number of regions, such that the data within each region exhibit some form of homogeneity. Due to the nature of their partition strategies, partition processes can often create many unnecessary divisions in sparse regions when trying to describe data in dense regions. To avoid this problem we introduce a parsimonious partition model – the Rectangular Bounding Process (RBP) – to efficiently partition multi-dimensional spaces, by employing a bounding strategy to enclose data points within rectangular bounding boxes. The RBP is self-consistent and as such can be directly extended from a finite hypercube to an infinite (unbounded) space. We extend the RBP to establish a data-dependent RBP (data-RBP) to generate bounding boxes only over existing data points in a sequential manner, which can effectively reduce model complexity and enable online learning. To achieve this, we design an alternative way to generate bounding boxes and prove the distributional equivalence between the data-RBP and the RBP when empty boxes are removed. We demonstrate application of the RBP and the data-RBP in three scenarios: regression trees, relational modelling, and random feature construction for online learning. Extensive experimental results validate the performance of the RBP and the data-RBP for both accuracy and efficiency.},
  archive      = {J_TPAMI},
  author       = {Xuhui Fan and Bin Li and Prosha A. Rahman and Scott A. Sisson},
  doi          = {10.1109/TPAMI.2025.3618585},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Data-dependent rectangular bounding processes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning knowledge-based prompts for robust 3D mask presentation attack detection. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3618630'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D mask presentation attack detection is crucial for protecting face recognition systems against the rising threat of 3D mask attacks. While most existing methods utilize multimodal features or remote photoplethysmography (rPPG) signals to distinguish between real faces and 3D masks, they face significant challenges, such as the high costs associated with multimodal sensors and limited generalization ability. Detection-related text descriptions offer concise, universal information and are cost-effective to obtain. However, the potential of vision-language multimodal features for 3D mask presentation attack detection remains unexplored. In this paper, we propose a novel knowledge-based prompt learning framework to explore the strong generalization capability of vision-language models for 3D mask presentation attack detection. Specifically, our approach incorporates entities and triples from knowledge graphs into the prompt learning process, generating fine-grained, task-specific explicit prompts that effectively harness the knowledge embedded in pretrained vision-language models. Furthermore, considering different input images may emphasize distinct knowledge graph elements, we introduce a visual-specific knowledge filter based on an attention mechanism to refine relevant elements according to the visual context. Additionally, we leverage causal graph theory insights into the prompt learning process to further enhance the generalization ability of our method. During training, a spurious correlation elimination paradigm is employed, which removes category-irrelevant local image patches using guidance from knowledge-based text features, fostering the learning of generalized causal prompts that align with category-relevant local patches. Experimental results demonstrate that the proposed method achieves state-of-the-art intra- and crossscenario detection performance on benchmark datasets.},
  archive      = {J_TPAMI},
  author       = {Fangling Jiang and Qi Li and Bing Liu and Weining Wang and Caifeng Shan and Zhenan Sun and Ming-Hsuan Yang},
  doi          = {10.1109/TPAMI.2025.3618630},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning knowledge-based prompts for robust 3D mask presentation attack detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Layer-adaptive-augmentation-based graph contrastive learning with feature decorrelation. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3618329'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive Learning (GCL) methods typically leverage augmentation techniques to generate different graph views for comparison, thereby learning corresponding representations for graph-related tasks in label-scarce scenarios. However, existing GCL methods suffer from two primary limitations: 1) they use predefined or one-time perturbations for augmentation, ignoring adaptive noise injection during forward propagation and thus leading to suboptimal model robustness; 2) their contrast mechanisms mainly focus on the agreement of inter-graph representations while neglecting the dimensional feature redundancy within intra-graph representations. To solve these issues, we propose Layer-adaptive-augmentation-based Graph Contrastive Learning with feature Decorrelation (LGCLD). Firstly, the designed layer- wise adaptive augmentation method performs dynamic perturbations while maintaining the semantic similarity between augmented and original graphs, which can improve model robustness. Secondly, we introduce an Agreement-Decorrelation loss (AD loss) that simultaneously optimizes the agreement between graph-level representations and the feature correlation among different dimensions within each graph-level representation, promoting the model to learn informative and non-redundant graph-level representations. Furthermore, we analyze the reasonableness of AD loss through the graph information bottleneck principle. Experiments on various-domain graph datasets demonstrate that LGCLD achieves better or competitive performance compared with a series of state-of-the-art baselines.},
  archive      = {J_TPAMI},
  author       = {Yuhua Xu and Junli Wang and Rui Duan and Changjun Jiang},
  doi          = {10.1109/TPAMI.2025.3618329},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Layer-adaptive-augmentation-based graph contrastive learning with feature decorrelation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistically aligned view-unaligned clustering with adaptive template selection. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3618984'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most existing multi-view modeling scenarios, cross-view correspondence (CVC) between instances of the same target from different views, like paired image-text data, is a crucial prerequisite for effortlessly deriving a consistent representation. Nevertheless, this premise is frequently compromised in certain applications, where each view is organized and transmitted independently, resulting in the view-unaligned problem (VuP). Restoring CVC of unaligned multi-view data is a challenging and highly demanding task that has received limited attention from the research community. To tackle this practical challenge, we propose to integrate the permutation derivation procedure into the bipartite graph paradigm for view-unaligned clustering, termed Probabilistically Aligned View-unaligned Clustering with Adaptive Template Selection (PAVuC-ATS). Specifically, we learn consistent anchors and view-specific graphs by the bipartite graph, and derive permutations applied to the unaligned graphs by reformulating the alignment between two latent representations as a 2-step transition of a Markov chain with adaptive template selection, thereby achieving the probabilistic alignment. The convergence of the resultant optimization problem is validated both experimentally and theoretically. Extensive experiments on six benchmark datasets demonstrate the superiority of the proposed PAVuC-ATS over the baseline methods.},
  archive      = {J_TPAMI},
  author       = {Wenhua Dong and Xiao-Jun Wu and Zhenhua Feng and Sara Atito and Muhammad Awais and Josef Kittler},
  doi          = {10.1109/TPAMI.2025.3618984},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Probabilistically aligned view-unaligned clustering with adaptive template selection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SMPLest-X: Ultimate scaling for expressive human pose and shape estimation. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3618174'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods focus on training innovative architectural designs on confined datasets. In this work, we investigate the impact of scaling up EHPS towards a family of generalist foundation models. 1) For data scaling, we perform a systematic investigation on 40 EHPS datasets, encompassing a wide range of scenarios that a model trained on any single dataset cannot handle. More importantly, capitalizing on insights obtained from the extensive benchmarking process, we optimize our training scheme and select datasets that lead to a significant leap in EHPS capabilities. Ultimately, we achieve diminishing returns at 10M training instances from diverse data sources. 2) For model scaling, we take advantage of vision transformers (up to ViT-Huge as the backbone) to study the scaling law of model sizes in EHPS. To exclude the influence of algorithmic design, we base our experiments on two minimalist architectures: SMPLer-X, which consists of an intermediate step for hand and face localization, and SMPLest-X, an even simpler version that reduces the network to its bare essentials and highlights significant advances in the capture of articulated hands. With big data and the large model, the foundation models exhibit strong performance across diverse test benchmarks and excellent transferability to even unseen environments. Moreover, our finetuning strategy turns the generalist into specialist models, allowing them to achieve further performance boosts. Notably, our foundation models consistently deliver state-of-the-art results on seven benchmarks such as AGORA, UBody, EgoBody, and our proposed SynHand dataset for comprehensive hand evaluation. (Code is available at: https://github.com/wqyin/SMPLest-X).},
  archive      = {J_TPAMI},
  author       = {Wanqi Yin and Zhongang Cai and Ruisi Wang and Ailing Zeng and Chen Wei and Qingping Sun and Haiyi Mei and Yanjun Wang and Hui En Pang and Mingyuan Zhang and Lei Zhang and Chen Change Loy and Atsushi Yamashita and Lei Yang and Ziwei Liu},
  doi          = {10.1109/TPAMI.2025.3618174},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SMPLest-X: Ultimate scaling for expressive human pose and shape estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wonder3D++: Cross-domain diffusion for high-fidelity 3D generation from a single image. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3618675'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce Wonder3D++, a novel method for efficiently generating high-fidelity textured meshes from single-view images. Recent methods based on Score Distillation Sampling (SDS) have shown the potential to recover 3D geometry from 2D diffusion priors, but they typically suffer from time-consuming per-shape optimization and inconsistent geometry. In contrast, certain works directly produce 3D information via fast network inferences, but their results are often of low quality and lack geometric details. To holistically improve the quality, consistency, and efficiency of single-view reconstruction tasks, we propose a cross-domain diffusion model that generates multi-view normal maps and the corresponding color images. To ensure the consistency of generation, we employ a multi-view cross-domain attention mechanism that facilitates information exchange across views and modalities. Lastly, we introduce a cascaded 3D mesh extraction algorithm that drives high-quality surfaces from the multi-view 2D representations in only about 3 minute in a coarse-to-fine manner. Our extensive evaluations demonstrate that our method achieves high-quality reconstruction results, robust generalization, and good efficiency compared to prior works.},
  archive      = {J_TPAMI},
  author       = {Yuxiao Yang and Xiaoxiao Long and Zhiyang Dou and Cheng Lin and Yuan Liu and Qingsong Yan and Yuexin Ma and Haoqian Wang and Zhiqiang Wu and Wei Yin},
  doi          = {10.1109/TPAMI.2025.3618675},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Wonder3D++: Cross-domain diffusion for high-fidelity 3D generation from a single image},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep neural network parameter selection via dataset similarity under meta-learning framework. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3618991'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing the performance of deep neural networks (DNNs) remains a significant challenge due to the sensitivity of models to both hyperparameter selection and weight initialization. Existing approaches typically address these two factors independently, which often leads to limiting adaptability and overall effectiveness. In this paper, we present a novel meta-learning framework that jointly recommends hyperparameters and initial weights by leveraging dataset similarity. Our method begins by extracting meta-features from a collection of historical datasets. For a given query dataset, similarity is computed based on distances in the meta-feature space, and the most similar historical datasets are used to recommend the underlying parameter configurations. To capture the diverse characteristics of image datasets, we introduce two complementary types of meta-features. The first, referred to as shallow or visible meta-features, comprises five groups of statistical measures that summarize color and texture information. The second, termed deep or invisible meta-features, consists of 512 descriptors extracted from a convolutional neural network pre-trained on ImageNet. We evaluated our framework in 105 real-world image classification tasks, using 75 datasets for historical modeling and 30 for querying. Experimental results with both vision transformers and convolutional neural networks demonstrate that our approach consistently outperforms state-of-the-art baselines, underscoring the effectiveness of dataset-driven parameter recommendation in deep learning.},
  archive      = {J_TPAMI},
  author       = {Liping Deng and Maziar Raissi and MingQing Xiao},
  doi          = {10.1109/TPAMI.2025.3618991},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep neural network parameter selection via dataset similarity under meta-learning framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified cross-modal medical image synthesis with hierarchical mixture of product-of-experts. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3616632'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a deep mixture of multimodal hierarchical variational auto-encoders called MMHVAE that synthesizes missing images from observed images in different modalities. MMHVAE's design focuses on tackling four challenges: (i) creating a complex latent representation of multimodal data to generate high-resolution images; (ii) encouraging the variational distributions to estimate the missing information needed for cross-modal image synthesis; (iii) learning to fuse multimodal information in the context of missing data; (iv) leveraging dataset-level information to handle incomplete data sets at training time. Extensive experiments are performed on the challenging problem of pre-operative brain multi-parametric magnetic resonance and intra-operative ultrasound imaging.},
  archive      = {J_TPAMI},
  author       = {Reuben Dorent and Nazim Haouchine and Alexandra Golby and Sarah Frisken and Tina Kapur and William Wells},
  doi          = {10.1109/TPAMI.2025.3616632},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unified cross-modal medical image synthesis with hierarchical mixture of product-of-experts},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Explicit visual prompting for universal foreground segmentations. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3619490'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foreground segmentation is a fundamental problem in computer vision, which includes salient object detection, forgery detection, defocus blur detection, shadow detection, and camouflage object detection. Previous works have typically relied on domain-specific solutions to address accuracy and robustness issues in those applications. In this paper, we present a unified framework for a number of foreground segmentation tasks without any task-specific designs. We take inspiration from the widely-used pre-training and then prompt tuning protocols in NLP and propose a new visual prompting model, named Explicit Visual Prompting (EVP). Different from the previous visual prompting which is typically a dataset-level implicit embedding, our key insight is to enforce the tunable parameters focusing on the explicit visual content from each individual image, i.e., the features from frozen patch embeddings and high-frequency components. Our method freezes a pre-trained model and then learns task-specific knowledge using a few extra parameters. Despite introducing only a small number of tunable parameters, EVP achieves superior performance than full fine-tuning and other parameter-efficient fine-tuning methods. Experiments in fourteen datasets across five tasks show the proposed method outperforms other task-specific methods while being considerably simple. The proposed method demonstrates the scalability in different architectures,pre-trained weights, and tasks. The code is available at: https://github.com/NiFangBaAGe/Explicit-Visual-Prompt.},
  archive      = {J_TPAMI},
  author       = {Weihuang Liu and Xi Shen and Chi-Man Pun and Xiaodong Cun},
  doi          = {10.1109/TPAMI.2025.3619490},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Explicit visual prompting for universal foreground segmentations},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
