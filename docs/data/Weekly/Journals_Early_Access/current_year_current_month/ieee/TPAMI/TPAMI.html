<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPAMI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpami">TPAMI - 51</h2>
<ul>
<li><details>
<summary>
(2025). Structure-induced gradient regulation for generalizable vision-language models. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3604454'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning, a recently emerging paradigm, adapts vision-language pre-trained models to new tasks efficiently by learning “soft prompts” for frozen models. However, in few-shot scenarios, its effectiveness is limited by sensitivity to the initialization and the time-consuming search for optimal initialization, hindering rapid adaptation. Additionally, prompt tuning risks reducing the models' generalizability due to overfitting on scarce training samples. To overcome these challenges, we introduce a novel Gradient-RegulAted Meta-prompt learning (GRAM) framework that jointly meta-learns an efficient soft prompt initialization for better adaptation and a lightweight gradient regulating function for strong cross-domain generalizability in a meta-learning paradigm using only the weakly labeled image-text pre-training data. This is achieved through a Cross-Modal Hierarchical Clustering algorithm that organizes extensive image-text data into a structured hierarchy, facilitating robust meta-learning across diverse domains. Rather than designing a specific prompt tuning method, our GRAM can be easily incorporated into various prompt tuning methods in a model-agnostic way and bring about consistent improvement for them. Further, we consider a more practical but challenging setting: test-time prompt tuning with only unlabeled test samples and propose an improved structure-induced gradient regulating function to leverage the structured semantics of the meta-learning data for zero-shot generalization. This novel approach exploits the hierarchically clustered meta-learning data to model relationships between test-time data and meta-learning prototypes, facilitating the transfer of invariant knowledge without explicit annotations. Meanwhile, we introduce a structure complexity-informed strategy for adaptively constructing meta-training tasks and generating prototypes, which fully considers the diverse semantics within hierarchical clusters of different complexities. Comprehensive experiments demonstrate the state-of-the-art few- and zero-shot generalizability of our method.},
  archive      = {J_TPAMI},
  author       = {Juncheng Li and Minghe Gao and Siliang Tang and Longhui Wei and Jun Xiao and Fei Wu and Richang Hong and Meng Wang and Qi Tian},
  doi          = {10.1109/TPAMI.2025.3604454},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Structure-induced gradient regulation for generalizable vision-language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An end-to-end depth-based pipeline for selfie image rectification. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3604574'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Portraits or selfie images taken from a close distance typically suffer from perspective distortion. In this paper, we propose an end-to-end deep learning-based rectification pipeline to mitigate the effects of perspective distortion. We learn to predict the facial depth by training a deep CNN. The estimated depth is utilized to adjust the camera-to-subject distance by moving the camera farther, increasing the camera focal length, and reprojecting the 3D image features to the new perspective. The reprojected features are then fed to an inpainting module to fill in the missing pixels. We leverage a differentiable renderer to enable end-to-end training of our depth estimation and feature extraction nets to improve the rectified outputs. To boost the results of the inpainting module, we incorporate an auxiliary module to predict the horizontal movement of the camera which decreases the area that requires hallucination of challenging face parts such as ears. Unlike previous works, we process the full-frame input image at once without cropping the subject's face and processing it separately from the rest of the body, eliminating the need for complex post-processing steps to attach the face back to the subject's body. To train our network, we utilize the popular game engine Unreal Engine to generate a large synthetic face dataset containing various subjects, head poses, expressions, eyewear, clothes, and lighting. Quantitative and qualitative results show that our rectification pipeline outperforms previous methods, and produces comparable results with a time-consuming 3D GAN-based method while being more than 260 times faster.},
  archive      = {J_TPAMI},
  author       = {Ahmed Alhawwary and Janne Mustaniemi and Phong Nguyen-Ha and Janne Heikkilä},
  doi          = {10.1109/TPAMI.2025.3604574},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {An end-to-end depth-based pipeline for selfie image rectification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LatentAugment: Data augmentation via guided manipulation of GAN's latent space. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3598866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Augmentation (DA) is a technique to increase the quantity and diversity of the training data, and by that alleviate overfitting and improve generalisation. However, standard DA produces synthetic data for augmentation with limited diversity. Generative Adversarial Networks (GANs) may unlock additional information in a dataset by generating synthetic samples having the appearance of real images. However, these models struggle to simultaneously address three key requirements: fidelity and high-quality samples; diversity and mode coverage; and fast sampling. Indeed, GANs generate high-quality samples rapidly, but have poor mode coverage, limiting their adoption in DA applications. We propose LatentAugment, a DA strategy that overcomes the low diversity of GANs, opening up for use in DA applications. Without external supervision, LatentAugment modifies latent vectors and moves them into latent space regions to maximise the synthetic images' diversity and fidelity. It is also agnostic to the dataset and the downstream task. A wide set of experiments shows that LatentAugment improves the generalisation of a deep model translating from MRI-to-CT beating both standard DA as well GAN-based sampling. We further demonstrate its effectiveness when translating from low-energy mammograms to dual-energy subtracted images in contrast-enhanced spectral mammography. Moreover, still in comparison with GAN-based sampling, LatentAugment synthetic samples show superior mode coverage and diversity. Code is available at: https://github.com/ltronchin/LatentAugment.},
  archive      = {J_TPAMI},
  author       = {Lorenzo Tronchin and Minh H. Vu and Paolo Soda and Tommy Löfstedt},
  doi          = {10.1109/TPAMI.2025.3598866},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LatentAugment: Data augmentation via guided manipulation of GAN's latent space},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MovieChat+: Question-aware sparse memory for long video question answering. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3604614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, integrating video foundation models and large language models to build a video understanding system can overcome the limitations of specific vision tasks. Yet, existing methods either employ complex spatial-temporal modules or rely heavily on additional perception models to extract temporal features for video understanding, performing well only on short videos. For long videos, the computational complexity and memory costs associated with long-term temporal connections are significantly increased, posing additional challenges. Leveraging the hierarchical memory structure of the Atkinson-Shiffrin memory model, with tokens in Transformers being employed as the carriers of memory in combination, we propose MovieChat within a training-free memory consolidation mechanism to overcome these challenges, which transfers dense frames from short-term memory into sparse tokens in long-term memory by temporally merging adjacent frames. We lift pre-trained large multi-modal models for understanding long videos without additional trainable modules, employing a zero-shot approach. Additionally, in our new version, MovieChat+, we design an enhanced training-free vision-question matching-based memory consolidation mechanism to better anchor predictions to relevant visual content. MovieChat achieves state-of-the-art performance in long video understanding, along with the released MovieChat-1K benchmark with 1K long video, 2K temporal grounding labels, and 14K manual annotations. Resources are available at: https://github.com/rese1f/MovieChat.},
  archive      = {J_TPAMI},
  author       = {Enxin Song and Wenhao Chai and Tian Ye and Jenq-Neng Hwang and Xi Li and Gaoang Wang},
  doi          = {10.1109/TPAMI.2025.3604614},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MovieChat+: Question-aware sparse memory for long video question answering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PRANCE: Joint token-optimization and structural channel-pruning for adaptive ViT inference. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3605239'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The troublesome model size and quadratic computational complexity associated with token quantity pose significant deployment challenges for Vision Transformers (ViTs) in practical applications. Despite recent advancements in model pruning and token reduction techniques speed up the inference speed of ViTs, these approaches either adopt a fixed sparsity ratio or overlook the meaningful interplay between architectural optimization and token selection. Consequently, this static and single-dimension compression often leads to pronounced accuracy degradation under aggressive compression rates, as they fail to fully explore redundancies across these two orthogonal dimensions. Therefore, we introduce PRANCE, a framework which can jointly optimize activated channels and tokens on a per-sample basis, aiming to accelerate ViTs' inference process from a unified data and architectural perspective. However, the joint framework poses challenges to both architectural and decision-making aspects. Firstly, while ViTs inherently support variable-token inference, they do not facilitate dynamic computations for variable channels. To overcome this limitation, we propose a meta-network using weight-sharing techniques to support arbitrary channels of the Multi-Head Self-Attention (MHSA) and Multi-Layer Perceptron (MLP) layers, serving as a foundational model for architectural decision-making. Secondly, simultaneously optimizing the model structure and input data constitutes a combinatorial optimization problem with an extremely large decision space, reaching up to around $10^{14}$, making supervised learning infeasible. To this end, we design a lightweight selector employing Proximal Policy Optimization algorithm (PPO) for efficient decision-making. Furthermore, we introduce a novel “Result-to-Go” training mechanism that models ViTs' inference process as a Markov decision process, significantly reducing action space and mitigating delayed-reward issues during training. Additionally, our framework simultaneously supports different kinds of token optimization methods such as pruning, merging, and sequential pruning-merging strategies. Extensive experiments demonstrate the effectiveness of PRANCE in reducing FLOPs by approximately 50%, retaining only about 10% of tokens while achieving lossless Top-1 accuracy.},
  archive      = {J_TPAMI},
  author       = {Ye Li and Chen Tang and Yuan Meng and Jiajun Fan and Zenghao Chai and Xinzhu Ma and Zhi Wang and Wenwu Zhu},
  doi          = {10.1109/TPAMI.2025.3605239},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PRANCE: Joint token-optimization and structural channel-pruning for adaptive ViT inference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Make identity indistinguishable: Utility-preserving face dataset publication with provable privacy guarantees. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3605195'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of personal devices, there are abundant valuable face image datasets in the industry, which provides opportunities for the development of visual models. However, privacy concerns related to identity sensitive information hinder face datasets sharing. Despite existing works dedicated to removing identity sensitive information from images, they either lack provable privacy guarantees or compromise crucial face dataset utilities, e.g., identity correlation and image naturalness. To overcome these weaknesses, we propose a novel face dataset publication scheme that protects face images by obfuscating face features. The obfuscated features still retain a certain level of correlation, allowing the protected dataset to be used for training. In the process of obfuscating the features, we design a novel metric differential privacy mechanism, which can enhance the correlation between features while ensuring privacy. Furthermore, we construct a latent diffusion model with identity and attribute as inputs to improve the naturalness of generated images. Extensive experimental results and theoretical analysis demonstrate our scheme significantly outperforms existing works in providing privacy protection while maintaining high dataset utility for downstream tasks.},
  archive      = {J_TPAMI},
  author       = {Yushu Zhang and Junhao Ji and Tao Wang and Ruoyu Zhao and Wenying Wen and Yong Xiang},
  doi          = {10.1109/TPAMI.2025.3605195},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Make identity indistinguishable: Utility-preserving face dataset publication with provable privacy guarantees},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PMGT-VR: A decentralized proximal-gradient algorithmic framework with variance reduction. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3606874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the decentralized composite optimization problem. We propose a novel decentralized variance-reduction proximal-gradient algorithmic framework, called PMGT-VR, which combines several techniques, including multi-consensus, gradient tracking, and variance reduction. The proposed framework imitates centralized algorithms and algorithms under this framework achieve convergence rates similar to that of their centralized counterparts. We also describe and analyze two representative algorithms, PMGT-SAGA and PMGT-LSVRG, and compare them to existing state-of-the-art proximal algorithms. To the best of our knowledge, PMGT-VR is the first linearly convergent decentralized stochastic algorithm that can solve decentralized composite optimization problems. Numerical experiments are provided to demonstrate the effectiveness of the proposed algorithms.},
  archive      = {J_TPAMI},
  author       = {Haishan Ye and Wei Xiong and Tong Zhang},
  doi          = {10.1109/TPAMI.2025.3606874},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PMGT-VR: A decentralized proximal-gradient algorithmic framework with variance reduction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCGS: Multiview consistency enhancement for sparse-view 3D gaussian radiance fields. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3607103'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiance fields represented by 3D Gaussians excel at synthesizing novel views, offering both high training efficiency and fast rendering. However, with sparse input views, the lack of multi-view consistency constraints results in poorly initialized Gaussians and unreliable heuristics for optimization, leading to suboptimal performance. Existing methods often incorporate depth priors from dense estimation networks but overlook the inherent multi-view consistency in input images. Additionally, they rely on dense initialization, which limits the efficiency of scene representation. To overcome these challenges, we propose a view synthesis framework based on 3D Gaussian Splatting, named MCGS, enabling photorealistic scene reconstruction from sparse views. The key innovations of MCGS in enhancing multi-view consistency are as follows: i) We leverage matching priors from a sparse matcher to initialize Gaussians primarily on textured regions, while low-texture areas are populated with randomly distributed Gaussians. This yields a compact yet sufficient set of initial Gaussians. ii) We propose a multi-view consistency-guided progressive pruning strategy to dynamically eliminate inconsistent Gaussians. This approach confines their optimization to a consistency-constrained space, which ensures robust and coherent scene reconstruction. These strategies enhance robustness to sparse views, accelerate rendering, and reduce memory consumption, making MCGS a practical framework for 3D Gaussian Splatting.},
  archive      = {J_TPAMI},
  author       = {Yuru Xiao and Deming Zhai and Wenbo Zhao and Kui Jiang and Junjun Jiang and Xianming Liu},
  doi          = {10.1109/TPAMI.2025.3607103},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MCGS: Multiview consistency enhancement for sparse-view 3D gaussian radiance fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Centroiding point-objects with event cameras. <em>TPAMI</em>, 1-10. (<a href='https://doi.org/10.1109/TPAMI.2025.3604385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event-based sensors (EBS), with their low latency and high dynamic range, are a promising means for tracking unresolved point-objects. Conventional EBS centroiding methods assume the generated events follow a Gaussian distribution and require long event streams ($\gt 1$s) for accurate localization. However, these assumptions are inadequate for centroiding unresolved objects, since the EBS circuitry causes non-Gaussian event distributions, and because using long event streams negates the low-latency advantage of EBS. In this work, we derive a closed-form spatiotemporal event distribution that accounts for these non-Gaussian effects and relaxes the long-time window requirement. Using Fisher analysis, we show that the spatial distribution of events in short time windows ($\leq 20$ ms) contains sufficient information for accurately estimating both position and velocity. To validate our analysis, we create the first EBS dataset of unresolved point-objects with subpixel ground truth using a high-speed monitor. We demonstrate that a small LSTM network can estimate an object's position within 1pixel and velocity within $\pm 17\%$ using only 5ms of event data, outperforming traditional approaches. These improvements enable accurate and quick centroiding of fast and dim objects, and we publish all code and data to support future research.},
  archive      = {J_TPAMI},
  author       = {Connor Hashemi and Dennis Melamed and Albert W. Reed and Nitesh Menon and Keigo Hirakawa and Scott McCloskey},
  doi          = {10.1109/TPAMI.2025.3604385},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Centroiding point-objects with event cameras},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving generalized visual grounding with instance-aware joint learning. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3607387'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized visual grounding tasks, including Generalized Referring Expression Comprehension (GREC) and Segmentation (GRES), extend the classical visual grounding paradigm by accommodating multi-target and non-target scenarios. Specifically, GREC focuses on accurately identifying all referential objects at the coarse bounding box level, while GRES aims for achieve fine-grained pixel-level perception. However, existing approaches typically treat these tasks independently, overlooking the benefits of jointly training GREC and GRES to ensure consistent multi-granularity predictions and streamline the overall process. Moreover, current methods often treat GRES as a semantic segmentation task, neglecting the crucial role of instance-aware capabilities and the necessity of ensuring consistent predictions between instance-level boxes and masks. To address these limitations, we propose InstanceVG, a multi-task generalized visual grounding framework equipped with instance-aware capabilities, which leverages instance queries to unify the joint and consistency predictions of instance-level boxes and masks. To the best of our knowledge, InstanceVG is the first framework to simultaneously tackle both GREC and GRES while incorporating instance-aware capabilities into generalized visual grounding. To instantiate the framework, we assign each instance query a prior reference point, which also serves as an additional basis for target matching. This design facilitates consistent predictions of points, boxes, and masks for the same instance. Extensive experiments obtained on ten datasets across four tasks demonstrate that InstanceVG achieves state-of-the-art performance, significantly surpassing the existing methods in various evaluation metrics. The code and model will be made publicly available.},
  archive      = {J_TPAMI},
  author       = {Ming Dai and Wenxuan Cheng and Jiang-Jiang Liu and Lingfeng Yang and Zhenhua Feng and Wankou Yang and Jingdong Wang},
  doi          = {10.1109/TPAMI.2025.3607387},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Improving generalized visual grounding with instance-aware joint learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer learning of stochastic kriging for individualized prediction. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3607773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic Kriging (SK) is a generalized variant of Gaussian process regression, and it is developed for dealing with non-i.i.d. noise in functional responses. Although SK has achieved substantial success in various engineering applications, its intrinsic modeling strategy by focusing on the sample mean limits its flexibility and capability of predicting individual functional samples. Moreover, the performance of SK can be impaired under scarce data scenarios, which are commonly encountered in engineering applications, especially for start-up or just deployed systems. In this paper, we propose a novel transfer learning framework to address the challenges of individualization and data scarcity in traditional SK. The proposed framework features a within-process model to facilitate individualized prediction and a between-process model to leverage information from related processes for resolving the issue of data scarcity. The within- and between-process models are integrated through a tailored convolution process, which quantifies interactions within and between processes using a specially designed covariance matrix and corresponding kernel parameters. Statistical properties are investigated on the parameter estimation of the proposed framework, which provide theoretical guarantees for the performance of transfer learning. The proposed method is compared with benchmark methods through various numerical and real case studies, and the results demonstrate the superiority of the proposed method in dealing with individualized prediction of functional responses, especially when limited data are available in the process of interest.},
  archive      = {J_TPAMI},
  author       = {Jinwei Yao and Jianguo Wu and Yongxiang Li and Chao Wang},
  doi          = {10.1109/TPAMI.2025.3607773},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Transfer learning of stochastic kriging for individualized prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sentence-level relation semantics learning via contrastive sentences. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3607794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sentence-level semantics plays a key role in language understanding. There exist subtle relations and dependencies among sentence-level samples, which is to be exploited. For example, in relational triple extraction, existing models overemphasize extraction modules, ignoring the sentence-level semantics and relation information, which causes (1) the semantics fed to extraction modules is relation-unaware; (2) each sample is trained individually without considering inter-sample dependency. To address these issues, we first propose the model-agnostic multi-relation detection task, which incorporates relation information into text encoding to generate the relation-aware semantics. Then we propose the model-agnostic multi-relation supervised contrastive learning, which leverages the relation-derived inter-sample dependencies as a supervised signal to learn discriminative semantics via drawing together or pushing away the sentence-level semantics regarding whether they share the same/similar relations. Besides, we design the reverse label frequency weighting and hierarchical label embedding mechanisms to alleviate label imbalance and integrate relation hierarchy. Our method can be applied to any RTE model and we conduct extensive experiments on five backbones by augmenting them with our method. Experimental results on four public benchmarks show that our method can bring significant and consistent improvements to various backbones and model analysis further verify the effectiveness of our method.},
  archive      = {J_TPAMI},
  author       = {Bowen Xing and Ivor W. Tsang},
  doi          = {10.1109/TPAMI.2025.3607794},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sentence-level relation semantics learning via contrastive sentences},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combo: Co-speech holistic 3D human motion generation and efficient customizable adaptation in harmony. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3607711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel framework, Combo, for harmonious co-speech holistic 3D human motion generation and efficient customizable adaption. In particular, we identify that one fundamental challenge as the multiple-input-multiple-output (MIMO) nature of the generative model of interest. More concretely, on the input end, the model typically consumes both speech signals and character guidance (e.g., identity and emotion), which hinders further adaptation to varying guidance; on the output end, holistic human motions mainly consist of facial expressions and body movements, which are inherently correlated but non-trivial to coordinate in current data-driven generation process. In response to the above challenge, we propose tailored designs to both ends. For the former, we propose to pre-train on data regarding a fixed identity with neutral emotion, and defer the incorporation of customizable conditions (identity and emotion) to fine-tuning stage, which is boosted by our novel X-Adapter for parameter-efficient fine-tuning. For the latter, we propose a simple yet effective transformer design, DU-Trans, which first divides into two branches to learn individual features of face expression and body movements, and then unites those to learn a joint bi-directional distribution and directly predicts combined coefficients. Evaluated on BEAT2 and SHOW datasets, Combo is highly effective in generating high-quality motions but also efficient in transferring identity and emotion. Project website: https://xc-csc101.github.io/combo/.},
  archive      = {J_TPAMI},
  author       = {Chao Xu and Mingze Sun and Zhi-Qi Cheng and Fei Wang and Yang Liu and Baigui Sun and Ruqi Huang and Alexander Hauptmann},
  doi          = {10.1109/TPAMI.2025.3607711},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Combo: Co-speech holistic 3D human motion generation and efficient customizable adaptation in harmony},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Segmenting the motion components of a video: A long-term unsupervised model. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3608065'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human beings have the ability to continuously analyze a video and immediately extract the motion components. We want to adopt this paradigm to provide a coherent and stable motion segmentation over the video sequence. In this perspective, we propose a novel long-term spatio-temporal model operating in a totally unsupervised way. It takes as input the volume of consecutive optical flow (OF) fields, and delivers a volume of segments of coherent motion over the video. More specifically, we have designed a transformer-based network, where we leverage a mathematically well-founded framework, the Evidence Lower Bound (ELBO), to derive the loss function. The loss function combines a flow reconstruction term involving spatio-temporal parametric motion models combining, in a novel way, polynomial (quadratic) motion models for the spatial dimensions and B-splines for the time dimension of the video sequence, and a regularization term enforcing temporal consistency on the segments. We report experiments on four VOS benchmarks, demonstrating competitive quantitative results while performing motion segmentation on a sequence in one go. We also highlight through visual results the key contributions on temporal consistency brought by our method.},
  archive      = {J_TPAMI},
  author       = {Etienne Meunier and Patrick Bouthemy},
  doi          = {10.1109/TPAMI.2025.3608065},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Segmenting the motion components of a video: A long-term unsupervised model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward effective knowledge distillation: Navigating beyond small-data pitfall. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3607982'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spectacular success of training large models on extensive datasets highlights the potential of scaling up for exceptional performance. To deploy these models on edge devices, knowledge distillation (KD) is commonly used to create a compact model from a larger, pretrained teacher model. However, as models and datasets rapidly scale up in practical applications, it is crucial to consider the applicability of existing KD approaches originally designed for limited-capacity architectures and small-scale datasets. In this paper, we revisit current KD methods and identify the presence of a small-data pitfall, where most modifications to vanilla KD prove ineffective on large-scale datasets. To guide the design of consistently effective KD methods across different data scales, we conduct a meticulous evaluation of the knowledge transfer process. Our findings reveal that incorporating more useful information is crucial for achieving consistently effective KD methods, while modifications in loss functions show relatively less significance. In light of this, we present a paradigmatic example that combines vanilla KD with deep supervision, incorporating additional information into the student during distillation. This approach surpasses almost all recent KD methods. We believe our study will offer valuable insights to guide the community in navigating beyond the small-data pitfall and toward consistently effective KD.},
  archive      = {J_TPAMI},
  author       = {Zhiwei Hao and Jianyuan Guo and Kai Han and Han Hu and Chang Xu and Yunhe Wang},
  doi          = {10.1109/TPAMI.2025.3607982},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Toward effective knowledge distillation: Navigating beyond small-data pitfall},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards real zero-shot camouflaged object segmentation without camouflaged annotations. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3600461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camouflaged Object Segmentation (COS) faces significant challenges due to the scarcity of annotated data, where meticulous pixel-level annotation is both labor-intensive and costly, primarily due to the intricate object-background boundaries. Addressing the core question, ”Can COS be effectively achieved in a zero-shot manner without manual annotations for any camouflaged object?”, we propose an affirmative solution. We analyze the learned attention patterns for camouflaged objects and introduce a robust zero-shot COS framework. Our findings reveal that while transformer models for salient object segmentation (SOS) prioritize global features in their attention mechanisms, camouflaged object segmentation exhibits both global and local attention biases. Based on these findings, we design a framework that adapts with the inherent local pattern bias of COS while incorporating global attention patterns and a broad semantic feature space derived from SOS. This enables efficient zero-shot transfer for COS. Specifically, We incorporate an Masked Image Modeling (MIM) based image encoder optimized for Parameter-Efficient Fine-Tuning (PEFT), a Multimodal Large Language Model (M-LLM), and a Multi-scale Fine-grained Alignment (MFA) mechanism. The MIM encoder captures essential local features, while the PEFT module learns global and semantic representations from SOS datasets. To further enhance semantic granularity, we leverage the M-LLM to generate caption embeddings conditioned on visual cues, which are meticulously aligned with multi-scale visual features via MFA. This alignment enables precise interpretation of complex semantic contexts. Moreover, we introduce a learnable codebook to represent the M-LLM during inference, significantly reducing computational demands while maintaining performance. Our framework demonstrates its versatility and efficacy through rigorous experimentation, achieving state-of-the-art performance in zero-shot COS with $F_{\beta }^{w}$ scores of 72.9% on CAMO and 71.7% on COD10K. By removing the M-LLM during inference, we achieve an inference speed comparable to that of traditional end-to-end models, reaching 18.1 FPS. Additionally, our method excels in polyp segmentation, and underwater scene segmentation, outperforming challenging baselines in both zero-shot and supervised settings, thereby highlighting its potential for broad applicability in diverse segmentation tasks.},
  archive      = {J_TPAMI},
  author       = {Cheng Lei and Jie Fan and Xinran Li and Tian-zhu Xiang and Ao Li and Ce Zhu and Le Zhang},
  doi          = {10.1109/TPAMI.2025.3600461},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards real zero-shot camouflaged object segmentation without camouflaged annotations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). H2OT: Hierarchical hourglass tokenizer for efficient video pose transformers. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3608284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformers have been successfully applied in the field of video-based 3D human pose estimation. However, the high computational costs of these video pose transformers (VPTs) make them impractical on resource-constrained devices. In this paper, we present a hierarchical plug-and-play pruning-and-recovering framework, called Hierarchical Hourglass Tokenizer (H2OT), for efficient transformer-based 3D human pose estimation from videos. H2OT begins with progressively pruning pose tokens of redundant frames and ends with recovering full-length sequences, resulting in a few pose tokens in the intermediate transformer blocks and thus improving the model efficiency. It works with two key modules, namely, a Token Pruning Module (TPM) and a Token Recovering Module (TRM). TPM dynamically selects a few representative tokens to eliminate the redundancy of video frames, while TRM restores the detailed spatio-temporal information based on the selected tokens, thereby expanding the network output to the original full-length temporal resolution for fast inference. Our method is general-purpose: it can be easily incorporated into common VPT models on both seq2seq and seq2frame pipelines while effectively accommodating different token pruning and recovery strategies. In addition, our H2OT reveals that maintaining the full pose sequence is unnecessary, and a few pose tokens of representative frames can achieve both high efficiency and estimation accuracy. Extensive experiments on multiple benchmark datasets demonstrate both the effectiveness and efficiency of the proposed method. Code and models are available at https://github.com/NationalGAILab/HoT.},
  archive      = {J_TPAMI},
  author       = {Wenhao Li and Mengyuan Liu and Hong Liu and Pichao Wang and Shijian Lu and Nicu Sebe},
  doi          = {10.1109/TPAMI.2025.3608284},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {H2OT: Hierarchical hourglass tokenizer for efficient video pose transformers},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reinterpreting hypergraph kernels: Insights through homomorphism analysis. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3608902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing expressive hypergraph kernels that can effectively capture high-order structural information is a fundamental challenge in hypergraph learning. In this paper, we propose a novel comparison framework based on hypergraph homomorphisms to evaluate and compare the expressive ability of existing hypergraph kernels. We revisit classical kernels such as Hypergraph Weisfeiler-Lehman (HG WL) and Hypergraph Rooted kernels, providing theoretical conditions under which they fail to distinguish non-isomorphic hypergraphs. Motivated by these insights, we introduce the Hypergraph Subtree-Cycle Kernel, which augments subtree-based features with cycle-based structural patterns to enhance expressiveness. We propose two variants: HG SCKernelv1 and HG SCKernelv2. Extensive experiments on five graph and ten hypergraph classification benchmarks demonstrate the superior performance of our methods, confirming the effectiveness of integrating homomorphism-guided design into hypergraph kernels.},
  archive      = {J_TPAMI},
  author       = {Yifan Zhang and Shaoyi Du and Yifan Feng and Shihui Ying and Yue Gao},
  doi          = {10.1109/TPAMI.2025.3608902},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reinterpreting hypergraph kernels: Insights through homomorphism analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). M3D: A multimodal, multilingual and multitask dataset for grounded document-level information extraction. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3609288'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal information extraction (IE) tasks have attracted increasing attention because many studies have shown that multimodal information benefits text information extraction. However, existing multimodal IE datasets mainly focus on sentence-level image-facilitated IE in English text, and pay little attention to video-based multimodal IE and fine-grained visual grounding. Therefore, in order to promote the development of multimodal IE, we constructed a multimodal multilingual multitask dataset, named M3D, which has the following features: (1) It contains paired document-level text and video to enrich multimodal information; (2) It supports two widely-used languages, namely English and Chinese; (3) It includes more multimodal IE tasks such as entity recognition, entity chain extraction, relation extraction and visual grounding. In addition, our dataset introduces an unexplored theme, i.e., biography, enriching the domains of multimodal IE resources. To establish a benchmark for our dataset, we propose an innovative hierarchical multimodal IE model. This model effectively leverages and integrates multimodal information through a Denoised Feature Fusion Module (DFFM). Furthermore, in non-ideal scenarios, modal information is often incomplete. Thus, we designed a Missing Modality Construction Module (MMCM) to alleviate the issues caused by missing modalities. Our model achieved an average performance of 53.80% and 53.77% on four tasks in English and Chinese datasets, respectively, which set a reasonable standard for subsequent research. In addition, we conducted more analytical experiments to verify the effectiveness of our proposed module. We believe that our work can promote the development of the field of multimodal IE.},
  archive      = {J_TPAMI},
  author       = {Jiang Liu and Bobo Li and Xinran Yang and Na Yang and Hao Fei and Mingyao Zhang and Fei Li and Donghong Ji},
  doi          = {10.1109/TPAMI.2025.3609288},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {M3D: A multimodal, multilingual and multitask dataset for grounded document-level information extraction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A unified perspective for loss-oriented imbalanced learning via localization. <em>TPAMI</em>, 1-19. (<a href='https://doi.org/10.1109/TPAMI.2025.3609440'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the inherent imbalance in real-world datasets, naïve Empirical Risk Minimization (ERM) tends to bias the learning process towards the majority classes, hindering generalization to minority classes. To rebalance the learning process, one straightforward yet effective approach is to modify the loss function via class-dependent terms, such as re-weighting and logit-adjustment. However, existing analysis of these loss-oriented methods remains coarse-grained and fragmented, failing to explain some empirical results. After reviewing prior work, we find that the properties used through their analysis are typically global, i.e., defined over the whole dataset. Hence, these properties fail to effectively capture how class-dependent terms influence the learning process. To bridge this gap, we turn to explore the localized versions of such properties i.e., defined within each class. Specifically, we employ localized calibration to provide consistency validation across a broader range of losses and localized Lipschitz continuity to provide a fine-grained generalization bound. In this way, we reach a unified perspective for improving and adjusting loss-oriented methods. Finally, a principled learning algorithm is developed based on these insights. Empirical results on both traditional ResNets and foundation models validate our theoretical analyses and demonstrate the effectiveness of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Zitai Wang and Qianqian Xu and Zhiyong Yang and Zhikang Xu and Linchao Zhang and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2025.3609440},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A unified perspective for loss-oriented imbalanced learning via localization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian unsupervised disentanglement of anatomy and geometry for deep groupwise image registration. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3609521'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article presents a general Bayesian learning framework for multi-modal groupwise image registration. The method builds on probabilistic modelling of the image generative process, where the underlying common anatomy and geometric variations of the observed images are explicitly disentangled as latent variables. Therefore, groupwise image registration is achieved via hierarchical Bayesian inference. We propose a novel hierarchical variational auto-encoding architecture to realise the inference procedure of the latent variables, where the registration parameters can be explicitly estimated in a mathematically interpretable fashion. Remarkably, this new paradigm learns groupwise image registration in an unsupervised closed-loop self-reconstruction process, sparing the burden of designing complex image-based similarity measures. The computationally efficient disentangled network architecture is also inherently scalable and flexible, allowing for groupwise registration on large-scale image groups with variable sizes. Furthermore, the inferred structural representations from multi-modal images via disentanglement learning are capable of capturing the latent anatomy of the observations with visual semantics. Extensive experiments were conducted to validate the proposed framework, including four different datasets from cardiac, brain, and abdominal medical images. The results have demonstrated the superiority of our method over conventional similarity-based approaches in terms of accuracy, efficiency, scalability, and interpretability.},
  archive      = {J_TPAMI},
  author       = {Xinzhe Luo and Xin Wang and Linda Shapiro and Chun Yuan and Jianfeng Feng and Xiahai Zhuang},
  doi          = {10.1109/TPAMI.2025.3609521},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Bayesian unsupervised disentanglement of anatomy and geometry for deep groupwise image registration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MV2DFusion: Leveraging modality-specific object semantics for multi-modal 3D detection. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3609348'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of autonomous vehicles has significantly increased the demand for robust 3D object detection systems. While cameras and LiDAR sensors each offer unique advantages-cameras provide rich texture information and LiDAR offers precise 3D spatial data-relying on a single modality often leads to performance limitations. This paper introduces MV2DFusion, a multi-modal detection framework that integrates the strengths of both worlds through an advanced query-based fusion mechanism. By introducing an image query generator to align with image-specific attributes and a point cloud query generator, MV2DFusion effectively combines modality-specific object semantics without biasing toward one single modality. Then the sparse fusion process can be accomplished based on the valuable object semantics, ensuring efficient and accurate object detection across various scenarios. Our framework's flexibility allows it to integrate with any image and point cloud-based detectors, showcasing its adaptability and potential for future advancements. Extensive evaluations on the nuScenes and Argoverse2 datasets demonstrate that MV2DFusion achieves state-of-the-art performance, particularly excelling in long-range detection scenarios.},
  archive      = {J_TPAMI},
  author       = {Zitian Wang and Zehao Huang and Yulu Gao and Naiyan Wang and Si Liu},
  doi          = {10.1109/TPAMI.2025.3609348},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MV2DFusion: Leveraging modality-specific object semantics for multi-modal 3D detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Single voter spreading for efficient correspondence grouping and 3D registration. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3609474'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining highly consistent correspondences between point clouds is crucial for computer vision tasks such as 3D registration and recognition. Due to nuisances such as limited overlap and noise, initial correspondences often contain a large number of outliers, imposing a great challenge to downstream tasks. In this paper, we present a novel single voter spreading (SVOS) method for efficient 3D correspondence grouping and 3D registration. Our core insight is to leverage low-order graph constraints only in a single voter spreading voting scheme to achieve comparable constrain-ability as complex constraints without searching them. First, a simple first-order graph is constructed for the initial correspondence set. Second, a two-stage voting method is proposed, including single voter voting and spread voters voting. Each voting stage involves both local and global voting via edge constraints only. This promises good selectivity while making the voting process time- and storage-efficient. Finally, top-scored correspondences are opted for robust transformation estimation. Experiments on U3M, 3DMatch/3DLoMatch, ETH, and KITTI-LC datasets verify that SVOS achieves new state-of-the-art correspondence grouping and registration performance, while being light-weight and robust to graph construction parameters. The code will be available at https://github.com/ZhaoZeng-pro/SVOS.},
  archive      = {J_TPAMI},
  author       = {Siwen Quan and Zhao Zeng and Xiyu Zhang and Jiaqi Yang},
  doi          = {10.1109/TPAMI.2025.3609474},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Single voter spreading for efficient correspondence grouping and 3D registration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mask-DiFuser: A masked diffusion model for unified unsupervised image fusion. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3609323'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The absence of ground truth (GT) in most fusion tasks poses significant challenges for model optimization, evaluation, and generalization. Existing fusion methods achieving complementary context aggregation predominantly rely on hand-crafted fusion rules and sophisticated loss functions, which introduce subjectivity and often fail to adapt to complex real-world scenarios. To address this challenge, we propose Mask-DiFuser, a novel fusion paradigm that ingeniously transforms the unsupervised image fusion task into a dual masked image reconstruction task by incorporating masked image modeling with a diffusion model, overcoming various issues arising from the absence of GT. In particular, we devise a dual masking scheme to simulate complementary information and employ a diffusion model to restore source images from two masked inputs, thereby aggregating complementary contexts. A content encoder with an attention parallel feature mixer is deployed to extract and integrate complementary features, offering local content guidance. Moreover, a semantic encoder is developed to supply global context which is integrated into the diffusion model via a cross-attention mechanism. During inference, Mask-DiFuser begins with a Gaussian distribution and iteratively denoises it conditioned on multi-source images to directly generate fused images. The masked diffusion model, learning priors from high-quality natural images, ensures that fusion results align more closely with human visual perception. Extensive experiments on several fusion tasks, including infrared-visible, medical, multi-exposure, and multi-focus image fusion, demonstrate that Mask-DiFuser significantly outshines SOTA fusion alternatives.},
  archive      = {J_TPAMI},
  author       = {Linfeng Tang and Chunyu Li and Jiayi Ma},
  doi          = {10.1109/TPAMI.2025.3609323},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Mask-DiFuser: A masked diffusion model for unified unsupervised image fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REST: Holistic learning for end-to-end semantic segmentation of whole-scene remote sensing imagery. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3609767'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation of remote sensing imagery (RSI) is a fundamental task that aims at assigning a category label to each pixel. To pursue precise segmentation with one or more fine-grained categories, semantic segmentation often requires holistic segmentation of whole-scene RSI (WRI), which is normally characterized by a large size. However, conventional deep learning methods struggle to handle holistic segmentation of WRI due to the memory limitations of the graphics processing unit (GPU), thus requiring to adopt suboptimal strategies such as cropping or fusion, which result in performance degradation. Here, we introduce the Robust End-to-end semantic Segmentation architecture for whole-scene remoTe sensing imagery (REST). REST is the first intrinsically end-to-end framework for truly holistic segmentation of WRI, supporting a wide range of encoders and decoders in a plug-and-play fashion. It enables seamless integration with mainstream semantic segmentation methods, and even more advanced foundation models. Specifically, we propose a novel spatial parallel interaction mechanism (SPIM) within REST to overcome GPU memory constraints and achieve global context awareness. Unlike traditional parallel methods, SPIM enables REST to process a WRI effectively and efficiently by combining parallel computation with a divide-and-conquer strategy. Both theoretical analysis and experiments demonstrate that REST attains near-linear throughput scalability as additional GPUs are employed. Extensive experiments demonstrate that REST consistently outperforms existing cropping-based and fusion-based methods across a variety of scenarios, ranging from single-class to multi-class segmentation, from multispectral to hyperspectral imagery, and from satellite to drone platforms. The robustness and versatility of REST are expected to offer a promising solution for the holistic segmentation of WRI, with the potential for further extension to large-size medical imagery segmentation. The source code will be released at https://weichenrs.github.io/REST.},
  archive      = {J_TPAMI},
  author       = {Wei Chen and Lorenzo Bruzzone and Bo Dang and Yuan Gao and Youming Deng and Jin-Gang Yu and Liangqi Yuan and Yansheng Li},
  doi          = {10.1109/TPAMI.2025.3609767},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {REST: Holistic learning for end-to-end semantic segmentation of whole-scene remote sensing imagery},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DreamReward-X: Boosting high-quality 3D generation with human preference alignment. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3609680'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in 3D content generation have shown remarkable success by leveraging pretrained large-scale diffusion models. However, existing 3D generation results are far from perfect as one primary challenge lies in aligning 3D content with human preference, especially in text-driven 3D generation. In this paper, we propose a novel 3D generation framework, coined DreamReward, to learn and improve text-driven 3D generation models from human preference feedback. First, we collect 25K+ expert comparisons based on a systematic annotation pipeline including filtering, rating, and ranking. Then, we build Reward3D, the first general-purpose text-to-3D human preference reward model to encode human preferences effectively. Building upon the 3D reward model, we finally perform theoretical analysis and present the Reward3D Feedback Learning (DreamFL) algorithm to guide the noisy pretrained distribution toward the actual user-prompt distributions in optimization. With the rapid development and growing popularity of 4D and image-driven 3D generation, we further extend our DreamReward into 4D generation (DreamReward-4D) and image-to-3D generation (DreamReward-img) in a low-cost but effective manner. Despite the impressive results created by DreamReward, the diversity in text-driven 3D generation is limited due to inherent maximum likelihood-seeking issues. To address this, we explore the gap between Denoising Diffusion Implicit Models (DDIM) and SDS-based DreamFL in the generation process and propose DreamReward++, where we introduce a reward-aware noise sampling strategy to unleash text-driven diversity during the generation process while ensuring human preference alignment. Grounded by theoretical proof and extensive experiment comparisons, our method successfully generates high-fidelity and diverse 3D results with significant boosts in prompt alignment with human intention. Our results demonstrate the great potential for learning from human feedback to improve 3D generation.},
  archive      = {J_TPAMI},
  author       = {Fangfu Liu and Junliang Ye and Yikai Wang and Hanyang Wang and Zhengyi Wang and Jun Zhu and Yueqi Duan},
  doi          = {10.1109/TPAMI.2025.3609680},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DreamReward-X: Boosting high-quality 3D generation with human preference alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Object detection data synthesis via box-to-image generation based on diffusion models. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3609962'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern diffusion-based image generative models have made significant progress and become promising to enrich training data for the object detection task. However, the generation quality and the controllability for complex scenes containing multi-class objects and dense objects with occlusions remain limited. This paper presents ODGEN, a novel method to generate high-quality images conditioned on bounding boxes, thereby facilitating data synthesis for object detection. Given a domain-specific object detection dataset, we first fine-tune a pre-trained diffusion model on both cropped foreground objects and entire images to fit target distributions. Then we propose to control the diffusion model using synthesized visual prompts with spatial constraints and object-wise textual descriptions. ODGEN exhibits robustness in handling complex scenes and specific domains. Further, we design a dataset synthesis pipeline to evaluate ODGEN on 7 domain-specific benchmarks to demonstrate its effectiveness. Adding training data generated by ODGEN improves up to 25.3% mAP@.50:.95 with object detectors like YOLOv5 and YOLOv7, outperforming prior controllable generative methods. We also design an evaluation protocol based on COCO-2014 to validate the synthetic data of ODGEN in general domains and observe an advantage up to 5.6% in mAP@.50:.95 against existing methods. In addition, we employ a series of large-scale object detection datasets to train a general model named Stable Box Diffusion, which covers thousands of object categories in most common scenes.},
  archive      = {J_TPAMI},
  author       = {Jingyuan Zhu and Huimin Ma and Jiansheng Chen and Jian Yuan},
  doi          = {10.1109/TPAMI.2025.3609962},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Object detection data synthesis via box-to-image generation based on diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parse trees guided LLM prompt compression. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3609956'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Offering rich contexts to Large Language Models (LLMs) has shown to boost the performance in various tasks, but the resulting longer prompt would increase the computational cost and might exceed the input limit of LLMs. Recently, some prompt compression methods have been suggested to shorten the length of prompts by using language models to generate shorter prompts or by developing computational models to select important parts of original prompt. The generative compression methods would suffer from issues like hallucination, while the selective compression methods have not involved linguistic rules and overlook the global structure of prompt. To this end, we propose a novel selective compression method called PartPrompt. It first obtains a parse tree for each sentence based on linguistic rules, and calculates local information entropy for each node in a parse tree. These local parse trees are then organized into a global tree according to the hierarchical structure such as the dependency of sentences, paragraphs, and sections. After that, the root-ward propagation and leaf-ward propagation are proposed to adjust node values over the global tree. Finally, a recursive algorithm is developed to prune the global tree based on the adjusted node values. The experiments show that PartPrompt receives the state-of-the-art performance across various datasets, metrics, compression ratios, and target LLMs for inference. The in-depth ablation studies confirm the effectiveness of designs in PartPrompt, and other additional experiments also demonstrate its superiority in terms of the coherence of compressed prompts and in the extreme long prompt scenario.},
  archive      = {J_TPAMI},
  author       = {Wenhao Mao and Chengbin Hou and Tianyu Zhang and Xinyu Lin and Ke Tang and Hairong Lv},
  doi          = {10.1109/TPAMI.2025.3609956},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Parse trees guided LLM prompt compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards size-invariant salient object detection: A generic evaluation and optimization approach. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3609882'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a fundamental yet underexplored issue in Salient Object Detection (SOD): the size-invariant property for evaluation protocols, particularly in scenarios when multiple salient objects of significantly different sizes appear within a single image. We first present a novel perspective to expose the inherent size sensitivity of existing widely used SOD metrics. Through careful theoretical derivations, we show that the evaluation outcome of an image under current SOD metrics can be essentially decomposed into a sum of several separable terms, with the contribution of each term being directly proportional to its corresponding region size. Consequently, the prediction errors would be dominated by the larger regions, while smaller yet potentially more semantically important objects are often overlooked, leading to biased performance assessments and practical degradation. To address this challenge, a generic Size-Invariant Evaluation (SIEva) framework is proposed. The core idea is to evaluate each separable component individually and then aggregate the results, thereby effectively mitigating the impact of size imbalance across objects. Building upon this, we further develop a dedicated optimization framework (SIOpt), which adheres to the size-invariant principle and significantly enhances the detection of salient objects across a broad range of sizes. Notably, SIOpt is model-agnostic and can be seamlessly integrated with a wide range of SOD backbones. Theoretically, we also present generalization analysis of SOD methods and provide evidence supporting the validity of our new evaluation protocols. Finally, comprehensive experiments speak to the efficacy of our proposed approach.},
  archive      = {J_TPAMI},
  author       = {Shilong Bao and Qianqian Xu and Feiran Li and Boyu Han and Zhiyong Yang and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2025.3609882},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards size-invariant salient object detection: A generic evaluation and optimization approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive batch size time evolving stochastic gradient descent for federated learning. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3610169'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variance reduction has been shown to improve the performance of Stochastic Gradient Descent (SGD) in centralized machine learning. However, when it is extended to federated learning systems, many issues may arise, including (i) mega-batch size settings; (ii) additional noise introduced by the gradient difference between the current iteration and the snapshot point; and (iii) gradient (statistical) heterogeneity. In this paper, we propose a lightweight algorithm termed federated adaptive batch size time evolving variance reduction (FedATEVR) to tackle these issues, consisting of an adaptive batch size setting scheme and a time-evolving variance reduction gradient estimator. In particular, we use the historical gradient information to set an appropriate mega-batch size for each client, which can steadily accelerate the local SGD process and reduce the computation cost. The historical information involves both global and local gradient, which mitigates unstable varying in mega-batch size introduced by gradient heterogeneity among the clients. For each client, the gradient difference between the current iteration and the snapshot point is used to tune the time-evolving weight of the variance reduction term in the gradient estimator. This can avoid meaningless variance reduction caused by the out-of-date snapshot point gradient. We theoretically prove that our algorithm can achieve a linear speedup of of $\mathcal {O}(\frac{1}{\sqrt{SKT}})$ for non-convex objective functions under partial client participation. Extensive experiments demonstrate that our proposed method can achieve higher test accuracy than the baselines and decrease communication rounds greatly.},
  archive      = {J_TPAMI},
  author       = {Xuming An and Li Shen and Yong Luo and Han Hu and Dacheng Tao},
  doi          = {10.1109/TPAMI.2025.3610169},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adaptive batch size time evolving stochastic gradient descent for federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ACLI: A CNN pruning framework leveraging adjacent convolutional layer interdependence and $\gamma$-weakly submodularity. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3610113'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Today, convolutional neural network (CNN) pruning techniques often rely on manually crafted importance criteria and pruning structures. Due to their heuristic nature, these methods may lack generality, and their performance is not guaranteed. In this paper, we propose a theoretical framework to address this challenge by leveraging the concept of $\gamma$-weak submodularity, based on a new efficient importance function. By deriving an upper bound on the absolute error in the layer subsequent to the pruned layer, we formulate the importance function as a $\gamma$-weakly submodular function. This formulation enables the development of an easy-to-implement, low-complexity, and data-free oblivious algorithm for selecting filters to be removed from a convolutional layer. Extensive experiments show that our method outperforms state-of-the-art benchmark networks across various datasets, with a computational cost comparable to the simplest pruning techniques, such as $l_{2}$-norm pruning. Notably, the proposed method achieves an accuracy of 76.52%, compared to 75.15% for the overall best baseline, with a 25.5% reduction in network parameters. According to our proposed resource-efficiency metric for pruning methods, the ACLI approach demonstrates orders-of-magnitude higher efficiency than the other baselines, while maintaining competitive accuracy.},
  archive      = {J_TPAMI},
  author       = {S. Tofigh and M. Askarizadeh and M. Omair Ahmad and M.N.S. Swamy and KK Nguyen},
  doi          = {10.1109/TPAMI.2025.3610113},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ACLI: A CNN pruning framework leveraging adjacent convolutional layer interdependence and $\gamma$-weakly submodularity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting transferable adversarial images: Systemization, evaluation, and new insights. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3610085'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferable adversarial images raise critical security concerns for computer vision systems in real-world, blackbox attack scenarios. Although many transfer attacks have been proposed, existing research lacks a systematic and comprehensive evaluation. In this paper, we systemize transfer attacks into five categories around the general machine learning pipeline and provide the first comprehensive evaluation, with 23 representative attacks against 11 representative defenses, including the recent, transfer-oriented defense and the real-world Google Cloud Vision. In particular, we identify two main problems of existing evaluations: (1) for attack transferability, lack of intra-category analyses with fair hyperparameter settings, and (2) for attack stealthiness, lack of diverse measures. Our evaluation results validate that these problems have indeed caused misleading conclusions and missing points, and addressing them leads to new, consensuschallenging insights, such as (1) an early attack, DI, even outperforms all similar follow-up ones, (2) the state-of-the-art (whitebox) defense, DiffPure, is even vulnerable to (black-box) transfer attacks, and (3) even under the same Lp constraint, different attacks yield dramatically different stealthiness results regarding diverse imperceptibility metrics, finer-grained measures, and a user study. We hope that our analyses will serve as guidance on properly evaluating transferable adversarial images and advance the design of attacks and defenses.},
  archive      = {J_TPAMI},
  author       = {Zhengyu Zhao and Hanwei Zhang and Renjue Li and Ronan Sicre and Laurent Amsaleg and Michael Backes and Qi Li and Qian Wang and Chao Shen},
  doi          = {10.1109/TPAMI.2025.3610085},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Revisiting transferable adversarial images: Systemization, evaluation, and new insights},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generative causality-driven network for graph multi-task learning. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3610096'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task learning (MTL) is a standard learning paradigm in machine learning. The central idea of MTL is to capture the shared knowledge among multiple tasks for mitigating the problem of data sparsity where the annotated samples for each task are quite limited. Recent studies indicate that graph multi-task learning (GMTL) yields the promising improvement over previous MTL methods. GMTL represents tasks on a task relation graph, and further leverages graph neural networks (GNNs) to learn complex task relationships. Although GMTL achieves the better performance, the construction of task relation graph heavily depends on simple heuristic tricks, which results in the existence of spurious task correlations and the absence of true edges between tasks with strong connections. This problem largely limits the effectiveness of GMTL. To this end, we propose the Generative Causality-driven Network (GCNet), a novel framework that progressively learns the causal structure between tasks to discover which tasks are beneficial to be jointly trained for improving generalization ability and model robustness. To be specific, in the feature space, GCNet first introduces a feature-level generator to generate the structure prior for reducing learning difficulty. Afterwards, GCNet develops a output-level generator which is parameterized as a new causal energy-based model (EBM) to refine the learned structure prior in the output space driven by causality. Benefiting from our proposed causal framework, we theoretically derive an intervention contrastive estimation for training this causal EBM efficiently. Experiments are conducted on multiple synthetic and real-world datasets. Extensive empirical results and model analyses demonstrate the superior performance of GCNet over several competitive MTL baselines.},
  archive      = {J_TPAMI},
  author       = {Xixun Lin and Qing Yu and Yanan Cao and Lixin Zou and Chuan Zhou and Jia Wu and Chenliang Li and Peng Zhang and Shirui Pan},
  doi          = {10.1109/TPAMI.2025.3610096},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generative causality-driven network for graph multi-task learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Step-wise distribution-aligned style prompt tuning for source-free cross-domain few-shot learning. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3610039'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing cross-domain few-shot learning (CDFSL) methods, which develop training strategies in the source domain to enhance model transferability, face challenges when applied to large-scale pre-trained models (LMs), as their source domains and training strategies are not accessible. Besides, fine-tuning LMs specifically for CDFSL requires substantial computational resources, which limits their practicality. Therefore, this paper investigates the source-free CDFSL (SF-CDFSL) problem to solve the few-shot learning (FSL) task in target domain using only a pre-trained model and a few target samples, without requiring source data or training strategies. However, the inaccessibility of source data prevents explicitly reducing the domain gaps between the source and target. To tackle this challenge, this paper proposes a novel approach, Step-wise Distribution-aligned Style Prompt Tuning (StepSPT), to implicitly narrow the domain gaps from the perspective of prediction distribution optimization. StepSPT initially proposes a style prompt that adjusts the target samples to mirror the expected distribution. Furthermore, StepSPT tunes the style prompt and classifier by exploring a dual-phase optimization process (external and internal processes). In the external process, a step-wise distribution alignment strategy is introduced to tune the proposed style prompt by factorizing the prediction distribution optimization problem into the multi-step distribution alignment problem. In the internal process, the classifier is updated via standard cross-entropy loss. Evaluation on 5 datasets illustrates the superiority of StepSPT over existing prompt tuning-based methods and state-of-the-art methods (SOTAs). Furthermore, ablation studies and performance analyzes highlight the efficacy of StepSPT. The code will be made public at https://github.com/xuhuali-mxj/StepSPT.},
  archive      = {J_TPAMI},
  author       = {Huali Xu and Li Liu and Tianpeng Liu and Shuaifeng Zhi and Shuzhou Sun and Ming-Ming Cheng},
  doi          = {10.1109/TPAMI.2025.3610039},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Step-wise distribution-aligned style prompt tuning for source-free cross-domain few-shot learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). I&S-ViT: An inclusive & stable method for post-training ViTs quantization. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3610466'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Albeit the scalable performance of vision transformers (ViTs), the dense computational costs undermine their position in industrial applications. Post-training quantization (PTQ), tuning ViTs with a tiny dataset and running in a low-bit format, well addresses the cost issue but unluckily bears more performance drops in lower-bit cases. In this paper, we introduce I&S-ViT, a novel method that regulates the PTQ of ViTs in an inclusive and stable fashion. I&S-ViT first identifies two issues in the PTQ of ViTs: (1) Quantization inefficiency in the prevalent log2 quantizer for post-Softmax activations; (2) Rugged and magnified loss landscape in coarse-grained quantization granularity for post-LayerNorm activations. Then, I&S-ViT addresses these issues by introducing: (1) A novel shift-uniform-log2 quantizer (SULQ) that incorporates a shift mechanism followed by uniform quantization to achieve both an inclusive domain representation and accurate distribution approximation; (2) A three-stage smooth optimization strategy (SOS) that amalgamates the strengths of channel-wise and layer-wise quantization to enable stable learning. Comprehensive evaluations across diverse vision tasks validate I&S-ViT's superiority over existing PTQ of ViTs methods, particularly in low-bit scenarios. For instance, I&S-ViT elevates the performance of W3A3 ViT-B by an impressive 50.68%. Our code is available at https://github.com/zysxmu/IaS-ViT.},
  archive      = {J_TPAMI},
  author       = {Yunshan Zhong and Jiawei Hu and Mingbao Lin and Mengzhao Chen and Rongrong Ji},
  doi          = {10.1109/TPAMI.2025.3610466},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {I&S-ViT: An inclusive & stable method for post-training ViTs quantization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MSFA image denoising using physics-based noise model and noise-decoupled network. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3610243'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multispectral filter array (MSFA) camera is increasingly used due to its compact size and fast capturing speed. However, because of its narrow-band property, it often suffers from the light-deficient problem, and images captured are easily overwhelmed by noise. As a type of commonly used denoising method, neural networks have shown their power to achieve satisfactory denoising results. However, their performance highly depends on high-quality noisy-clean image pairs. For the task of MSFA image denoising, there is currently neither a paired real dataset nor an accurate noise model capable of generating realistic noisy images. To this end, we present a physics-based noise model that is capable to match the real noise distribution and synthesize realistic noisy images. In our noise model, those different types of noise can be divided into SimpleDist component and ComplexDist component. The former contains all the types of noise that can be described using a simple probability distribution like Gaussian or Poisson distribution, and the latter contains the complicated color bias noise that cannot be modeled using a simple probability distribution. Besides, we design a noise-decoupled network consisting of a SimpleDist noise removal network (SNRNet) and a ComplexDist noise removal network (CNRNet) to sequentially remove each component. Moreover, according to the non-uniformity of color bias noise in our noise model, we introduce a learnable position embedding in CNRNet to indicate the position information. To verify the effectiveness of our physics-based noise model and noise-decoupled network, we collect a real MSFA denoising dataset with paired long-exposure clean images and short-exposure noisy images. Experiments are conducted to prove that the network trained using synthetic data generated by our noise model performs as well as trained using paired real data, and our noise-decoupled network outperforms other state-of-the-art denoising methods. The project page is avaliable at https://github.com/ying-fu/msfa denoising.},
  archive      = {J_TPAMI},
  author       = {Yuqi Jiang and Ying Fu and Qiankun Liu and Jun Zhang},
  doi          = {10.1109/TPAMI.2025.3610243},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MSFA image denoising using physics-based noise model and noise-decoupled network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D hand pose estimation via articulated anchor-to-joint 3D local regressors. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3609907'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose to address monocular 3D hand pose estimation from a single RGB or depth image via articulated anchor-to-joint 3D local regressors, in form of A2J-Transformer+. The key idea is to make the local regressors (i.e., anchor points) in 3D space be aware of hand's local fine details and global articulated context jointly, to facilitate predicting their 3D offsets toward hand joints with linear weighted aggregation for joint localization. Our intuition is that, local fine details help to estimate accurate offset but may suffer from the issues including serious occlusion, confusing similar patterns, and overfitting risk. On the other hand, hand's global articulated context can essentially provide additional descriptive clues and constraints to alleviate these issues. To set anchor points adaptively in 3D space, A2J-Transformer+ runs in a 2-stage manner. At the first stage, since the input modality property anchor points distribute more densely on X-Y plane, it leads to lower prediction accuracy along Z direction compared with those in the X and Y directions. To alleviate this, at the second stage anchor points are set near the joints yielded by the first stage evenly along X, Y, and Z directions. This treatment brings two main advantages: (1) balancing the prediction accuracy along X, Y, and Z directions, and (2) ensuring the anchor-joint offsets are of small values relatively easy to estimate. Wide-range experiments on three RGB hand datasets (InterHand2.6M, HO-3D V2 and RHP) and three depth hand datasets (NYU, ICVL and HANDS 2017) verify A2J-Transformer+'s superiority and generalization ability for different modalities (i.e., RGB and depth) and hand cases (i.e., single hand, interacting hands, and hand-object interaction), even outperforming model-based manners. The test on ITOP dataset reveals that, A2J-Transformer+ can also be applied to 3D human pose estimation task. The source code and supporting material will be released upon acceptance.},
  archive      = {J_TPAMI},
  author       = {Changlong Jiang and Yang Xiao and Jinghong Zheng and Haohong Kuang and Cunlin Wu and Mingyang Zhang and Zhiguo Cao and Min Du and Joey Tianyi Zhou and Junsong Yuan},
  doi          = {10.1109/TPAMI.2025.3609907},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {3D hand pose estimation via articulated anchor-to-joint 3D local regressors},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient nearest neighbor search using dynamic programming. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3610211'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a collection of points in $\mathbb {R}^{3}$, KD-Tree and R-Tree are well-known nearest neighbor search (NNS) algorithms that rely on spatial partitioning and indexing techniques. However, when the query point is far from the data points or the data points inherently represent a 2-manifold surface, their query performance may degrade. To address this, we propose a novel dynamic programming technique that precomputes a Directed Acyclic Graph (DAG) to encode the proximity structure between data points. More specifically, the DAG captures how the proximity structure evolves during the incremental construction of the Voronoi diagram of the data points. Experimental results demonstrate that our method achieves a speed increase of 1-10x. Furthermore, our algorithm demonstrates significant practical value in diverse applications. We validated its effectiveness through extensive testing in four key applications: Point-to-Mesh Distance Queries, Iterative Closest Point (ICP) Registration, Density Peak Clustering, and Point-to-Segments Distance Queries. A particularly notable feature of our approach is its unique ability to efficiently identify the nearest neighbor among the first $k$ points in the point cloud, a capability that enables substantial acceleration in low-dimensional applications like Density Peak Clustering. As a natural extension of our incremental construction process, our method can also be readily adapted for farthest-point sampling tasks. These experimental results across multiple domains underscore the broad applicability and practical importance of our approach.},
  archive      = {J_TPAMI},
  author       = {Pengfei Wang and Jiantao Song and Shiqing Xin and Shuangmin Chen and Changhe Tu and Wenping Wang and Jiaye Wang},
  doi          = {10.1109/TPAMI.2025.3610211},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient nearest neighbor search using dynamic programming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Task-distributionally robust data-free meta-learning. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3609625'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data-Free Meta-Learning (DFML) aims to enable efficient learning of unseen few-shot tasks, by meta-learning from multiple pre-trained models without accessing their original training data. While existing DFML methods typically generate synthetic data from these models to perform meta-learning, a comprehensive analysis of DFML's robustness-particularly its failure modes and vulnerability to potential attacks-remains notably absent. Such an analysis is crucial as algorithms often operate in complex and uncertain real-world environments. This paper fills this significant gap by systematically investigating the robustness of DFML, identifying two critical but previously overlooked vulnerabilities: Task-Distribution Shift (TDS) and Task-Distribution Corruption (TDC). TDS refers to the sequential shifts in the evolving task distribution, leading to the catastrophic forgetting of previously learned meta-knowledge. TDC exposes a security flaw of DFML, revealing its susceptibility to attacks when the pre-trained model pool includes untrustworthy models that deceptively claim to be beneficial but are actually harmful. To mitigate these vulnerabilities, we propose a trustworthy DFML framework comprising three components: synthetic task reconstruction, meta-learning with task memory interpolation, and automatic model selection. Specifically, utilizing model inversion techniques, we reconstruct synthetic tasks from multiple pre-trained models to perform meta-learning. To prevent forgetting, we introduce a strategy to replay interpolated historical tasks to efficiently recall previous meta-knowledge. Furthermore, our framework seamlessly incorporates an automatic model selection mechanism to automatically filter out untrustworthy models during the meta-learning process. Extensive experiments across various datasets with two types of untrustworthy models confirm the superiority of our method in significantly enhancing the robustness of DFML. Code is available at https://github.com/Egg-Hu/Trustworthy-DFML.},
  archive      = {J_TPAMI},
  author       = {Zixuan Hu and Yongxian Wei and Li Shen and Zhenyi Wang and Baoyuan Wu and Chun Yuan and Dacheng Tao},
  doi          = {10.1109/TPAMI.2025.3609625},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Task-distributionally robust data-free meta-learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SNNTracker: Online high-speed multi-object tracking with spike camera. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3610696'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-object tracking (MOT) is crucial for applications such as autonomous driving and robotics, yet traditional image-based methods struggle in high-speed scenarios due to motion blur and temporal gaps caused by low frame rates. Spike cameras, with their ability to continuously record spatiotemporal signals, overcome these limitations. However, existing spike-based methods often rely on intermediate image reconstruction or discrete clustering, which limits their real-time performance and temporal continuity. To address this, we propose SNNTracker, the first fully spiking neural network (SNN)-based MOT algorithm tailored for spike cameras. SNNTracker integrates a dynamic neural field (DNF)-based attention mechanism for target detection and a winner-take-all (WTA)-based tracking module with online spike-timing-dependent plasticity (STDP) for adaptive learning of object trajectories. By directly processing spike streams without reconstruction, SNNTracker reduces latency, computational overhead, and dependency on image quality, making it ideal for ultra-high-speed environments. It maintains robust, continuous tracking even under occlusions, severe lighting variations, or temporary object disappearance, by leveraging SNN-estimated motion predictions and long-term online clustering. We construct three types of spike-camera MOT datasets covering dense and sparse annotations across diverse real-world scenarios, including camera ego-motion, deformable and ultra-fast motion (up to 2600 RPM), occlusion, indoor/outdoor lighting changes, and low-visibility tracking. Extensive experiments demonstrate that SNNTracker consistently outperforms state-of-the-art MOT methods—both ANN- and SNN-based—achieving MOTA scores above 96% and up to 100% in many sequences. Our results highlight the advantages of spike-driven SNNs for low-latency, high-speed, and label-free multi-object tracking, advancing neuromorphic vision for real-time perception.},
  archive      = {J_TPAMI},
  author       = {Yajing Zheng and Chengen Li and Jiyuan Zhang and Zhaofei Yu and Tiejun Huang},
  doi          = {10.1109/TPAMI.2025.3610696},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SNNTracker: Online high-speed multi-object tracking with spike camera},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PlaneRecTR++: Unified query learning for joint 3D planar reconstruction and pose estimation. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3610500'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D plane reconstruction from images can usually be divided into several sub-tasks of plane detection, segmentation, parameters regression and possibly depth prediction for per-frame, along with plane correspondence and relative camera pose estimation between frames. Previous works tend to divide and conquer these sub-tasks with distinct network modules, overall formulated by a two-stage paradigm. With an initial camera pose and per-frame plane predictions provided from the first stage, exclusively designed modules, potentially relying on extra plane correspondence labelling, are applied to merge multi-view plane entities and produce 6DoF camera pose. As none of existing works manage to integrate above closely related sub-tasks into a unified framework but treat them separately and sequentially, we suspect it potentially as a main source of performance limitation for existing approaches. Motivated by this finding and the success of query-based learning in enriching reasoning among semantic entities, in this paper, we propose PlaneRecTR++, a Transformer-based architecture, which for the first time unifies all sub-tasks related to multi-view reconstruction and pose estimation with a compact single-stage model, refraining from initial pose estimation and plane correspondence supervision. Extensive quantitative and qualitative experiments demonstrate that our proposed unified learning achieves mutual benefits across sub-tasks, obtaining a new state-of-the-art performance on public ScanNetv1, ScanNetv2, NYUv2-Plane, and MatterPort3D datasets.},
  archive      = {J_TPAMI},
  author       = {Jingjia Shi and Shuaifeng Zhi and Kai Xu},
  doi          = {10.1109/TPAMI.2025.3610500},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PlaneRecTR++: Unified query learning for joint 3D planar reconstruction and pose estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). StyleShot: A snapshot on any style. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3610614'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image Style Transfer aims to replicate the style of a reference image based on the content from a text description or another image. With the significant advancements in image generation through diffusion models, recent studies have attempted to either fine-tuning embeddings to learn the single style or utilizing the pre-trained CLIP image encoder to extract style representations. However, style-tuning requires substantial computational resources and the pre-trained CLIP image encoder is trained for semantic understanding rather than for style representation. To address these challenges, we introduce a style-aware encoder and a well-organized style dataset called StyleGallery to learn a good style representation that is crucial and sufficient for generalized style transfer without test-time tuning. With dedicated design for style learning, this style-aware encoder is trained to extract expressive style representation from multi-level patches with decoupling training strategy, and StyleGallery enables the generalization ability. Moreover, we employ a content extraction and content-fusion encoder to enhance image-driven style transfer. We highlight that, our approach, named StyleShot, is simple yet effective in mimicking various desired styles, i.e., 3D, flat, abstract or even fine-grained styles, without test-time tuning. Rigorous experiments validate that, StyleShot achieves superior performance across a wide range of styles compared to existing state-of-the-art text- and image-driven methods.},
  archive      = {J_TPAMI},
  author       = {Junyao Gao and Yanan Sun and Yanchen Liu and Yinhao Tang and Yanhong Zeng and Ding Qi and Kai Chen and Cairong Zhao},
  doi          = {10.1109/TPAMI.2025.3610614},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {StyleShot: A snapshot on any style},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LVOS: A benchmark for large-scale long-term video object segmentation. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3611020'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video object segmentation (VOS) aims to distinguish and track target objects in a video. Despite the excellent performance achieved by off-the-shelf VOS models, part of the existing VOS benchmarks mainly focuses on short-term videos, where objects remain visible most of the time. However, these benchmarks may not fully capture challenges encountered in practical applications, and the absence of long-term datasets restricts further investigation of VOS in realistic scenarios. Thus, we propose a novel benchmark named LVOS, comprising 720 videos with 296,401 frames and 407,945 high-quality annotations. Videos in LVOS last 1.14 minutes on average. Each video includes various attributes, especially challenges encountered in the wild, such as long-term reappearing and cross-temporal similar objects. Compared to previous benchmarks, our LVOS better reflects VOS models' performance in real scenarios. Based on LVOS, we evaluate 15 existing VOS models under 3 different settings and conduct a comprehensive analysis. On LVOS, these models suffer a large performance drop, highlighting the challenge of achieving precise tracking and segmentation in real-world scenarios. Attribute-based analysis indicates that one of the significant factors contributing to accuracy decline is the increased video length, interacting with complex challenges such as long-term reappearance, cross-temporal confusion, and occlusion, which emphasize LVOS's crucial role. We hope our LVOS can advance development of VOS in real scenes.},
  archive      = {J_TPAMI},
  author       = {Lingyi Hong and Zhongying Liu and Wenchao Chen and Chenzhi Tan and Yuang Feng and Xinyu Zhou and Pinxue Guo and Jinglun Li and Zhaoyu Chen and Shuyong Gao and Wei Zhang and Wenqiang Zhang},
  doi          = {10.1109/TPAMI.2025.3611020},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LVOS: A benchmark for large-scale long-term video object segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SS-NeRF: Physically based sparse spectral rendering with neural radiance field. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3611376'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose SS-NeRF, the end-to end Neural Radiance Field (NeRF)-based architectures for highquality physically based rendering with sparse inputs. We modify the classical spectral rendering into two main steps, 1) the generation of a series of spectrum maps spanning different wavelengths, 2) the combination of these spectrum maps for the RGB output. The proposed architecture follows these two steps through the proposed multi-layer perceptron (MLP)-based architecture (SpectralMLP) and spectrum attention UNet (SAUNet). Given the ray origin and the ray direction, the SpectralMLP constructs the spectral radiance field to obtain spectrum maps of novel views, which are then sent to the SAUNet to produce RGB images of white-light illumination. Applying NeRF to build up the spectral rendering is a more physically-based way from the perspective of ray-tracing. Further, the spectral radiance fields decompose difficult scenes and improve the performance of NeRF-based methods. Previous baseline, such as SpectralNeRF, outperforms recent methods in synthesizing novel views but requires relatively dense viewpoints for accurate scene reconstruction. To tackle this, we propose SS-NeRF to enhance the detail of scene representation with sparse inputs. In SS-NeRF, we first design the depth-aware continuity to optimize the reconstruction based on single-view depth predictions. Then, the geometric-projected consistency is introduced to optimize the multi-view geometry alignment. Additionally, we introduce a superpixel-aligned consistency to ensure that the average color within each superpixel region remains consistent. Comprehensive experimental results demonstrate that the proposed method is superior to recent state-ofthe-art methods when synthesizing new views on both synthetic and real-world datasets.},
  archive      = {J_TPAMI},
  author       = {Ru Li and Jia Liu and Guanghui Liu and Shengping Zhang and Bing Zeng and Shuaicheng Liu},
  doi          = {10.1109/TPAMI.2025.3611376},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SS-NeRF: Physically based sparse spectral rendering with neural radiance field},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defenses in adversarial machine learning: A systematic survey from the lifecycle perspective. <em>TPAMI</em>, 1-20. (<a href='https://doi.org/10.1109/TPAMI.2025.3611340'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial phenomena have been widely observed in machine learning (ML) systems, especially those using deep neural networks. These phenomena describe situations where ML systems may produce predictions that are inconsistent and incomprehensible to humans in certain specific cases. Such behavior poses a serious security threat to the practical application of ML systems. To exploit this vulnerability, several advanced attack paradigms have been developed, mainly including backdoor attacks, weight attacks, and adversarial examples. For each individual attack paradigm, various defense mechanisms have been proposed to enhance the robustness of models against the corresponding attacks. However, due to the independence and diversity of these defense paradigms, it is challenging to assess the overall robustness of an ML system against different attack paradigms. This survey aims to provide a systematic review of all existing defense paradigms from a unified lifecycle perspective. Specifically, we decompose a complete ML system into five stages: pre-training, training, post-training, deployment, and inference. We then present a clear taxonomy to categorize representative defense methods at each stage. The unified perspective and taxonomy not only help us analyze defense mechanisms but also enable us to understand the connections and differences among different defense paradigms. It inspires future research to develop more advanced and comprehensive defense strategies.},
  archive      = {J_TPAMI},
  author       = {Baoyuan Wu and Mingli Zhu and Meixi Zheng and Zihao Zhu and Shaokui Wei and Mingda Zhang and Hongrui Chen and Danni Yuan and Li Liu and Qingshan Liu},
  doi          = {10.1109/TPAMI.2025.3611340},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Defenses in adversarial machine learning: A systematic survey from the lifecycle perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting deformable convolution on graphs: Large-range modeling and robustness. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3611386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolution Networks (GCNs) have achieved remarkable success in representation of structured graph data. As we know that traditional GCNs are generally defined on the fixed first-order neighborhood receptive field which makes them be incapable to capture the long-range dependencies between distant nodes and also vulnerable to graph attacks and noises. To address these limitations, we revisit deformable convolution on graphs and propose a novel deformable graph convolution, termed Neighborhood-Deformable Graph Convolution (NDGC). The core of NDGC is to explicitly achieve the deformable convolution on graphs by introducing virtual neighbors which encode large-range information via the offsetting and interpolation function. That is, the introduced virtual neighbors can provide a larger receptive field with deformable receptive shape for graph convolution definition. Also, NDGC conducts message aggregation on the deformable virtual neighbors which thus performs more robustly w.r.t. graph attacks and noises. In particular, NDGC provides a general neighborhood deformable scheme, seamlessly integrating with many graph convolution definitions to derive their deformable variants. Experimental results validate the effectiveness and advantages of the proposed NDGC networks on several graph learning tasks.},
  archive      = {J_TPAMI},
  author       = {Ziyan Zhang and Bo Jiang and Jin Tang and Bin Luo},
  doi          = {10.1109/TPAMI.2025.3611386},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Revisiting deformable convolution on graphs: Large-range modeling and robustness},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep lookup network. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3605660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks are constructed with massive operations with different types and are highly computationally intensive. Among these operations, multiplication operation is higher in computational complexity and usually requires more energy consumption with longer inference time than other operations, which hinders the deployment of convolutional neural networks on mobile devices. In many resource-limited edge devices, complicated operations can be calculated via lookup tables to reduce computational cost. Motivated by this, in this paper, we introduce a generic and efficient lookup operation which can be used as a basic operation for the construction of neural networks. Instead of calculating the multiplication of weights and activation values, simple yet efficient lookup operations are adopted to compute their responses. To enable end-to-end optimization of the lookup operation, we construct the lookup tables in a differentiable manner and propose several training strategies to promote their convergence. By replacing computationally expensive multiplication operations with our lookup operations, we develop lookup networks for the image classification, image super-resolution, and point cloud classification tasks. It is demonstrated that our lookup networks can benefit from the lookup operations to achieve higher efficiency in terms of energy consumption and inference speed while maintaining competitive performance to vanilla convolutional networks. Extensive experiments show that our lookup networks produce state-of-the-art performance on different tasks (both classification and regression tasks) and different data types (both images and point clouds).},
  archive      = {J_TPAMI},
  author       = {Yulan Guo and Longguang Wang and Wendong Mao and Xiaoyu Dong and Yingqian Wang and Li Liu and Wei An},
  doi          = {10.1109/TPAMI.2025.3605660},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Deep lookup network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SOOD++: Leveraging unlabeled data to boost oriented object detection. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3611519'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised object detection (SSOD), leveraging unlabeled data to boost object detectors, has become a hot topic recently. However, existing SSOD approaches mainly focus on horizontal objects, leaving oriented objects common in aerial images unexplored. At the same time, the annotation cost of oriented objects is significantly higher than that of their horizontal counterparts (an approximate 36.5% increase in costs). Therefore, in this paper, we propose a simple yet effective Semi-supervised Oriented Object Detection method termed SOOD++. Specifically, we observe that objects from aerial images usually have arbitrary orientations, small scales, and dense distribution, which inspires the following core designs: a Simple Instance-aware Dense Sampling (SIDS) strategy is used to generate comprehensive dense pseudo-labels; the Geometry-aware Adaptive Weighting (GAW) loss dynamically modulates the importance of each pair between pseudo-label and corresponding prediction by leveraging the intricate geometric information of aerial objects; we treat aerial images as global layouts and explicitly build the many-to-many relationship between the sets of pseudo-labels and predictions via the proposed Noise-driven Global Consistency (NGC). Extensive experiments conducted on various oriented object datasets under various labeled settings demonstrate the effectiveness of our method. For example, on the DOTA-V2.0/DOTA-V1.5 benchmark, the proposed method outperforms previous state-of-the-art (SOTA) by a large margin (+2.90/2.14, +2.16/2.18, and +2.66/2.32) mAP under 10%, 20%, and 30% labeled data settings, respectively, with single-scale training and testing. More importantly, it still improves upon a strong supervised baseline with 70.66 mAP, trained using the full DOTA-V1.5 train-val set, by +1.82 mAP, resulting in a 72.48 mAP, pushing the new state-of-the-art. Moreover, our method demonstrates stable generalization ability across different oriented detectors, even for multi-view oriented 3D object detectors. The code will be made available.},
  archive      = {J_TPAMI},
  author       = {Dingkang Liang and Wei Hua and Chunsheng Shi and Zhikang Zou and Xiaoqing Ye and Xiang Bai},
  doi          = {10.1109/TPAMI.2025.3611519},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {SOOD++: Leveraging unlabeled data to boost oriented object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pathway-aware multimodal transformer (PAMT): Integrating pathological image and gene expression for interpretable cancer survival analysis. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3611531'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrating multimodal data of pathological image and gene expression for cancer survival analysis can achieve better results than using a single modality. However, existing multimodal learning methods ignore fine-grained interactions between both modalities, especially the interactions between biological pathways and pathological image patches. In this article, we propose a novel Pathway-Aware Multimodal Transformer (PAMT) framework for interpretable cancer survival analysis. Specifically, the PAMT learns fine-grained modality interaction through three stages: (1) In the intra-modal pathway-pathway / patch-patch interaction stage, we use the Transformer model to perform intra-modal information interaction; (2) In the inter-modal pathway-patch alignment stage, we introduce a novel label-free contrastive loss to aligns semantic information between different modalities so that the features of the two modalities are mapped to the same semantic space; and (3) In the inter-modal pathway-patch fusion stage, to model the medical prior knowledge of “genotype determines phenotype”, we propose a pathway-to-patch cross fusion module to perform inter-modal information interaction under the guidance of pathway prior. In addition, the inter-modal cross fusion module of PAMT endows good interpretability, helping a pathologist to screen which pathway plays a key role, to locate where on whole slide image (WSI) are affected by the pathway, and to mine prognosis-relevant pathology image patterns. Experimental results based on three datasets of bladder urothelial carcinoma, lung squamous cell carcinoma, and lung adenocarcinoma demonstrate that the proposed framework significantly outperforms the state-of-the-art methods. Finally, based on the PAMT model, we develop a website that directly visualizes the impact of 186 pathways on all areas of WSI, available at http://222.128.10.254:18822/#/.},
  archive      = {J_TPAMI},
  author       = {Rui Yan and Xueyuan Zhang and Zihang Jiang and Baizhi Wang and Xiuwu Bian and Fei Ren and S. Kevin Zhou},
  doi          = {10.1109/TPAMI.2025.3611531},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Pathway-aware multimodal transformer (PAMT): Integrating pathological image and gene expression for interpretable cancer survival analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-guidance: Boosting flow and diffusion generation on their own. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3611831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Proper guidance strategies are essential to achieve high-quality generation results without retraining diffusion and flow-based text-to-image models. Existing guidance either requires specific training or strong inductive biases of diffusion model networks, which potentially limits their ability and application scope. Motivated by the observation that artifact outliers can be detected by a significant decline in the density from a noisier to a cleaner noise level, we propose Self-Guidance (SG), which can significantly improve the quality of the generated image by suppressing the generation of low-quality samples. The biggest difference from existing guidance is that SG only relies on the sampling score function of the original diffusion or flow model at different noise levels, with no need for any tricky and expensive guidance-specific training. This makes SG highly flexible to be used in a plug-and-play manner by any diffusion or flow models. We also introduce an efficient variant of SG, named SG-prev, which reuses the output from the immediately previous diffusion step to avoid additional forward passes of the diffusion network. We conduct extensive experiments on text-to-image and text-to-video generation with different architectures, including UNet and transformer models. With open-sourced diffusion models such as Stable Diffusion 3.5 and FLUX, SG exceeds existing algorithms on multiple metrics, including both FID and Human Preference Score. SG-prev also achieves strong results over both the baseline and the SG, with 50 percent more efficiency. Moreover, we find that SG and SG-prev both have a surprisingly positive effect on the generation of physiologically correct human body structures such as hands, faces, and arms, showing their ability to eliminate human body artifacts with minimal efforts. We have released our code at https://github.com/maple-research-lab/Self-Guidance.},
  archive      = {J_TPAMI},
  author       = {Tiancheng Li and Weijian Luo and Zhiyang Chen and Liyuan Ma and Guo-Jun Qi},
  doi          = {10.1109/TPAMI.2025.3611831},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Self-guidance: Boosting flow and diffusion generation on their own},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Sample-level prototypical federated learning. <em>TPAMI</em>, 1-12. (<a href='https://doi.org/10.1109/TPAMI.2025.3612302'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing concerns about privacy and data regulations, federated learning (FL) has been emerging as a solution to train machine learning models collaboratively with non-exchangeable data from multiple clients. As a result of data locality, data is usually not identically or independently (non-IID) distributed across clients, and the non-IID property has long been the key challenge in FL. Furthermore, in real-world cross-silo scenarios, it is ubiquitous that clients are organizations owning private data from multiple domains internally, which exacerbates the non-IID issue. For example, in healthcare applications, each client (hospital) gathers data from patients with heterogeneous demographics. While previous works have made efforts to address the non-IID challenge across clients by assuming various relations among client-level data distributions and enabling personalized models at the client level, they ignore the internal data heterogeneity within each client or require explicit data domain indicators, which are hardly accessible in real-world data. Here, we propose (SL-PFL) to bridge the gap. SL-PFL incorporates prototypical learning under the FL framework and provides a fine-grained personalized model for each data sample instead of learning one uniform model for all samples of each client. Meanwhile, it can be trained using data without ground-truth domain indicators. Experimental results demonstrate that our proposed method with sample-level personalized models outperforms existing FL methods with a global model or client-level personalized models on various real-world regression and classification tasks from weather, computer vision, and healthcare applications.},
  archive      = {J_TPAMI},
  author       = {Chuizheng Meng and Jianke Yang and Hao Niu and Guillaume Habault and Roberto Legaspi and Shinya Wada and Chihiro Ono and Yan Liu},
  doi          = {10.1109/TPAMI.2025.3612302},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Sample-level prototypical federated learning},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
