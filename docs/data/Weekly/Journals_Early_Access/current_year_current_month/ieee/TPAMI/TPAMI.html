<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPAMI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpami">TPAMI - 10</h2>
<ul>
<li><details>
<summary>
(2025). $\beta$-DARTS++: Bi-level regularization for proxy-robust differentiable architecture search. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3616249'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Architecture Search (NAS) has attracted increasing attention in recent years because of its capability to design neural networks automatically. Among them, differential NAS approaches such as DARTS, have gained popularity for search efficiency. However, they still suffer from three main issues, that are, the weak stability due to the performance collapse, the poor generalization ability of the searched architectures, and the inferior robustness to different kinds of proxies (i.e., computationally reduced search configurations). To solve the search stability and searched architecture's generalization problems, a simple-but-effective regularization method, termed as Beta-Decay, is proposed to regularize the DARTS-based NAS searching process (referred as $\beta$-DARTS). Specifically, Beta-Decay regularization can impose constraints to keep the value and variance of activated architecture parameters from being too large, thereby ensuring fair competition among architecture parameters and making the supernet less sensitive to the impact of input on the operation set. In-depth theoretical analyses on how it works and why it works are provided, and comprehensive experiments on a variety of search spaces and datasets validate that Beta-Decay regularization can help to stabilize the searching process and make the searched network more transferable across different datasets. To address the proxy robustness problem, we first benchmark differentiable NAS methods under a wide range of proxy data, proxy channels, proxy layers, and proxy epochs, since the robustness of NAS under different kinds of proxies has not been explored before. We then conclude some interesting findings and find that $\beta$-DARTS always achieves the best result among all compared NAS methods under almost all proxy settings. We further introduce the novel flooding regularization to the weight optimization of $\beta$-DARTS (termed as Bi-level regularization), and experimentally and theoretically verify its effectiveness for improving the proxy robustness of differentiable NAS. In summary, our search scheme shows lots of outstanding properties for practical applications,},
  archive      = {J_TPAMI},
  author       = {Peng Ye and Tong He and Baopu Li and Tao Chen and Lei Bai and Wanli Ouyang},
  doi          = {10.1109/TPAMI.2025.3616249},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {$\beta$-DARTS++: Bi-level regularization for proxy-robust differentiable architecture search},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ID-guard: A universal framework for combating facial manipulation via breaking identification. <em>TPAMI</em>, 1-16. (<a href='https://doi.org/10.1109/TPAMI.2025.3616232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The misuse of deep learning-based facial manipulation poses a serious threat to civil rights. To prevent such fraud at its source, proactive defense methods have been proposed that embed invisible adversarial perturbations into images, disrupting the manipulation process and rendering the forged output unconvincing to observers. However, non-targeted disruption of the output may leave identifiable facial features intact, potentially leading to the stigmatization of individuals. In this work, we propose a universal framework for combating facial manipulation, termed ID-Guard. The framework employs a single forward pass of an encoder-decoder network to generate cross-model transferable adversarial perturbations. We introduce a novel Identity Destruction Module (IDM) to suppress identifiable features in manipulated faces. The perturbation generation is optimized by formulating the disruption of various manipulation types as a multi-task learning problem, with a dynamic weighting strategy designed to enhance cross-model performance. Experimental results show that ID-Guard effectively defends against diverse facial manipulation models while degrading identifiable regions in manipulated images. It also enables disrupted images to evade facial inpainting and facial recognition systems. Moreover, ID-Guard can be seamlessly integrated as a plug-and-play component into other tasks, such as adversarial training.},
  archive      = {J_TPAMI},
  author       = {Zuomin Qu and Wei Lu and Xiangyang Luo and Qian Wang and Xiaochun Cao},
  doi          = {10.1109/TPAMI.2025.3616232},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {ID-guard: A universal framework for combating facial manipulation via breaking identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DTL: Parameter- and memory-efficient disentangled vision learning. <em>TPAMI</em>, 1-14. (<a href='https://doi.org/10.1109/TPAMI.2025.3616318'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cost of finetuning a pretrained model on downstream tasks steadily increases as they grow larger. Parameter-efficient transfer learning (PETL) is proposed to reduce this cost by changing only a tiny subset of trainable parameters. But, the GPU memory footprint during training is not effectively reduced in PETL. This issue happens because trainable parameters from these methods are generally tightly entangled with the backbone, such that a lot of intermediate states have to be stored for back propagation. To alleviate this issue, we introduce Disentangled Transfer Learning (DTL), which disentangles the trainable parameters from the backbone using a lightweight Compact Side Network (CSN). By progressively extracting task-specific information with a few low-rank linear mappings and appropriately adding the information back to the backbone, CSN effectively realizes knowledge transfer in various downstream recognition tasks. We further extend DTL to more difficult tasks such as object detection and semantic segmentation by employing a more sparse architectural design. Extensive experiments validate the effectiveness of DTL, which not only reduces a large amount of GPU memory usage and trainable parameters, but also outperforms existing PETL methods by a significant margin in accuracy.},
  archive      = {J_TPAMI},
  author       = {Minghao Fu and Ke Zhu and Zonghao Ding and Jianxin Wu},
  doi          = {10.1109/TPAMI.2025.3616318},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DTL: Parameter- and memory-efficient disentangled vision learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-matrix completion: A novel framework for structurally missing elements. <em>TPAMI</em>, 1-15. (<a href='https://doi.org/10.1109/TPAMI.2025.3616607'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common assumption in matrix completion (MC) and tensor completion (TC) is that the missing locations are sampled randomly. However, in real-world scenarios, the unobserved elements are often not arbitrarily located, and may concentrate within entire rows or columns. We refer to this missing mechanism as structural missingness, and traditional MC and TC schemes suffer from drastic degradation under these circumstances. This work addresses the challenge of restoring structural missingness by introducing a novel framework for simultaneously reconstructing multiple matrices, called multi-matrix completion (MMC). In MMC, tri-factorization across matrices captures the correlation between matrices, and Tikhonov regularization on each matrix exploits its correlation. This design enables MMC to efficiently handle both random and structural missingness. In addition, MMC is not affected by the smoothness along matrices which makes it suitable for a wider variety of data compared to Fourier transform based TC methods. The alternating direction method of multipliers is utilized to solve the resultant optimization problem. The global convergence of the algorithm is supported by comprehensive theoretical analyses. We demonstrate the versatility of MMC through extensive experiments in image and video restoration, and showcase its superior performance in comparison to traditional MC and TC methods. The code is available at https://github.com/ShuDun23/MMC.},
  archive      = {J_TPAMI},
  author       = {Hao Nan Sheng and Zhi-Yong Wang and Hing Cheung So and Abdelhak M. Zoubir},
  doi          = {10.1109/TPAMI.2025.3616607},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Multi-matrix completion: A novel framework for structurally missing elements},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAIT: Triple-win compression towards high accuracy, fast inference, and favorable transferability for ViTs. <em>TPAMI</em>, 1-17. (<a href='https://doi.org/10.1109/TPAMI.2025.3616854'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision Transformers (ViTs) have emerged as state-of-the-art models for various vision tasks recently. However, their heavy computation costs remain daunting for resource-limited devices. To address this, researchers have dedicated themselves to compressing redundant information in ViTs for acceleration. However, existing approaches generally sparsely drop redundant image tokens by token pruning or brutally remove channels by channel pruning, leading to a sub-optimal balance between model performance and inference speed. Moreover, they struggle when transferring compressed models to downstream vision tasks that require the spatial structure of images, such as semantic segmentation. To tackle these issues, we propose CAIT, a joint compression method for ViTs that achieves a harmonious blend of high accuracy, fast inference speed, and favorable transferability to downstream tasks. Specifically, we introduce an asymmetric token merging (ATME) strategy to effectively integrate neighboring tokens. It can successfully compress redundant token information while preserving the spatial structure of images. On top of it, we further design a consistent dynamic channel pruning (CDCP) strategy to dynamically prune unimportant channels in ViTs. Thanks to CDCP, insignificant channels in multi-head self-attention modules of ViTs can be pruned uniformly, significantly enhancing the model compression. Extensive experiments on multiple benchmark datasets show that our proposed method can achieve state-of-the-art performance across various ViTs.},
  archive      = {J_TPAMI},
  author       = {Ao Wang and Hui Chen and Zijia Lin and Sicheng Zhao and Jungong Han and Guiguang Ding},
  doi          = {10.1109/TPAMI.2025.3616854},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CAIT: Triple-win compression towards high accuracy, fast inference, and favorable transferability for ViTs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LargeAD: Large-scale cross-sensor data pretraining for autonomous driving. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3617126'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in vision foundation models (VFMs) have revolutionized visual perception in 2D, yet their potential for 3D scene understanding, particularly in autonomous driving applications, remains underexplored. In this paper, we introduce LargeAD, a versatile and scalable framework designed for large-scale 3D pretraining across diverse real-world driving datasets. Our framework leverages VFMs to extract semantically rich superpixels from 2D images, which are aligned with LiDAR point clouds to generate high-quality contrastive samples. This alignment facilitates cross-modal representation learning, enhancing the semantic consistency between 2D and 3D data. We introduce several key innovations: (i) VFM-driven superpixel generation for detailed semantic representation, (ii) a VFM-assisted contrastive learning strategy to align multimodal features, (iii) superpoint temporal consistency to maintain stable representations across time, and (iv) multi-source data pretraining to generalize across various LiDAR configurations. Our approach achieves substantial gains over state-of-the-art methods in linear probing and fine-tuning for LiDAR-based segmentation and object detection. Extensive experiments on 11 large-scale multi-sensor datasets highlight our superior performance, demonstrating adaptability, efficiency, and robustness in real-world autonomous driving scenarios.},
  archive      = {J_TPAMI},
  author       = {Lingdong Kong and Xiang Xu and Youquan Liu and Jun Cen and Runnan Chen and Wenwei Zhang and Liang Pan and Kai Chen and Ziwei Liu},
  doi          = {10.1109/TPAMI.2025.3617126},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LargeAD: Large-scale cross-sensor data pretraining for autonomous driving},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient 3D surface super-resolution via normal-based multimodal restoration. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3614184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-fidelity 3D surface is essential for vision tasks across various domains such as medical imaging, cultural heritage preservation, quality inspection, virtual reality, and autonomous navigation. However, the intricate nature of 3D data representations poses significant challenges in restoring diverse 3D surfaces while capturing fine-grained geometric details at a low cost. This paper introduces an efficient multimodal normal-based 3D surface super-resolution (mn3DSSR) framework, designed to address the challenges of microgeometry enhancement and computational overhead. Specifically, we have constructed one of the largest normalbased multimodal dataset, ensuring superior data quality and diversity through meticulous subjective selection. Furthermore, we explore a new two-branch multimodal alignment approach along with a multimodal split fusion module to mitigate computational complexity while improving restoration performances. To address the limitations associated with normal-based multimodal learning, we develop novel normal-induced loss functions that facilitate geometric consistency and improve feature alignment. Extensive experiments conducted on seven benchmark datasets across four different 3D data representations demonstrate that mn3DSSR consistently outperforms state-ofthe-art super-resolution methods in terms of restoration accuracy with high computational efficiency.},
  archive      = {J_TPAMI},
  author       = {Miaohui Wang and Yunheng Liu and Wuyuan Xie and Boxin Shi and Jianmin Jiang},
  doi          = {10.1109/TPAMI.2025.3614184},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Efficient 3D surface super-resolution via normal-based multimodal restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Schedule-robust continual learning. <em>TPAMI</em>, 1-13. (<a href='https://doi.org/10.1109/TPAMI.2025.3614868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning (CL) tackles a fundamental challenge in machine learning, aiming to continuously learn novel data from non-stationary data streams while mitigating forgetting of previously learned data. Although existing CL algorithms have introduced various practical techniques for combating forgetting, little attention has been devoted to studying how data schedules – which dictate how the sample distribution of a data stream evolves over time – affect the CL problem. Empirically, most CL methods are susceptible to schedule changes: they exhibit markedly lower accuracy when dealing with more “difficult schedules over the same underlying training data. In practical scenarios, data schedules are often unknown and a key challenge is thus to design CL methods that are robust to diverse schedules to ensure model reliability. In this work, we introduce the novel concept of schedule robustness for CL and propose Schedule-Robust Continual Learning (SCROLL), a strong baseline satisfying this desirable property. SCROLL trains a linear classifier on a suitably pre-trained representation, followed by model adaptation using replay data only. We connect SCROLL to a meta-learning formulation of CL with provable guarantees on schedule robustness. Empirically, the proposed method significantly outperforms existing CL methods and we provide extensive ablations to highlight its properties.},
  archive      = {J_TPAMI},
  author       = {Ruohan Wang and Marco Ciccone and Massimiliano Pontil and Carlo Ciliberto},
  doi          = {10.1109/TPAMI.2025.3614868},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Schedule-robust continual learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning dynamic graph embeddings with neural controlled differential equations. <em>TPAMI</em>, 1-10. (<a href='https://doi.org/10.1109/TPAMI.2025.3617660'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on representation learning for dynamic graphs with temporal interactions. A fundamental issue is that both the graph structure and the nodes own their own dynamics, and their blending induces intractable complexity in the temporal evolution over graphs. Drawing inspiration from the recent progress of physical dynamic models in deep neural networks, we propose Graph Neural Controlled Differential Equations (GN-CDEs), a continuous-time framework that jointly models node embeddings and structural dynamics by incorporating a graph enhanced neural network vector field with a time-varying graph path as the control signal. Our framework exhibits several desirable characteristics, including the ability to express dynamics on evolving graphs without piecewise integration, the capability to calibrate trajectories with subsequent data, and robustness to missing observations. Empirical evaluation on a range of dynamic graph representation learning tasks demonstrates the effectiveness of our proposed approach in capturing the complex dynamics of dynamic graphs.},
  archive      = {J_TPAMI},
  author       = {Tiexin Qin and Benjamin Walker and Terry Lyons and Hong Yan and Haoliang Li},
  doi          = {10.1109/TPAMI.2025.3617660},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning dynamic graph embeddings with neural controlled differential equations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EvLight++: Low-light video enhancement with an event camera: A large-scale real-world dataset, novel method, and more. <em>TPAMI</em>, 1-18. (<a href='https://doi.org/10.1109/TPAMI.2025.3617801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event cameras offer significant advantages for low-light video enhancement, primarily due to their high dynamic range. Current research, however, is severely limited by the absence of large-scale, real-world, and spatio-temporally aligned event-video datasets. To address this, we introduce a large-scale dataset with over 30,000 pairs of frames and events captured under varying illumination. This dataset was curated using a robotic arm that traces a consistent non-linear trajectory, achieving spatial alignment precision under 0.03mm and temporal alignment with errors under 0.01s for 90% of the dataset. Based on the dataset, we propose EvLight++, a novel event-guided low-light video enhancement approach designed for robust performance in real-world scenarios. Firstly, we design a multi-scale holistic fusion branch to integrate structural and textural information from both images and events. To counteract variations in regional illumination and noise, we introduce Signal-to-Noise Ratio (SNR)-guided regional feature selection, enhancing features from high SNR regions and augmenting those from low SNR regions by extracting structural information from events. To incorporate temporal information and ensure temporal coherence, we further introduce a recurrent module and temporal loss in the whole pipeline. Extensive experiments on ours and the synthetic SDSD dataset demonstrate that EvLight++ significantly outperforms both single image- and video-based methods by 1.37 dB and 3.71 dB, respectively. To further explore its potential in downstream tasks like semantic segmentation and monocular depth estimation, we extend our datasets by adding pseudo segmentation and depth labels via meticulous annotation efforts with foundation models. Experiments under diverse low-light scenes show that the enhanced results achieve a 15.97% improvement in mIoU for semantic segmentation.},
  archive      = {J_TPAMI},
  author       = {Kanghao Chen and Guoqiang Liang and Yunfan Lu and Hangyu Li and Lin Wang},
  doi          = {10.1109/TPAMI.2025.3617801},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {EvLight++: Low-light video enhancement with an event camera: A large-scale real-world dataset, novel method, and more},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
