<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg">TVCG - 10</h2>
<ul>
<li><details>
<summary>
(2025). Upright-net+: Enhanced learning of upright orientation for 3D point clouds. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3605201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic 3D shape analysis is heavily influenced by the pose of input 3D models, as the continuous nature of pose space introduces complexities that usually exceed the encoding capacities of standard deep learning frameworks. To tackle this challenge, we present Upright-Net+, an enhancement of our previous model, Upright-Net, specifically developed for estimating upright orientation in 3D point clouds. Our approach is grounded in the design principle that ”form ever follows function,” treating the natural base of an object as a functional structure that stabilizes it in its typical pose, influenced by physical laws and geometric properties. We reformulate the continuous orientation problem into a discrete classification task, focusing on learning the points that constitute the natural base of a 3D model. The upright orientation is determined by aligning the normal orientation of this base towards the mass center. To mitigate over-smoothing in the global feature embeddings from stacked graph convolutional layers, we introduce a Global Positional Encoding Module using Relative Distance Histogram Statistics Embedding (GPE-RDHS), which reduces structural ambiguity and enhances orientation estimation. We also enhanced a weighted residual loss term to penalize false positive predictions, enhancing overall model performance. Our method demonstrates exceptional performance in upright orientation estimation and reveals that the learned orientation-aware features significantly benefit downstream tasks, particularly in classification.},
  archive      = {J_TVCG},
  author       = {Xufang Pang and Feng Li and Hongjie Zhuang and Ning Ding and Xiaopin Zhong and Shengfeng He and Wenxi Liu and Bo Jiang},
  doi          = {10.1109/TVCG.2025.3605201},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Upright-net+: Enhanced learning of upright orientation for 3D point clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectrum alignment for robust 3D point cloud correspondences estimation. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3605711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating dense point-to-point correspondences between two isometric shapes represented as 3D point clouds is a fundamental problem in geometry processing, with applications in texture and motion transfer. However, this task becomes particularly challenging when the shapes undergo non-rigid transformations, as is often the case with approximately isometric point clouds. Most existing algorithms address this challenge by establishing correspondences between functions defined on the shapes, rather than directly between points, because function mappings admit a linear representation in the spectral domain. State-of-the-art methods compute this linear representation using the eigenfunctions of the Laplace–Beltrami Operator (LBO) along with a small set of initial corresponding functions between the shapes. However, for approximately isometric point clouds, two key issues arise: (1) the eigenfunctions of the LBO may become misaligned, and (2) the initial corresponding functions may include outliers, both of which degrade the quality of the resulting correspondences. In this work, we propose an efficient approach to align the spectra of the LBOs of the two shapes, enabling the eigenfunctions to remain compatible even for approximately isometric 3D point clouds. Additionally, we introduce a technique to make function correspondence estimation robust to outliers. We validate our approach by comparing it with state-of-the-art 3D shape-matching algorithms on benchmark datasets, demonstrating its effectiveness.},
  archive      = {J_TVCG},
  author       = {Deepanshu Solanki and Rajendra Nagar},
  doi          = {10.1109/TVCG.2025.3605711},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spectrum alignment for robust 3D point cloud correspondences estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-multiplexing and filtering holography: Enhancing depth cues and robustness against noise. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3606509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holography is a promising approach to recreate lifelike 3D scenes. However, due to the current Spatial Light Modulators (SLMs) lacking sufficient pixels, the defocused planes of holograms always exhibit obvious interference phenomena. The methods based on random phase can alleviate this problem, but they always affect the imaging quality of the focal plane. Meanwhile, direct current (DC) noise of nondiffracted light in SLMs, coupled with ubiquitous dynamic noise, has long been a fundamental issue affecting holographic display quality. In this study, we proposed a method based on static high-pass filtering and time-multiplexing that overcomes the traditional tradeoff between divergence capability of holograms and display quality in focal planes. Simultaneously, the proposed method can eliminate DC and dynamic noise with a simple and robust structure. Moreover, we further extended artificial intelligencedriven algorithms to achieve higher-quality on-axis amplitudeonly holograms. The Simulations and experiments demonstrated that the developed method is a promising and easily generalized solution for time-multiplexing holography},
  archive      = {J_TVCG},
  author       = {Chenhang Shen and Yuhang Zheng and Yifei Xie and Zhu Wang and Yulang Peng and Weilong Zhou and Junming Zhu and Zichun Le},
  doi          = {10.1109/TVCG.2025.3606509},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Time-multiplexing and filtering holography: Enhancing depth cues and robustness against noise},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceived weight of mediated reality sticks. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3591181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediated reality, where augmented reality (AR) and diminished reality (DR) meet, enables visual modifications to real-world objects. A physical object with a mediated reality visual change retains its original physical properties. However, it is perceived differently from the original when interacted with. We present such a mediated reality object, a stick with different lengths or a stick with a missing portion in the middle, to investigate how users perceive its weight and center of gravity. We conducted two user studies ($N=10$), each of which consisted of two substudies. We found that the length of mediated reality sticks influences the perceived weight. A longer stick is perceived as lighter, and vice versa. The stick with a missing portion tends to be recognized as one continuous stick. Thus, its weight and center of gravity (COG) remain the same. We formulated the relationship between inertia based on the reported COG and perceived weight in the context of dynamic touch.},
  archive      = {J_TVCG},
  author       = {Satoshi Hashiguchi and Yuta Kataoka and Asako Kimura and Shohei Mori},
  doi          = {10.1109/TVCG.2025.3591181},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceived weight of mediated reality sticks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taming high-resolution auxiliary G-buffers for deep supersampling of rendered content. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3609456'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution images come with rich color information and texture details. Due to the rapid upgrading of display devices and rendering technologies, high-resolution real-time rendering faces the computational overhead challenge. To address this, the current mainstream solution is to render at a lower resolution and then upsample to the target resolution by supersampling techniques. However, while many prior supersampling approaches have attempted to exploit rich rendered data such as color, depth, motion vectors at low resolution, there is little discussion on how to harness high-frequency information that is readily available in the high-resolution (HR) G-buffers of modern renders. In this paper, we seek to investigate how to fully leverage information from HR G-buffers to maximize the visual quality of supersampling results. We propose a neural network for real-time supersampling of rendered content, which is based on several core designs, including gated G-buffers encoder, G-buffers attended encoder and reflection-aware loss. These designs are especially made for the sake of effectively using HR G-buffers, enabling faithful recovery of a variety of high-frequency scene details from low-resolution, highly aliased inputs. Furthermore, a simple occlusion-aware blender is proposed to efficiently rectify invalid features in the warped previous frame, allowing us to better exploit history information to improve temporal stability. The experiments show that our method, equipped with strong ability to harness HR G-buffer information, significantly improves the visual fidelity of high-resolution reconstructions upon previous state-of-the-art methods, even for challenging $4 \times 4$ upsampling, while still being compute-efficient.},
  archive      = {J_TVCG},
  author       = {Pengjie Wang and Chengzhi Yuan and Jie Guo and Xiaosong Yang and Houjie Li and Ian Stephenson and Jian Chang and Ying Cao},
  doi          = {10.1109/TVCG.2025.3609456},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Taming high-resolution auxiliary G-buffers for deep supersampling of rendered content},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D-FRAME: Direction-field-based wireframe extraction for complex CAD models. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3609350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting wireframes from CAD models represented by point cloud remains a significant challenge in computer graphics. This difficulty arises from two main factors: first, imperfections in the point cloud data, such as lack of orientation, noise, and sparsity; and second, the inherent complexity of geometric shapes, which often feature a high density of sharp edges in close proximity. In this paper, we propose D-FRAME, a multi-stage wireframe extraction framework that incorporates a novel direction field to improve edge detection quality and connectivity, a refinement strategy to address sparse or noisy edge points, and a final coarse-to-fine connection module to extract a robust wireframe. The direction field not only facilitates connectivity but also enhances the precision of extracted edges by mitigating the impact of misclassified points. By combining the Restricted Voronoi Diagram (RVD) with the extracted wireframes and the original point cloud, our approach also achieves highly faithful reconstruction of CAD model. Experiments conducted on synthetic and real-world scanned CAD datasets demonstrate that D-FRAME effectively manages noise, sparsity, and complex geometries, yielding high-fidelity wireframes. Code is available at https://github.com/yuanfeng-01/D-FRAME-test.},
  archive      = {J_TVCG},
  author       = {Yuan Feng and Honghao Dai and Guangshun Wei and Long Ma and Pengfei Wang and Yuanfeng Zhou and Ying He},
  doi          = {10.1109/TVCG.2025.3609350},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {D-FRAME: Direction-field-based wireframe extraction for complex CAD models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive affordances in visualization: Related constructs, design factors, and framework. <em>TVCG</em>, 1-17. (<a href='https://doi.org/10.1109/TVCG.2025.3610803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classically, affordance research investigates how the shape of objects communicates actions to potential users. Cognitive affordances, a subset of this research, characterize how the design of objects influences cognitive actions, such as information processing. Within visualization, cognitive affordances inform how graphs' design decisions communicate information to their readers. Although several related concepts exist in visualization, a formal translation of affordance theory to visualization is still lacking. In this paper, we review and translate affordance theory to visualization by formalizing how cognitive affordances operate within a visualization context. We also review common methods and terms, and compare related constructs to cognitive affordances in visualization. Based on a synthesis of research from psychology, human-computer interaction, and visualization, we propose a framework of cognitive affordances in visualization that enumerates design decisions and reader characteristics that influence a visualization's hierarchy of communicated information. Finally, we demonstrate how this framework can guide the evaluation and redesign of visualizations.},
  archive      = {J_TVCG},
  author       = {Racquel Fygenson and Lace Padilla and Enrico Bertini},
  doi          = {10.1109/TVCG.2025.3610803},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cognitive affordances in visualization: Related constructs, design factors, and framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analytical texture mapping. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3611315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resampling of warped images has been a topic of research for a long time but only seldomly has focused on theoretically exact resampling. We present a resampling method for minification, applied on the texture mapping function of a 3D graphics pipeline, that is derived from sampling theory without making any approximations. Our method supports freely selectable 2D integratable prefilter (anti-aliasing) functions and uses a 2D box reconstruction filter. We have implemented our method both for CPU and GPU (OpenGL) using multiple prefilter functions defined by piece-wise polynomials. The correctness of our exact resampling method has been made plausible by comparing texture mapping results of our method with those of extreme supersampling. We additionally show how the prefilter of our method can also be applied for high quality polygon edge anti-aliasing. Since our proposed method does not use any approximations, up to numerical precision, it can be used as a reference for approximate texture mapping methods.},
  archive      = {J_TVCG},
  author       = {Koen Meinds and Elmar Eisemann},
  doi          = {10.1109/TVCG.2025.3611315},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analytical texture mapping},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LucidDreamer: Domain-free generation of 3D gaussian splatting scenes. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3611489'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating high-quality 3D scenes is a critical challenge in computer vision, driven by advances in 3D graphics and the growing demand for immersive environments. While object-centric 3D generation has achieved significant progress, scene generation remains difficult due to the scarcity of large-scale 3D scene datasets and scalability constraints of conventional 3D representations, which hinder efficient large-scale expansion. To address these challenges, we propose LucidDreamer, a novel pipeline that synthesizes diverse, high-quality, and expandable 3D scenes using a unified 3D Gaussian splatting representation. Our approach employs an iterative Navigation-Dreaming-Alignment process, leveraging 2D image generation and depth estimation to construct photorealistic, scalable 3D environments. By iteratively generating images and navigating through the scene, LucidDreamer fully utilizes the power of image generation models, enabling the creation of highly detailed and expandable 3D scenes. LucidDreamer supports various input modalities, including text, RGB, and RGBD, and enables dynamic modifications during generation. Experimental results demonstrate that LucidDreamer outperforms existing methods in generating high-quality, diverse, structurally consistent, and navigable 3D scenes. The project page is available on: https://luciddreamer-cvlab.github.io/.},
  archive      = {J_TVCG},
  author       = {Jaeyoung Chung and Suyoung Lee and Hyeongjin Nam and Jaerin Lee and Kyoung Mu Lee},
  doi          = {10.1109/TVCG.2025.3611489},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LucidDreamer: Domain-free generation of 3D gaussian splatting scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic importance monte carlo SPH vortical flows with lagrangian samples. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3612190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a Lagrangian dynamic importance Monte Carlo method without non-trivial random walks for solving the Velocity-Vorticity Poisson Equation (VVPE) in Smoothed Particle Hydrodynamics (SPH) for vortical flows. Key to our approach is the use of the Kinematic Vorticity Number (KVN) to detect vortex cores and to compute the KVN-based importance of each particle when solving the VVPE. We use Adaptive Kernel Density Estimation (AKDE) to extract a probability density distribution from the KVN for the the Monte Carlo calculations. Even though the distribution of the KVN can be non-trivial, AKDE yields a smooth and normalized result which we dynamically update at each time step. As we sample actual particles directly, the Lagrangian attributes of particle samples ensure that the continuously evolved KVN-based importance, modeled by the probability density distribution extracted from the KVN by AKDE, can be closely followed. Our approach enables effective vortical flow simulations with significantly reduced computational overhead and comparable quality to the classic Biot-Savart law that in contrast requires expensive global particle querying.},
  archive      = {J_TVCG},
  author       = {Xingyu Ye and Xiaokun Wang and Yanrui Xu and Alexandru C. Telea and Jiří Kosinka and Lihua You and Jian Jun Zhang and Jian Chang},
  doi          = {10.1109/TVCG.2025.3612190},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic importance monte carlo SPH vortical flows with lagrangian samples},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
