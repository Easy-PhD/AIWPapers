<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg">TVCG - 17</h2>
<ul>
<li><details>
<summary>
(2025). Upright-net+: Enhanced learning of upright orientation for 3D point clouds. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3605201'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic 3D shape analysis is heavily influenced by the pose of input 3D models, as the continuous nature of pose space introduces complexities that usually exceed the encoding capacities of standard deep learning frameworks. To tackle this challenge, we present Upright-Net+, an enhancement of our previous model, Upright-Net, specifically developed for estimating upright orientation in 3D point clouds. Our approach is grounded in the design principle that ”form ever follows function,” treating the natural base of an object as a functional structure that stabilizes it in its typical pose, influenced by physical laws and geometric properties. We reformulate the continuous orientation problem into a discrete classification task, focusing on learning the points that constitute the natural base of a 3D model. The upright orientation is determined by aligning the normal orientation of this base towards the mass center. To mitigate over-smoothing in the global feature embeddings from stacked graph convolutional layers, we introduce a Global Positional Encoding Module using Relative Distance Histogram Statistics Embedding (GPE-RDHS), which reduces structural ambiguity and enhances orientation estimation. We also enhanced a weighted residual loss term to penalize false positive predictions, enhancing overall model performance. Our method demonstrates exceptional performance in upright orientation estimation and reveals that the learned orientation-aware features significantly benefit downstream tasks, particularly in classification.},
  archive      = {J_TVCG},
  author       = {Xufang Pang and Feng Li and Hongjie Zhuang and Ning Ding and Xiaopin Zhong and Shengfeng He and Wenxi Liu and Bo Jiang},
  doi          = {10.1109/TVCG.2025.3605201},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Upright-net+: Enhanced learning of upright orientation for 3D point clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spectrum alignment for robust 3D point cloud correspondences estimation. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3605711'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating dense point-to-point correspondences between two isometric shapes represented as 3D point clouds is a fundamental problem in geometry processing, with applications in texture and motion transfer. However, this task becomes particularly challenging when the shapes undergo non-rigid transformations, as is often the case with approximately isometric point clouds. Most existing algorithms address this challenge by establishing correspondences between functions defined on the shapes, rather than directly between points, because function mappings admit a linear representation in the spectral domain. State-of-the-art methods compute this linear representation using the eigenfunctions of the Laplace–Beltrami Operator (LBO) along with a small set of initial corresponding functions between the shapes. However, for approximately isometric point clouds, two key issues arise: (1) the eigenfunctions of the LBO may become misaligned, and (2) the initial corresponding functions may include outliers, both of which degrade the quality of the resulting correspondences. In this work, we propose an efficient approach to align the spectra of the LBOs of the two shapes, enabling the eigenfunctions to remain compatible even for approximately isometric 3D point clouds. Additionally, we introduce a technique to make function correspondence estimation robust to outliers. We validate our approach by comparing it with state-of-the-art 3D shape-matching algorithms on benchmark datasets, demonstrating its effectiveness.},
  archive      = {J_TVCG},
  author       = {Deepanshu Solanki and Rajendra Nagar},
  doi          = {10.1109/TVCG.2025.3605711},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spectrum alignment for robust 3D point cloud correspondences estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Time-multiplexing and filtering holography: Enhancing depth cues and robustness against noise. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3606509'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holography is a promising approach to recreate lifelike 3D scenes. However, due to the current Spatial Light Modulators (SLMs) lacking sufficient pixels, the defocused planes of holograms always exhibit obvious interference phenomena. The methods based on random phase can alleviate this problem, but they always affect the imaging quality of the focal plane. Meanwhile, direct current (DC) noise of nondiffracted light in SLMs, coupled with ubiquitous dynamic noise, has long been a fundamental issue affecting holographic display quality. In this study, we proposed a method based on static high-pass filtering and time-multiplexing that overcomes the traditional tradeoff between divergence capability of holograms and display quality in focal planes. Simultaneously, the proposed method can eliminate DC and dynamic noise with a simple and robust structure. Moreover, we further extended artificial intelligencedriven algorithms to achieve higher-quality on-axis amplitudeonly holograms. The Simulations and experiments demonstrated that the developed method is a promising and easily generalized solution for time-multiplexing holography},
  archive      = {J_TVCG},
  author       = {Chenhang Shen and Yuhang Zheng and Yifei Xie and Zhu Wang and Yulang Peng and Weilong Zhou and Junming Zhu and Zichun Le},
  doi          = {10.1109/TVCG.2025.3606509},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Time-multiplexing and filtering holography: Enhancing depth cues and robustness against noise},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceived weight of mediated reality sticks. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3591181'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediated reality, where augmented reality (AR) and diminished reality (DR) meet, enables visual modifications to real-world objects. A physical object with a mediated reality visual change retains its original physical properties. However, it is perceived differently from the original when interacted with. We present such a mediated reality object, a stick with different lengths or a stick with a missing portion in the middle, to investigate how users perceive its weight and center of gravity. We conducted two user studies ($N=10$), each of which consisted of two substudies. We found that the length of mediated reality sticks influences the perceived weight. A longer stick is perceived as lighter, and vice versa. The stick with a missing portion tends to be recognized as one continuous stick. Thus, its weight and center of gravity (COG) remain the same. We formulated the relationship between inertia based on the reported COG and perceived weight in the context of dynamic touch.},
  archive      = {J_TVCG},
  author       = {Satoshi Hashiguchi and Yuta Kataoka and Asako Kimura and Shohei Mori},
  doi          = {10.1109/TVCG.2025.3591181},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceived weight of mediated reality sticks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Taming high-resolution auxiliary G-buffers for deep supersampling of rendered content. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3609456'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-resolution images come with rich color information and texture details. Due to the rapid upgrading of display devices and rendering technologies, high-resolution real-time rendering faces the computational overhead challenge. To address this, the current mainstream solution is to render at a lower resolution and then upsample to the target resolution by supersampling techniques. However, while many prior supersampling approaches have attempted to exploit rich rendered data such as color, depth, motion vectors at low resolution, there is little discussion on how to harness high-frequency information that is readily available in the high-resolution (HR) G-buffers of modern renders. In this paper, we seek to investigate how to fully leverage information from HR G-buffers to maximize the visual quality of supersampling results. We propose a neural network for real-time supersampling of rendered content, which is based on several core designs, including gated G-buffers encoder, G-buffers attended encoder and reflection-aware loss. These designs are especially made for the sake of effectively using HR G-buffers, enabling faithful recovery of a variety of high-frequency scene details from low-resolution, highly aliased inputs. Furthermore, a simple occlusion-aware blender is proposed to efficiently rectify invalid features in the warped previous frame, allowing us to better exploit history information to improve temporal stability. The experiments show that our method, equipped with strong ability to harness HR G-buffer information, significantly improves the visual fidelity of high-resolution reconstructions upon previous state-of-the-art methods, even for challenging $4 \times 4$ upsampling, while still being compute-efficient.},
  archive      = {J_TVCG},
  author       = {Pengjie Wang and Chengzhi Yuan and Jie Guo and Xiaosong Yang and Houjie Li and Ian Stephenson and Jian Chang and Ying Cao},
  doi          = {10.1109/TVCG.2025.3609456},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Taming high-resolution auxiliary G-buffers for deep supersampling of rendered content},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). D-FRAME: Direction-field-based wireframe extraction for complex CAD models. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3609350'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting wireframes from CAD models represented by point cloud remains a significant challenge in computer graphics. This difficulty arises from two main factors: first, imperfections in the point cloud data, such as lack of orientation, noise, and sparsity; and second, the inherent complexity of geometric shapes, which often feature a high density of sharp edges in close proximity. In this paper, we propose D-FRAME, a multi-stage wireframe extraction framework that incorporates a novel direction field to improve edge detection quality and connectivity, a refinement strategy to address sparse or noisy edge points, and a final coarse-to-fine connection module to extract a robust wireframe. The direction field not only facilitates connectivity but also enhances the precision of extracted edges by mitigating the impact of misclassified points. By combining the Restricted Voronoi Diagram (RVD) with the extracted wireframes and the original point cloud, our approach also achieves highly faithful reconstruction of CAD model. Experiments conducted on synthetic and real-world scanned CAD datasets demonstrate that D-FRAME effectively manages noise, sparsity, and complex geometries, yielding high-fidelity wireframes. Code is available at https://github.com/yuanfeng-01/D-FRAME-test.},
  archive      = {J_TVCG},
  author       = {Yuan Feng and Honghao Dai and Guangshun Wei and Long Ma and Pengfei Wang and Yuanfeng Zhou and Ying He},
  doi          = {10.1109/TVCG.2025.3609350},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {D-FRAME: Direction-field-based wireframe extraction for complex CAD models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cognitive affordances in visualization: Related constructs, design factors, and framework. <em>TVCG</em>, 1-17. (<a href='https://doi.org/10.1109/TVCG.2025.3610803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classically, affordance research investigates how the shape of objects communicates actions to potential users. Cognitive affordances, a subset of this research, characterize how the design of objects influences cognitive actions, such as information processing. Within visualization, cognitive affordances inform how graphs' design decisions communicate information to their readers. Although several related concepts exist in visualization, a formal translation of affordance theory to visualization is still lacking. In this paper, we review and translate affordance theory to visualization by formalizing how cognitive affordances operate within a visualization context. We also review common methods and terms, and compare related constructs to cognitive affordances in visualization. Based on a synthesis of research from psychology, human-computer interaction, and visualization, we propose a framework of cognitive affordances in visualization that enumerates design decisions and reader characteristics that influence a visualization's hierarchy of communicated information. Finally, we demonstrate how this framework can guide the evaluation and redesign of visualizations.},
  archive      = {J_TVCG},
  author       = {Racquel Fygenson and Lace Padilla and Enrico Bertini},
  doi          = {10.1109/TVCG.2025.3610803},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Cognitive affordances in visualization: Related constructs, design factors, and framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analytical texture mapping. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3611315'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resampling of warped images has been a topic of research for a long time but only seldomly has focused on theoretically exact resampling. We present a resampling method for minification, applied on the texture mapping function of a 3D graphics pipeline, that is derived from sampling theory without making any approximations. Our method supports freely selectable 2D integratable prefilter (anti-aliasing) functions and uses a 2D box reconstruction filter. We have implemented our method both for CPU and GPU (OpenGL) using multiple prefilter functions defined by piece-wise polynomials. The correctness of our exact resampling method has been made plausible by comparing texture mapping results of our method with those of extreme supersampling. We additionally show how the prefilter of our method can also be applied for high quality polygon edge anti-aliasing. Since our proposed method does not use any approximations, up to numerical precision, it can be used as a reference for approximate texture mapping methods.},
  archive      = {J_TVCG},
  author       = {Koen Meinds and Elmar Eisemann},
  doi          = {10.1109/TVCG.2025.3611315},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Analytical texture mapping},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LucidDreamer: Domain-free generation of 3D gaussian splatting scenes. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3611489'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating high-quality 3D scenes is a critical challenge in computer vision, driven by advances in 3D graphics and the growing demand for immersive environments. While object-centric 3D generation has achieved significant progress, scene generation remains difficult due to the scarcity of large-scale 3D scene datasets and scalability constraints of conventional 3D representations, which hinder efficient large-scale expansion. To address these challenges, we propose LucidDreamer, a novel pipeline that synthesizes diverse, high-quality, and expandable 3D scenes using a unified 3D Gaussian splatting representation. Our approach employs an iterative Navigation-Dreaming-Alignment process, leveraging 2D image generation and depth estimation to construct photorealistic, scalable 3D environments. By iteratively generating images and navigating through the scene, LucidDreamer fully utilizes the power of image generation models, enabling the creation of highly detailed and expandable 3D scenes. LucidDreamer supports various input modalities, including text, RGB, and RGBD, and enables dynamic modifications during generation. Experimental results demonstrate that LucidDreamer outperforms existing methods in generating high-quality, diverse, structurally consistent, and navigable 3D scenes. The project page is available on: https://luciddreamer-cvlab.github.io/.},
  archive      = {J_TVCG},
  author       = {Jaeyoung Chung and Suyoung Lee and Hyeongjin Nam and Jaerin Lee and Kyoung Mu Lee},
  doi          = {10.1109/TVCG.2025.3611489},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LucidDreamer: Domain-free generation of 3D gaussian splatting scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic importance monte carlo SPH vortical flows with lagrangian samples. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3612190'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a Lagrangian dynamic importance Monte Carlo method without non-trivial random walks for solving the Velocity-Vorticity Poisson Equation (VVPE) in Smoothed Particle Hydrodynamics (SPH) for vortical flows. Key to our approach is the use of the Kinematic Vorticity Number (KVN) to detect vortex cores and to compute the KVN-based importance of each particle when solving the VVPE. We use Adaptive Kernel Density Estimation (AKDE) to extract a probability density distribution from the KVN for the the Monte Carlo calculations. Even though the distribution of the KVN can be non-trivial, AKDE yields a smooth and normalized result which we dynamically update at each time step. As we sample actual particles directly, the Lagrangian attributes of particle samples ensure that the continuously evolved KVN-based importance, modeled by the probability density distribution extracted from the KVN by AKDE, can be closely followed. Our approach enables effective vortical flow simulations with significantly reduced computational overhead and comparable quality to the classic Biot-Savart law that in contrast requires expensive global particle querying.},
  archive      = {J_TVCG},
  author       = {Xingyu Ye and Xiaokun Wang and Yanrui Xu and Alexandru C. Telea and Jiří Kosinka and Lihua You and Jian Jun Zhang and Jian Chang},
  doi          = {10.1109/TVCG.2025.3612190},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic importance monte carlo SPH vortical flows with lagrangian samples},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DFG-PCN: Point cloud completion with degree-flexible point graph. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3612379'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud completion is a vital task focused on reconstructing complete point clouds and addressing the incompleteness caused by occlusion and limited sensor resolution. Traditional methods relying on fixed local region partitioning, such as k-nearest neighbors, which fail to account for the highly uneven distribution of geometric complexity across different regions of a shape. This limitation leads to inefficient representation and suboptimal reconstruction, especially in areas with fine-grained details or structural discontinuities. This paper proposes a point cloud completion framework called Degree-Flexible Point Graph Completion Network (DFG-PCN). It adaptively assigns node degrees using a detail-aware metric that combines feature variation and curvature, focusing on structurally important regions. We further introduce a geometry-aware graph integration module that uses Manhattan distance for edge aggregation and detail-guided fusion of local and global features to enhance representation. Extensive experiments on multiple benchmark datasets demonstrate that our method consistently outperforms state-of-the-art approaches.},
  archive      = {J_TVCG},
  author       = {Zhenyu Shu and Jian Yao and Shiqing Xin},
  doi          = {10.1109/TVCG.2025.3612379},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DFG-PCN: Point cloud completion with degree-flexible point graph},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scene-based foveated fluid animation in virtual reality. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3609904'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physically-based fluid animation in Virtual Reality (VR) significantly enhances the user experience through visually engaging flow motions. Nonetheless, such simulations are often limited by their substantial computational demands. A tailored adaptive simulation algorithm is important for high-performance VR fluid simulations, which dynamically allocate degrees of freedom (DoF) while accounting for user perception in VR. This paper proposes a novel scene-based gaze-contingent fluid simulation system for VR, featuring a highly adaptive fluid simulator integrated with a VR perceptual model that accounts for the foveation and geometry of fluid. Our method leverages an eccentricity and curvature-dependent perceptual model to dynamically allocate computational resources, improving the efficiency and maintaining spatio-temporal stability of fluid animation in VR. A user study was conducted to measure the simulation resolution thresholds for fluid animations in VR, considering various levels of eccentricity and curvature. Our findings indicate notable differences in perceptual thresholds based on these metrics. By incorporating these insights into our adaptive fluid simulator as a unified sizing function, we maintain perceptually optimal particle resolution, achieving up to a 3.62× performance improvement while delivering superior perceptual realism and user experience, as validated by a subjective evaluation study.},
  archive      = {J_TVCG},
  author       = {Yue Wang and Yan Zhang and Xuanhui Yang and Hui Wang and Xubo Yang},
  doi          = {10.1109/TVCG.2025.3609904},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Scene-based foveated fluid animation in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Textured mesh quality assessment using geometry and color field similarity. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3612942'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Textured mesh quality assessment (TMQA) is critical for various 3D mesh applications. However, existing TMQA methods often struggle to provide accurate and robust evaluations. Motivated by the effectiveness of fields in representing both 3D geometry and color information, we propose a novel point-based TMQA method called field mesh quality metric (FMQM). FMQM utilizes signed distance fields and a newly proposed color field named nearest surface point color field to realize effective mesh feature description. Four features related to visual perception are extracted from the geometry and color fields: geometry similarity, geometry gradient similarity, space color distribution similarity, and space color gradient similarity. Experimental results on three benchmark datasets demonstrate that FMQM outperforms state-of-the-art (SOTA) TMQA metrics. Furthermore, FMQM exhibits low computational complexity, making it a practical and efficient solution for real-world applications in 3D graphics and visualization. Our code is publicly available at: https://github.com/yyyykf/FMQM.},
  archive      = {J_TVCG},
  author       = {Kaifa Yang and Qi Yang and Yiling Xu and Zhu Li},
  doi          = {10.1109/TVCG.2025.3612942},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Textured mesh quality assessment using geometry and color field similarity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SketchRefiner: Text-guided sketch refinement through latent diffusion models. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3613388'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free-hand sketches serve as efficient tools for creativity and communication, yet expressing ideas clearly through sketches remains challenging for untrained individuals. Optimizing sketches through text guidance can enhance individuals' ability to effectively convey their ideas and improve overall communication efficiency. While recent advancements in Artificial Intelligence Generated Content (AIGC) have been notable, research on optimizing free-hand sketches remains relatively unexplored. In this paper, we introduce SketchRefiner, an innovative method designed to refine rough sketches from various categories into polished versions guided by text prompts. SketchRefiner utilizes a latent diffusion model with ControlNet to guide a differentiable rasterizer in optimizing a set of Bézier curves. We extend the score distillation sampling (SDS) loss and introduce a joint semantic loss to encourage sketches aligned with given text prompts and free-hand sketches. Additionally, we propose a fusion attention-map stroke initialization strategy to improve the quality of refined sketches. Furthermore, SketchRefiner provides users with fine-grained control over text guidance. Through extensive experiments, we demonstrate that our method can generate accurate and aesthetically pleasing refined sketches that closely align with input text prompts and sketches.},
  archive      = {J_TVCG},
  author       = {Yingjie Tian and Minghao Liu and Haoran Jiang and Yunbin Tu and Duo Su},
  doi          = {10.1109/TVCG.2025.3613388},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SketchRefiner: Text-guided sketch refinement through latent diffusion models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latent space map for visual utilization of generated data. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3614247'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Samples produced by generative models, called Generated Samples (GSs), have become a critical supplement to those collected from the real world in data-centric applications. Domain experts typically randomly collect many GSs and manually select a few of interest for applications. However, the methodology lacks guidance to locate desirable ones that exhibit specific features or adhere to application-oriented metrics among infinite generable candidates. These samples are generally concentrated in a few small regions of the generative model's latent space, called Generative Latent Space (GLS). This paper presents Latent Space Map that projects a GLS onto a plane to help users locate regions rich in desirable GSs. Our research revolves around two challenges in constructing the map. First, many GSs in a GLS are low-quality and useless for applications. Excluding them from the projection is challenging for their irregular distribution. We employ a Monte Carlo-based method to capture a manifold for projection, where high-quality GSs are mainly distributed. Second, the GLS is high-dimensional and unbounded, complicating the projection. We design a manifold projection method that endows the map with desirable characteristics to achieve high display accuracy and effective pattern perception for users freely observing the manifold. We further develop a system integrating Latent Space Map to aid in GS selection and refinement. Real-world cases, quantitative experiments, and feedback from domain experts confirm the usability and effectiveness of our approach.},
  archive      = {J_TVCG},
  author       = {Yang Zhang and Jie Li and Wei Zeng},
  doi          = {10.1109/TVCG.2025.3614247},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Latent space map for visual utilization of generated data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Perceptual model for foveated rendering with illuminance demodulation. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3614349'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Foveated rendering exploits the non-uniform acuity of human vision to allocate computational resources more efficiently by reducing image fidelity in the peripheral field of view. While existing perceptual models for foveated rendering focus primarily on spatial resolution and contrast sensitivity, they overlook the perceptual asymmetry between direct and indirect illumination. In this work, we introduce a novel perceptual model that incorporates illuminance demodulation to account for this distinction. Our model adaptively modulates the foveation rate based on the relative contributions of direct and indirect illumination. Building on this model, we develop a practical rendering framework that separately applies tailored foveation strategies to direct and indirect illumination effects. Quantitative metrics and user studies confirm that our method maintains perceptual equivalence to full-resolution rendering. The sparse rendering stage achieves a $2.18\times$ to $7.10\times$ speedup, contributing to an overall acceleration of $1.71\times$ to $3.26\times$.},
  archive      = {J_TVCG},
  author       = {Xiao Hu and Xiang Xu and JiuXing Zhang and YanNing Xu and Lu Wang},
  doi          = {10.1109/TVCG.2025.3614349},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Perceptual model for foveated rendering with illuminance demodulation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Parabolic sphere tracing of signed distance fields for old glass modelling and rendering. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3613853'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a method for modeling and rendering irregular and heterogeneous glass objects, with a specific emphasis on stained glass windows and window works often encountered in architecture from middle age to 18th century. The artisanal production of sheet glass results in glass panels displaying a vast variety of surface and volume irregularities like bubbles, irregular surface or smoothly varying refractive index, all of which contribute to the specific visual aspect of old glass. We propose to account for all the aforementioned effects in a unified framework based on signed distance functions and an analytic solution of the ray tracing equations on tetrahedral volume elements. We demonstrate how to construct an unbiased estimator for the transmitted lighting produced by such panels by using Fermat's principle and results from seismic ray theory. We use texture coordinates to map arbitrary sections of a complex glass panel onto the individual faces of a mesh, allowing the modeling and rendering of complex 3-dimensional objects composed of colored glass facets such as stained glass windows.},
  archive      = {J_TVCG},
  author       = {Quentin Huan and François Rousselle and Christophe Renaud},
  doi          = {10.1109/TVCG.2025.3613853},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Parabolic sphere tracing of signed distance fields for old glass modelling and rendering},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
