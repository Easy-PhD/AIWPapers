<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg">TVCG - 67</h2>
<ul>
<li><details>
<summary>
(2025). RP-SLAM: Real-time photorealistic SLAM with efficient 3D gaussian splatting. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3616173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian Splatting has emerged as a promising technique for high-quality 3D rendering, leading to increasing interest in integrating 3DGS into realism SLAM systems. However, existing methods face challenges such as Gaussian primitives redundancy, forgetting problem during continuous optimization, and difficulty in initializing primitives in monocular case due to lack of depth information. In order to achieve efficient and photorealistic mapping, we propose RP-SLAM, a 3D Gaussian splatting-based vision SLAM method for monocular and RGB-D cameras. RP-SLAM decouples camera poses estimation from Gaussian primitives optimization and consists of three key components. Firstly, we propose an efficient incremental mapping approach to achieve a compact and accurate representation of the scene through adaptive sampling and Gaussian primitives filtering. Secondly, a dynamic window optimization method is proposed to mitigate the forgetting problem and improve map consistency. Finally, for the monocular case, a monocular keyframe initialization method based on sparse point cloud is proposed to improve the initialization accuracy of Gaussian primitives, which provides a geometric basis for subsequent optimization. The results of numerous experiments demonstrate that RP-SLAM achieves state-of-the-art map rendering accuracy while ensuring real-time performance and model compactness.},
  archive      = {J_TVCG},
  author       = {Lizhi Bai and Chunqi Tian and Jun Yang and Siyu Zhang and Masanori Suganuma and Takayuki Okatani},
  doi          = {10.1109/TVCG.2025.3616173},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RP-SLAM: Real-time photorealistic SLAM with efficient 3D gaussian splatting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LeOp-GS: Learned optimizer with dynamic gradient update for sparse-view 3DGS. <em>TVCG</em>, 1-15. (<a href='https://doi.org/10.1109/TVCG.2025.3616156'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D Gaussian Splatting (3DGS) achieves remarkable speed and performance in novel view synthesis but suffers from overfitting and degraded reconstruction when handling sparse-view inputs. This paper innovatively addresses this challenge from a learning-to-optimize perspective by leveraging a learned optimizer (i.e., a multi-layer perceptron, MLP) to update the relevant parameters of 3DGS during the optimization process. Evidently, using a single MLP to handle all optimization variables, whose numbers may even vary during the optimization process, is impossible. Therefore, we present a point-wise position-aware optimizer that updates the parameters for each 3DGS point individually. Specifically, it takes the point coordinates and corresponding parameter values as input to predict the updates, thereby allowing efficient and adaptive optimization. In the case of sparse view modeling, the learned optimizer imposes position-aware constraints on the parameter updates during optimization. This effectively encourages the relevant parameters to converge stably to better solutions. To update the optimizer's parameters, we propose a dynamic gradient update strategy based on spatial perturbation and weighted fusion, enabling the optimizer to capture broader contextual information. Experiments demonstrate that our method effectively addresses the problem of modeling 3DGS from sparse training views, achieving state-of-the-art results across multiple datasets.},
  archive      = {J_TVCG},
  author       = {Xinyu Lei and Xuan Wang and Longjun Liu and Haoteng Li and Haonan Zhang},
  doi          = {10.1109/TVCG.2025.3616156},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LeOp-GS: Learned optimizer with dynamic gradient update for sparse-view 3DGS},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of realism on hand redirection in immersive environments. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616743'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Redirection in virtual reality (VR) enhances haptic feedback versatility by relaxing the need for precise alignment between virtual and physical objects. In mixed reality (MR), where users see the real world and their own hands, haptic redirection enables a physical interaction with virtual objects but poses greater challenges due to altering real-world perception. This paper investigates the effect of the realism of the user's surroundings and of the user's hand on haptic redirection. The user's familiarity with their actual physical surroundings and their actual hand could make the redirection manipulations easier–or harder–to detect. In a user study (N = 30) participants saw either a virtual environment or their actual physical surroundings, and saw their hand rendered either with a generic 3D model or with a live 2D video sprite of their actual hand. The study used a two-alternative forced choice (2AFC) design asking participants to detect hand redirections that bridged physical to virtual offsets of varying magnitudes. The results show that participants were not more sensitive to 2D video sprite hand redirection than to VR hand redirection, which supports the use of haptic redirection in MR.},
  archive      = {J_TVCG},
  author       = {Shuqi Liao and Yuqi Zhou and Voicu Popescu},
  doi          = {10.1109/TVCG.2025.3616743},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effect of realism on hand redirection in immersive environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PACT: Modeling coordination dynamics in scale-asymmetric virtual reality collaboration. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616831'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In virtual reality (VR), collaborators often experience the same environment at different visual scales, disrupting shared attention and increasing coordination difficulty. While prior work has focused on preventing misalignment, less is known about how teams recover when alignment fails. We examine collaboration under scale asymmetry, a particularly disruptive form of perceptual divergence. In a study with 36 VR teams, we identify behavioral patterns that distinguish adaptive recovery from persistent breakdown. Successful teams flexibly shifted between user-driven and system-supported cues, while others repeated ineffective strategies. Based on these findings, we introduce the Perceptual Asymmetry Coordination Theory (PACT), a dual-pathway model that describes coordination as an evolving process shaped by cue integration and strategic responsiveness. PACT reframes recovery not as a return to alignment, but as a dynamic adaptation to misalignment. These insights inform the design of VR systems that support recovery through multi-channel, adaptive coordination in scale-divergent environments.},
  archive      = {J_TVCG},
  author       = {Hayeon Kim and In-Kwon Lee},
  doi          = {10.1109/TVCG.2025.3616831},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PACT: Modeling coordination dynamics in scale-asymmetric virtual reality collaboration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The impact of AI-based real-time gesture generation and immersion on the perception of others and interaction quality in social XR. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616864'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores how people interact in dyadic social eXtended Reality (XR), focusing on two main factors: the animation type of a conversation partner's avatar and how immersed the user feels in the virtual environment. Specifically, we investigate how 1) idle behavior, 2) AI-generated gestures, and 3) motion-captured movements from a confederate (a controlled partner in the study) influence the quality of conversation and how that partner is perceived. We examined these effects in both symmetric interactions (where both participants use VR headsets and controllers) and asymmetric interactions (where one participant uses a desktop setup). We developed a social XR platform that supports asymmetric device configurations to provide varying levels of immersion. The platform also supports a modular avatar animation system providing idle behavior, real-time AI-generated co-speech gestures, and full-body motion capture. Using a 2×3 mixed design with 39 participants, we measured users' sense of spatial presence, their perception of the confederate, and the overall conversation quality. Our results show that users who were more immersed felt a stronger sense of presence and viewed their partner as more human-like and believable. Surprisingly, however, the type of avatar animation did not significantly affect conversation quality or how the partner was perceived. Participants often reported focusing more on what was said rather than how the avatar moved.},
  archive      = {J_TVCG},
  author       = {Christian Merz and Niklas Krome and Carolin Wienrich and Stefan Kopp and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2025.3616864},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The impact of AI-based real-time gesture generation and immersion on the perception of others and interaction quality in social XR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Radiance fields in XR: A survey on how radiance fields are envisioned and addressed for XR research. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616794'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of radiance fields (RF), such as 3D Gaussian Splatting (3DGS) and Neural Radiance Fields (NeRF), has revolutionized interactive photorealistic view synthesis and presents enormous opportunities for XR research and applications. However, despite the exponential growth of RF research, RF-related contributions to the XR community remain sparse. To better understand this research gap, we performed a systematic survey of current RF literature to analyze (i) how RF is envisioned for XR applications, (ii) how they have already been implemented, and (iii) the remaining research gaps. We collected 365 RF contributions related to XR from computer vision, computer graphics, robotics, multimedia, human-computer interaction, and XR communities, seeking to answer the above research questions. Among the 365 papers, we performed an analysis of 66 papers that already addressed a detailed aspect of RF research for XR. With this survey, we extended and positioned XR-specific RF research topics in the broader RF research field and provide a helpful resource for the XR community to navigate within the rapid development of RF research.},
  archive      = {J_TVCG},
  author       = {Ke Li and Mana Masuda and Susanne Schmidt and Shohei Mori},
  doi          = {10.1109/TVCG.2025.3616794},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Radiance fields in XR: A survey on how radiance fields are envisioned and addressed for XR research},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The role of visual augmentation on embodied skill acquisition across perspectives and body representations. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616832'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive embodiment holds great promise for motor skill acquisition, however the design and effect of real-time visual guidance across perspectives and body representations remain underexplored. This study introduces a puppet-inspired visual feedback framework that uses continuous visual linkages — line, color, and thickness cues — to externalize spatial deviation and scaffold embodied learning. To evaluate its effectiveness, we conducted a controlled virtual reality experiment (N = 40) involving gesture imitation tasks with fine (sign language) and gross (aviation marshalling) motor components, under first- and third-person viewpoints. Results showed that color-based guidance significantly improved imitation accuracy, short-term learning, and perceived embodiment, especially in finger-based and first-person settings. Subjective assessments (NASA-TLX, Motivation, IPQ, Embodiment) confirmed improvements in presence, agency, and task engagement.},
  archive      = {J_TVCG},
  author       = {Ruochen Cao and Zequn Liang and Chenkai Zhang and Andrew Cunningham and James A. Walsh and Rui Cao},
  doi          = {10.1109/TVCG.2025.3616832},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The role of visual augmentation on embodied skill acquisition across perspectives and body representations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal calibration and ground truth estimation for high-precision SLAM benchmarking in extended reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616838'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous localization and mapping (SLAM) plays a fundamental role in extended reality (XR) applications. As the standards for immersion in XR continue to increase, the demands for SLAM benchmarking have become more stringent. Trajectory accuracy is the key metric, and marker-based optical motion capture (MoCap) systems are widely used to generate ground truth (GT) because of their drift-free and relatively accurate measurements. However, the precision of MoCap-based GT is limited by two factors: the spatiotemporal calibration with the device under test (DUT) and the inherent jitter in the MoCap measurements. These limitations hinder accurate SLAM benchmarking, particularly for key metrics like rotation error and inter-frame jitter, which are critical for immersive XR experiences. This paper presents a novel continuous-time maximum likelihood estimator to address these challenges. The proposed method integrates auxiliary inertial measurement unit (IMU) data to compensate for MoCap jitter. Additionally, a variable time synchronization method and a pose residual based on screw congruence constraints are proposed, enabling precise spatiotemporal calibration across multiple sensors and the DUT. Experimental results demonstrate that our approach outperforms existing methods, achieving the precision necessary for comprehensive benchmarking of state-of-the-art SLAM algorithms in XR applications. Furthermore, we thoroughly validate the practicality of our method by benchmarking several leading XR devices and open-source SLAM algorithms. The code is publicly available at https://github.com/ylab-xrpg/xr-hpgt.},
  archive      = {J_TVCG},
  author       = {Zichao Shu and Shitao Bei and Lijun Li and Zetao Chen},
  doi          = {10.1109/TVCG.2025.3616838},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Spatiotemporal calibration and ground truth estimation for high-precision SLAM benchmarking in extended reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Source-free model adaptation for unsupervised 3D object retrieval. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3617082'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the explosive growth of 3D objects yet expensive annotation costs, unsupervised 3D object retrieval has become a popular but challenging research area. Existing labeled resources have been utilized to aid this task via transfer learning, which aligns the distribution of unlabeled data with the source one. However, the labeled resource are not always accessible due to the privacy disputes, limited computational capacity and other thorny restrictions. Therefore, we propose source-free model adaptation task for unsupervised 3D object management, which utilizes a pre-trained model to boost the performance with no access to source data and labels. Specifically, we compute representative prototypes to assume the source feature distribution, and design a bidirectional cumulative confidence-based adaptation strategy to adaptively align unlabeled samples towards prototypes. Subsequently, a dual-model distillation mechanism is proposed to generate source hypothesis for remedying the absence of ground-truth labels. The experiments on a cross-domain retrieval benchmark NTU-PSB (PSB-NTU) and a cross-modality retrieval benchmark MI3DOR also demonstrate the superiority of the proposed method even without access to raw data. Code is available at: https://github.com/Wyyspace1203/MA},
  archive      = {J_TVCG},
  author       = {Dan Song and Yiyao Wu and Yuting Ling and Diqiong Jiang and Yao Jin and Ruofeng Tong},
  doi          = {10.1109/TVCG.2025.3617082},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Source-free model adaptation for unsupervised 3D object retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bundling-aware graph drawing revisited. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3616583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge bundling algorithms can significantly improve the visualization of dense graphs by identifying and bundling together suitable groups of edges and thus reducing visual clutter. As such, bundling is often viewed as a post-processing step applied to a drawing, and the vast majority of edge bundling algorithms consider a graph and its drawing as input. A different way of thinking about edge bundling is to simultaneously optimize both the drawing and the bundling, which we investigate in this paper. We build on an earlier work where we introduced a novel algorithmic framework for bundling-aware graph drawing consisting of three main steps, namely Filter for a skeleton subgraph, Draw the skeleton, and Bundle the remaining edges against the drawing of the skeleton. We propose several alternative implementations and experimentally compare them against each other and the simple idea of first drawing the full graph and subsequently applying edge bundling to it. The experiments confirm that bundled drawings created by our Filter-Draw-Bundle framework outperform previous approaches according to metrics for edge bundling and graph drawing.},
  archive      = {J_TVCG},
  author       = {Markus Wallinger and Tommaso Piselli and Alessandra Tappini and Daniel Archambault and Giuseppe Liotta and Martin Nöllenburg},
  doi          = {10.1109/TVCG.2025.3616583},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Bundling-aware graph drawing revisited},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Application of transitional mixed reality interfaces: A co-design study with flood-prone communities. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616755'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flood risk communication in disaster-prone communities often relies on traditional tools (e.g., paper and browser-based hazard/flood maps) that struggle to engage community stakeholders and reflect intuitive flood situations. In this paper, we applied the transitional mixed reality (MR) interface concept from pioneering work and extended it for flood risk communication scenarios through co-design with community stakeholders to help vulnerable residents understand flood risk and facilitate preparedness. Starting with an initial transitional MR prototype, we conducted three iterative workshops - each dedicated to device usability, visualization techniques, and interaction methods. We collaborated with diverse community stakeholders in flood-prone areas, collecting feedback to refine the system according to community needs. Our preliminary evaluation indicates that this co-designed system significantly improves user understanding and engagement compared to traditional tools, though some older residents faced usability challenges. We detailed this iterative co-design process, critical insights and design implications, offering our work as a practical case of mixed reality application in strengthening flood risk communication. We also discuss the system's potential to support community-driven collaboration in flood preparedness.},
  archive      = {J_TVCG},
  author       = {Zhiling Jie and Geert Lugtenberg and Renjie Zhang and Armin Teubert and Makoto Fujisawa and Hideaki Uchiyama and Kiyoshi Kiyokawa and Isidro Butaslac and Taishi Sawabe and Hirokazu Kato},
  doi          = {10.1109/TVCG.2025.3616755},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Application of transitional mixed reality interfaces: A co-design study with flood-prone communities},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving mid-air sketching in room-scale virtual reality with dynamic color-to-depth and opacity cues. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616867'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive 3D mid-air sketching systems liberate users from the confines of traditional 2D sketching canvases. However, complications from perceptual challenges in Virtual Reality (VR), combined with the ergonomic and cognitive challenges of sketching in all three dimensions in mid-air lower the accuracy and aesthetic quality of 3D sketches. This paper explores how color-to-depth and opacity cues support users to create and perceive freehand, 3D strokes in room-scale sketching, unlocking a full 360°of freedom for creation. We implemented three graphic depth shader cues modifying the (1) alpha, (2) hue, and (3) value levels of a single color to dynamically adjust the color and transparency of meshes relative to their depth from the user. We investigated how these depth cues influence sketch efficiency, sketch quality, and total sketch experience with 24 participants in a comparative, counterbalanced, 4 × 1 within-subjects user study. First, with our graphic depth shader cues we could successfully transfer results of prior research in seated sketching tasks to room-scale scenarios. Our color-to-depth cues improved the similarity of sketches to target models. This highlights the usefulness of the color-to-depth approach even for the increased range of motion and depth in room-scale sketching. Second, our shaders assisted participants to complete tasks faster, spend a greater percentage of task time sketching, reduced the feeling of mental tiredness and improved the feeling of sketch efficiency in room-scale sketching. We discuss these findings and share our insights and conclusions to advance the research on improving spatial cognition in immersive sketching systems.},
  archive      = {J_TVCG},
  author       = {Samantha Monty and Dennis Mevißen and Marc Erich Latoschik},
  doi          = {10.1109/TVCG.2025.3616867},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Improving mid-air sketching in room-scale virtual reality with dynamic color-to-depth and opacity cues},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards augmented reality support for swarm monitoring: Evaluating visual cues to prevent fragmentation. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616840'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Swarm fragmentation, the breakdown of communication and coordination among robots, can critically compromise a swarm's mission. Integrating Augmented Reality support into swarm monitoring—especially through co-located visualisations anchored directly on the robots— may enable human operators to detect early signs of fragmentation and intervene effectively. In this work, we propose three localised visual cues—targeting robot connectivity, dominant decision influences, and movement direction—to make explicit the underlying Perception-Decision-Action (PDA) loop of each robot. Through an immersive Virtual Reality user study, 51 participants were tasked with both anticipating potential fragmentation and selecting the appropriate control to prevent it, while observing swarms exhibiting expansion, densification, flocking, and swarming behaviours. Our results reveal that a visualisation emphasising inter-robot connectivity significantly improves anticipation of fragmentation, though none of the cues consistently enhance control selection over a baseline condition. These findings underscore the potential of co-located AR-enhanced visual feedback to support human-swarm interaction and inform the design of future AR-based supervisory systems for robot swarms. A free copy of this paper and all supplemental materials are available at https://osf.io/49gny.},
  archive      = {J_TVCG},
  author       = {Aymeric Hénard and Étienne Peillard and Jérémy Rivière and Sébastien Kubicki and Gilles Coppin},
  doi          = {10.1109/TVCG.2025.3616840},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Towards augmented reality support for swarm monitoring: Evaluating visual cues to prevent fragmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WarpVision: Using spatial curvature to guide attention in virtual reality. <em>TVCG</em>, 1-9. (<a href='https://doi.org/10.1109/TVCG.2025.3616806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of consumer-targeted, low-cost virtual reality devices and facile authoring technologies, the development and design of experiences in virtual reality are also becoming more accessible to non-expert authors. However, the inherent freedom of exploration in these virtual spaces presents a significant challenge for designers seeking to guide user attention toward points and objects of interest. This paper proposes the new technique WarpVision, which utilizes spatial curvature to subtly guide the user's attention in virtual reality. WarpVision distorts an area around the point of interest, thus changing the size, form, and location of all objects and the space around them. In this way, the user's attention can be guided even when the point of interest is not in the immediate field of vision. WarpVision is evaluated in a user study based on a within-subjects design, comparing it to the state-of-the-art technique Deadeye. Participants completed visual search tasks across two virtual environments being supported with WarpVision at four different intensities. Results show that WarpVision significantly reduces search times compared to Deadeye. While both techniques introduce comparable levels of immersion disruption, WarpVision has a lower reported impact on the user's well-being.},
  archive      = {J_TVCG},
  author       = {Jérôme Kudnick and Martin Weier and Colin Groth and Biying Fu and Robin Horst},
  doi          = {10.1109/TVCG.2025.3616806},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {WarpVision: Using spatial curvature to guide attention in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Seeing what matters: Attentional (Mis-)Alignment between humans and AI in VR-simulated prediction of driving accidents. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616811'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study explores how human and AI visual attention differ in a short-term prediction task, particularly in the moments before an accident is about to happen. Since real-world studies of this kind would pose ethical and safety risks, we employed virtual reality (VR) to simulate an accident scenario. In the scenario, the driver approaches a fork in the road, knowing that one path would lead off a cliff crashing the car fatally—as the fork comes closer, the other, safe, path is suddenly blocked by trees, forcing the driver to make a split-second decision where to go. A total of $N = 71$ drivers completed the task, and we asked another $N = 30$ observers to watch short video clips leading up to the final event and to predict which way the driver would take. We then compared both prediction accuracy as well as attention patterns—how focus is distributed across objects—with AI systems, including vision language models (VLMs) and vision-only models. We found that overall, prediction performance increased as the accident time point approached; interestingly, humans fared better than AI systems overall except for the final time period just before the event. We also found that humans adapted their attention dynamically, shifting focus to important scene elements before an event, whereas AI attention remained static, overlooking key details of the scene. Importantly, as the accident time point approached, human-AI attentional alignment decreased, even though both types of models improved in prediction accuracy. Despite distinct temporal trajectories—vision-only models declining from an early advantage and VLMs peaking in the middle—both models achieved low to zero alignment with human attention. These findings highlight a critical dissociation: AI models make accurate predictions, but rely on visual strategies diverging from human processing, underscoring a gap between explainability and task performance.},
  archive      = {J_TVCG},
  author       = {Hoe Sung Ryu and Uijong Ju and Christian Wallraven},
  doi          = {10.1109/TVCG.2025.3616811},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Seeing what matters: Attentional (Mis-)Alignment between humans and AI in VR-simulated prediction of driving accidents},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An empirical evaluation of how virtual hand visibility affects near-field size perception and reporting of tangible objects in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616829'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In immersive virtual environments (IVEs), accurate size perception is critical, especially in training simulations designed to mimic real-world tasks, such as, nuclear power plant control room or medical procedures. These simulations have dials or instruments of varying sizes. Visual information of the objects alone, often fails to capture subtle size differences in virtual reality (VR). However, integrating haptic and hand-avatars may potentially improve accuracy and performance. This improvement could be especially beneficial for real-world scenarios where hand(s) are intermittently visible or obscured. To investigate how this intermittent presence or absence of body-scaled hand-avatars affects size perception when integrated with haptic information, we conducted 2×2 mixed-factorial experiment design using a near-field, size-estimation task in VR. The experiment conditions compared size estimations with or without virtual hand visibility in the perception and reporting phases. The task involved 16 graspable objects of varying sizes and randomly repeated 3 times across 48 trials per participant (total 80 participants). We employed Linear Mixed Models (LMMs) analysis to objective measures: perceived size, residual error and proportional errors. Results revealed that as the tangible-graspable size increases, overestimation reduces if the hand-avatars are visible in the reporting phase. Also, overestimation reduces as the number of trials increases, if the hand-avatars are visible in the reporting phase. Thus, the presence of hand-avatars facilitated perceptual calibration. This novel study, with different combinations of hand-avatar visibility, taking perception and reporting of size as two separate phases, could open future research directions in more complex scenarios for refined integration of sensory modalities and consequently enhance real-world application performance.},
  archive      = {J_TVCG},
  author       = {Chandni Murmu and Rohith Venkatakrishnan and Roshan Venkatakrishnan and Wen-Chieh Lin and Andrew C. Robb and Christopher Pagano and Sabarish V. Babu},
  doi          = {10.1109/TVCG.2025.3616829},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {An empirical evaluation of how virtual hand visibility affects near-field size perception and reporting of tangible objects in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A handheld stiffness display with a programmable spring and electrostatic clutches for haptic interaction in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616795'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handheld haptic devices often face challenges in delivering stiffness feedback with both high force output and good backdrivability, especially under practical constraints on power consumption, size, and weight. These difficulties stem from the inherent performance limitations of conventional actuation mechanisms. To address this issue, we propose a lightweight, low-power handheld device that provides wide-range stiffness feedback through a novel dual actuation design composed of two key components. A programmable spring (PS), implemented via an adjustable lever arm, enables tunable physical stiffness. Two electrostatic clutches (ECs) are integrated to compensate for the inherent limitations of PS-based interactions in stiffness display range, rendered object size, and free motion capability. The feedback force arises passively from the reaction of the PS and ECs to user input, effectively lowering both power consumption and actuator torque demands. A fully integrated prototype was developed, incorporating wireless communication, control, and power modules. The results of the evaluation experiments and user studies demonstrate that the device effectively renders stiffness across the full range, from free motion to full rigidity, and delivers more realistic elastic feedback compared to conventional electric motor-based systems.},
  archive      = {J_TVCG},
  author       = {Ke Shi and Quan Xiong and Maozeng Zhang and Aiguo Song and Lifeng Zhu},
  doi          = {10.1109/TVCG.2025.3616795},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A handheld stiffness display with a programmable spring and electrostatic clutches for haptic interaction in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Influence of object height, shadow and adapting luminance on outdoor depth perception in augmented reality. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616839'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented reality (AR) technology has great potential in the applications of training, exhibition, and visual guidance, all of which demand precise virtual-real registration in perceived depth. Many AR applications such as navigation and tourism guidance are usually implemented in outdoor environments. However, prior research on depth perception in AR predominantly focused on the indoor environment, characterized by a lower illumination level and more confined space compared to outdoor settings. To address this gap, this paper presented a systematic investigation into the depth perception in outdoor environments. Two experiments were conducted in this study: the first one aimed to explore how to eliminate the bias induced by the floating object and how the knowledge of object height influences the perceived depth. The second experiment examined how ambient luminance affects depth estimation in AR. Our findings revealed an overestimation of perceived depth when participants were unaware of the actual height of the floating object, but an underestimation when they were informed of this information prior to the experiment. Additionally, shadows effectively reduced depth errors regardless of whether participants were informed of the object's height. The second experiment further indicated that, in outdoor environments, reducing ambient luminance significantly improves the accuracy of depth perception in AR.},
  archive      = {J_TVCG},
  author       = {Shining Ma and Chaochao Liu and Jingyuan Wang and Yue Liu and Yongtian Wang and Weitao Song},
  doi          = {10.1109/TVCG.2025.3616839},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Influence of object height, shadow and adapting luminance on outdoor depth perception in augmented reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ZonAware: Identifying zoning out and increasing engagement in upper limb virtual reality rehabilitation. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616818'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zoning out, a form of cognitive disengagement, seriously challenges the effectiveness of virtual reality (VR) based upper limb rehabilitation. As therapy often involves repetitive tasks requiring sustained attention, undetected lapses in focus can reduce motor learning, engagement, and overall recovery outcomes. This research addresses this gap by proposing ZonAware, a novel strategy integrating real-time zoning out detection with adaptive intervention to enhance user engagement during VR rehabilitation. ZonAware identifies zoning out using five eye-tracking metrics: blink frequency, blink duration, pupil size, eye openness, and gaze duration. These signals are analysed through lightweight statistical models (Z-Score, Boxplot, and Modified Z-Score), with a hard voting mechanism producing binary classifications in real-time. Upon detection, a pattern changing intervention subtly modulates task difficulty by temporarily increasing, then decreasing it, to regain user focus without breaking immersion. Three user studies involving 70 healthy participants and 22 patients demonstrated the strategy's effectiveness. ZonAware achieved 98.24% detection accuracy with low latency (82–150 ms), reducing zoning out frequency by 53.57% and shortening disengagement duration from 18.1 to 4.8 seconds. The approach also improved user engagement, performance, and emotional motivation. ZonAware delivers one of the first real-time zoning out solutions for VR rehabilitation, offering an interpretable, theory-driven approach that enhances attention, engagement, and adaptability in human-computer interaction.},
  archive      = {J_TVCG},
  author       = {Kai-Lun Liao and Mengjie Huang and Jiajia Shi and Min Chen and Rui Yang},
  doi          = {10.1109/TVCG.2025.3616818},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {ZonAware: Identifying zoning out and increasing engagement in upper limb virtual reality rehabilitation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Investigating the effects of haptic illusions in collaborative virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616760'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Our sense of touch plays a crucial role in physical collaboration, yet rendering realistic haptic feedback in collaborative extended reality (XR) remains a challenge. Co-located XR systems predominantly rely on prefabricated passive props that provide high-fidelity interaction but offer limited adaptability. Haptic Illusions (HIs), which leverage multisensory integration, have proven effective in expanding haptic experiences in single-user contexts. However, their role in XR collaboration has not been explored. To examine the applicability of HIs in multi-user scenarios, we conducted an experimental user study (N=30) investigating their effect on a collaborative object handover task in virtual reality. We manipulated visual shape and size individually and analyzed their impact on users' performance, experience, and behavior. Results show that while participants adapted to the illusions by shifting sensory reliance and employing specific sensorimotor strategies, visuo-haptic mismatches reduced both performance and experience. Moreover, mismatched visualizations in asymmetric user roles negatively impacted performance. Drawing from these findings, we provide practical guidelines for incorporating HIs into collaborative XR, marking a first step toward richer haptic interactions in shared virtual spaces.},
  archive      = {J_TVCG},
  author       = {Yannick Weiss and Julian Rasch and Jonas Fischer and Florian Müller},
  doi          = {10.1109/TVCG.2025.3616760},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Investigating the effects of haptic illusions in collaborative virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HFM-GS: Half-face mapping 3DGS avatar based real-time HMD removal. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616801'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In extended reality (XR) applications, enhancing user perception often necessitates head-mounted display (HMD) removal. However, existing methods suffer from low time performance and suboptimal reconstruction quality. In this paper, we propose a half face mapping 3D Gaussian splatting avatar based HMD removal method (HFM-GS), which can perform real-time and high-fidelity online restoration of the complete face in HMD-occluded videos for XR applications after a short un-occluded face registration. We establish a mapping field between the upper and lower face Gaussians to enhance the adaptability to deformation. Then, we introduce correlation weight-based sampling to improve time performance and handle variations in the number of Gaussians. At last, we ensure model robustness through Gaussian Segregation Strategy. Compared to two state-of-the-art methods, our method achieves better quality and time performance. The results of the user study show that fidelity is significantly improved with our method.},
  archive      = {J_TVCG},
  author       = {Kangyu Wang and Jian Wu and Runze Fan and Hongwen Zhang and Sio Kei Im and Lili Wang},
  doi          = {10.1109/TVCG.2025.3616801},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {HFM-GS: Half-face mapping 3DGS avatar based real-time HMD removal},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Selection at a distance through a large transparent touch screen. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616756'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large transparent touch screens (LTTS) have recently become commercially available. These displays have the potential for engaging Augmented Reality (AR) applications, especially in public and shared spaces. However, the interaction with objects in the real environment behind the display remains challenging: Users must combine pointing and touch input if they want to select objects at varying distances. There is a lot of work on wearable or mobile AR displays, but little on how users interact with LTTS. Our goal is to contribute to a better understanding of natural user interaction for these AR displays. To this end, we developed a prototype and evaluated different pointing techniques for selecting 12 physical targets behind an LTTS, with distances ranging from 6 to 401 cm. We conducted a user study with 16 participants and measured user preferences, performance, and behavior. We analyzed the change in accuracy depending on the target position and the selection technique used. Our fndings include: (a) Users naturally align the touch point with their line of sight for targets farther than 36 cm behind the LTTS. (b) This technique provides the lowest angular deviation compared to other techniques. (c) Some user close one eye to improve their performance. Our results help to improve future AR scenarios using LTTS systems.},
  archive      = {J_TVCG},
  author       = {Sebastian Rigling and Steffen Koch and Dieter Schmalstieg and Bruce H. Thomas and Michael Sedlmair},
  doi          = {10.1109/TVCG.2025.3616756},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Selection at a distance through a large transparent touch screen},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SGSG: Stroke-guided scene graph generation. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616751'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D scene graph generation is essential for spatial computing in Extended Reality (XR), providing structured semantics for task planning and intelligent perception. However, unlike instance-segmentation-driven setups, generating semantic scene graphs still suffer from limited accuracy due to coarse and noisy point cloud data typically acquired in practice, and from the lack of interactive strategies to incorporate users, spatialized and intuitive guidance. We identify three key challenges: designing controllable interaction forms, involving guidance in inference, and generalizing from local corrections. To address these, we propose SGSG, a Stroke-Guided Scene Graph generation method that enables users to interactively refine 3D semantic relationships and improve predictions in real time. We propose three types of strokes and a lightweight SGstrokes dataset tailored for this modality. Our model integrates stroke guidance representation and injection for spatio-temporal feature learning and reasoning correction, along with intervention losses that combine consistency-repulsive and geometry-sensitive constraints to enhance accuracy and generalization. Experiments and the user study show that SGSG outperforms state-of-the-art methods 3DSSG and SGFN in overall accuracy and precision, surpasses JointSSG in predicate-level metrics, and reduces task load across all control conditions, establishing SGSG as a new benchmark for interactive 3D scene graph generation and semantic understanding in XR. Implementation resources are available at: https://github.com/Sycamore-Ma/SGSG-runtime.},
  archive      = {J_TVCG},
  author       = {Qixiang Ma and Runze Fan and Lizhi Zhao and Jian Wu and Sio-Kei Im and Lili Wang},
  doi          = {10.1109/TVCG.2025.3616751},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SGSG: Stroke-guided scene graph generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Viewpoint-tolerant depth perception for shared extended space experience on wall-sized display. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616758'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We proposed viewpoint-tolerant shared depth perception without individual tracking by leveraging human cognitive compensation in universally 3D rendered images on a wall-sized display. While traditional 3D-perception-enabled display systems have primarily focused on single-user scenarios—adapting rendering based on head and eye tracking—the use of wall-sized displays to extend spatial experiences and support perceptually coherent multi-user interactions remains underexplored. We investigated the effects of virtual depths (dv) and absolute viewing distance (da) on human cognitive compensation factors (perceived distance difference, viewing angle threshold, and perceived presence) to construct the wall display-based eXtended Reality (XR) space. Results show that participants experienced a compelling depth perception even from off-center angles of 23°–37°, and largely increasing virtual depth worsens depth perception and presence factors, highlighting the importance of balancing extended depth of virtual space and viewing distance from the wall-sized display. Drawing on these findings, wall-sized displays in venues such as museums, galleries, and classrooms can evolve beyond 2D information sharing to offer immersive, spatially extended group experiences without individualized tracking or wearables.},
  archive      = {J_TVCG},
  author       = {Dooyoung Kim and Jinseok Hong and Heejeong Ko and Woontack Woo},
  doi          = {10.1109/TVCG.2025.3616758},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Viewpoint-tolerant depth perception for shared extended space experience on wall-sized display},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the effects of augmented reality guidance position within a body-fixed coordinate system on pedestrian navigation. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AR head-mounted displays (HMDs) facilitate pedestrian navigation by integrating AR guidance into users' field of view (FOV). Displaying AR guidance using a body-fixed coordinate system has the potential to further leverage this integration by enabling users to control when the guidance appears in their FOV. However, it remains unclear how to effectively position AR guidance within this coordinate system during pedestrian navigation. Therefore, we explored the effects of three AR guidance positions (top, middle, and bottom) within a body-fixed coordinate system on pedestrian navigation in a virtual environment. Our results showed that AR guidance position significantly influenced eye movements, walking behaviors, and subjective evaluations. The top position resulted in the shortest duration of fixations on the guidance compared to the middle and bottom positions, and lower mental demand than the bottom position. The middle position had the smallest rate of vertical eye movement during gaze shifts between the guidance and the environment, and the smallest relative difference in walking speed between fixations on the guidance and the environment compared to the top and bottom positions. The bottom position led to the shortest duration and smallest amplitude of gaze shifts between the guidance and the environment compared to the top and middle positions, and lower frustration than the top position. Based on these findings, we offer design implications for AR guidance positioning within a body-fixed coordinate system during pedestrian navigation.},
  archive      = {J_TVCG},
  author       = {Shunbo Wang and Qing Xu and Klaus Schoeffmann},
  doi          = {10.1109/TVCG.2025.3616773},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring the effects of augmented reality guidance position within a body-fixed coordinate system on pedestrian navigation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Impact of avatar-locomotion congruence on user experience and identification in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616836'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As virtual reality (VR) continues to expand, particularly in social VR platforms and immersive gaming environments, understanding the factors that shape user experience is becoming increasingly important. Avatars and locomotion methods play central roles in influencing how users identify with their virtual representations and navigate virtual spaces. Despite extensive research on these elements individually, their relationship remains underexplored. In particular, little is known about how congruence between avatar appearance and locomotion method affects user perceptions. This study investigates the impact of avatar-locomotion congruence on user experience and avatar identification in VR. We conducted a within-subjects experiment with 30 participants, employing two visually distinct avatar types (human and gorilla) and two locomotion methods (human-like arm-swinging and gorilla-like arm-rolling), to assess their individual and combined effects. Our results indicate that congruence between avatar appearance and locomotion method enhances both avatar identification and user experience. These findings contribute to the understanding of the relationship between avatars and locomotion in VR, with potential applications in enhancing user experience in immersive gaming, social VR, and gamified remote physical therapy},
  archive      = {J_TVCG},
  author       = {Omar Khan and Hyeongil Nam and Kangsoo Kim},
  doi          = {10.1109/TVCG.2025.3616836},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Impact of avatar-locomotion congruence on user experience and identification in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Immersive intergroup contact: Using virtual reality to enhance empathy and reduce stigma towards schizophrenia. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616759'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stigma towards individuals with schizophrenia reduces quality of life, creating a barrier to accessing education and employment opportunities. Schizophrenia is one of the most stigmatized mental health conditions, and stigma is prevalent particularly among healthcare professionals. In this study, we investigated whether Virtual Reality (VR) can be incorporated into interventions to reduce stigma. In particular, we compared the effectiveness of three VR conditions based on intergroup contact theory in reducing stigma in form of implicit and explicit attitudes, and behavioral intentions. Through an immersive virtual consultation in a clinical setting, participants (N = 60) experienced one of three different conditions: the Doctor's perspective (embodiment in a majority group member during contact), the Patient's perspective (embodiment in a minority group member) and a Third-person perspective (vicarious contact). Results demonstrated an increase of stigma on certain explicit measures (perceived recovery and social restriction) but also an increase of empathy (perspective-taking, empathic concern) across all conditions regardless of perspective. More importantly, participants' viewpoint influenced the desire for social distance differently depending on the perspective: the Third-person observation significantly increased the desire for social distance, Doctor embodiment marginally decreased it, while Patient embodiment showed no significant change. No change was found in the Implicit Association Test. These findings suggest that VR intergroup contact can effectively reduce certain dimensions of stigma toward schizophrenia, but the type of perspective experienced significantly impacts outcomes.},
  archive      = {J_TVCG},
  author       = {Jiaqi Yin and Shihan Liu and Shao-Wen Lee and Andreas Kitsios and Marco Gillies and Michèle Denise Birtel and Harry Farmer and Xueni Pan},
  doi          = {10.1109/TVCG.2025.3616759},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Immersive intergroup contact: Using virtual reality to enhance empathy and reduce stigma towards schizophrenia},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extended reality check: Evaluating XR prototyping for human-robot interaction in contact-intensive tasks. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616753'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extended Reality (XR) has the potential to improve efficiency and safety in the user-centered development of human-robot interaction. However, the validity of using XR prototyping for user studies for contact-intensive robotic tasks remains underexplored. These in-contact tasks are particularly relevant due to challenges arising from indirect force perception in robot control. Therefore, in this work, we investigate a representative example of such a task: robotic ultrasound. A user study was conducted to assess the transferability of results from a simulated user study to real-world conditions, comparing two force-assistance approaches. The XR simulation replicates the physical study set-up employing a virtual robotic arm, its control interface, ultrasound imaging, and two force-assistance methods: automation and force visualization. Our results indicate that while differences in force deviation, perceived workload, and trust emerge between real and simulated setups, the overall findings remain consistent. Specifically, partial automation of robot control improves performance and trust while reducing workload, and visual feedback decreases force deviation in both real and simulated conditions. These findings highlight the potential of XR for comparative studies, even in complex robotic tasks.},
  archive      = {J_TVCG},
  author       = {Tonia Mielke and Mareen Allgaier and Christian Hansen and Florian Heinrich},
  doi          = {10.1109/TVCG.2025.3616753},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Extended reality check: Evaluating XR prototyping for human-robot interaction in contact-intensive tasks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MR-CoCo: An open mixed reality testbed for co-located couple product configuration and decision-making – A sailboat case study. <em>TVCG</em>, 1-9. (<a href='https://doi.org/10.1109/TVCG.2025.3616734'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The literature has demonstrated the advantages of Mixed Reality (MR) for product configuration by providing a more engaging and effective end-user experience. While collaborative and remote design tools in MR have been widely explored in previous studies, a noticeable gap remains in the exploration of co-located product configuration for couples. This gap is noteworthy since in many industries, couples (e.g., friends, partners) often make purchasing decisions together in physical retail environments. In this paper, we introduce MR-CoCo, an open MR testbed designed to explore collaborative configurations by co-located couples, both in the role of customers. The testbed is developed in Unity and features: (i) a shared MR space with virtual product 3D model anchoring, (ii) shared visualization of the current configuration, (iii) a versatile UI for selecting configuration areas, (iv) hand gestures for 3D drag and drop of colors and materials from 3D catalog to the product. A case study of the personalization of a sailboat is provided as proof of concept. The user study involved 24 couples (48 participants in total), simulating a purchasing experience and the related configuration using MR-CoCo. We assessed usability through post-experience evaluations, with the System Usability Scale (SUS) and the Co-Presence Configuration Questionnaire (CCQ) to measure collaboration and decision-making. The results demonstrated a high level of usability and perceived quality of collaboration. We also explore guidelines that can be used for remote collaboration applications, enabling configuration across a wide range of industries (e.g., automotive and clothing).},
  archive      = {J_TVCG},
  author       = {Fabio Vangi and Daniel Medeiros and Mine Dastan and Michele Fiorentino},
  doi          = {10.1109/TVCG.2025.3616734},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MR-CoCo: An open mixed reality testbed for co-located couple product configuration and decision-making – A sailboat case study},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Experiencing immersive virtual nature for well-being, restoration, performance, and nature connectedness: A scoping review. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616762'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a scoping review of immersive virtual nature experiences delivered via head-mounted displays (HMDs) and their role in promoting well-being, psychological restoration, cognitive performance, and nature connectedness. As access to natural environments becomes increasingly constrained by urbanization, technological lifestyles, and environmental change, immersive technologies offer a scalable and accessible alternative for engaging with nature. Guided by three core research questions, this review explores how HMD-mediated immersive technologies have been used to promote nature connectedness and well-being, what trends and outcomes have been observed across applications, and what methodological gaps or limitations exist in this growing body of work. Fifty-five peer-reviewed studies were analyzed and categorized into six key implication areas: emotional well-being, stress reduction, cognitive performance, attention recovery, restorative benefits, and nature connectedness. The review identifies immersive virtual nature as a promising application of extended reality (XR) technologies, with potential across healthcare, education, and daily life, while also emphasizing the need for more consistent methodologies and long-term research.},
  archive      = {J_TVCG},
  author       = {Jeewoo Kim and Svara Patel and Hyeongil Nam and Janghee Cho and Kangsoo Kim},
  doi          = {10.1109/TVCG.2025.3616762},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Experiencing immersive virtual nature for well-being, restoration, performance, and nature connectedness: A scoping review},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Effects of AI-powered embodied avatars on communication quality and social connection in asynchronous virtual meetings. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616761'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive technologies such as virtual and augmented reality (VR/AR) allow remote users to meet and interact in a shared virtual space using embodied virtual avatars, creating a sense of co-presence. However, asynchronous communication—essential in many real-world contexts—remains underexplored in these environments. Traditional playback-based systems lack interactivity and often fail to preserve critical contextual cues necessary for effective asynchronous communication. In this paper, we introduce AVAGENTs, AI-powered virtual avatars that replicate users' verbal and nonverbal cues from recordings of past meetings. Avagents can interpret meeting context and generate appropriate responses to questions posed by asynchronous viewers. Through a user study (N =30), we evaluated Avagents against a traditional playback method and a voice-based AI assistant across two asynchronous meeting scenarios: analytic reasoning and affective resonance. Results showed that Avagents enhance the asynchronous communication experience by increasing social presence, sense of belonging, emotional intimacy, and other user perceptions. We discuss the findings and their implications for designing effective AI-driven asynchronous communication tools in VR/AR environments.},
  archive      = {J_TVCG},
  author       = {Hyeongil Nam and Muskan Sarvesh and Seoyoung Kang and Woontack Woo and Kangsoo Kim},
  doi          = {10.1109/TVCG.2025.3616761},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Effects of AI-powered embodied avatars on communication quality and social connection in asynchronous virtual meetings},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing learning and knowledge retention of abstract physics concepts with virtual reality. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616826'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) is increasingly recognized as a powerful tool for science education, offering interactive environments to explore intangible concepts. Traditional teaching methods often struggle to convey abstract concepts in science, where many phenomena are not directly observable. VR can address this issue by modeling and visualizing complex and unobservable entities and processes, allowing learners to dynamically interact with what would otherwise not be directly perceptible. However, relatively few controlled studies have compared immersive VR learning with equivalent hands-on laboratory learning in physics education, particularly for more abstract topics. In this work, we designed a VR-based physics lab that is capable of visualizing electrons and electromagnetic fields to teach fundamental concepts of electronics and magnetism, closely replicating a traditional electronics learning kit used as a baseline for comparison. We evaluated the impact of the two conditions (VR versus traditional) on students' learning outcomes, motivation, engagement, and cognitive load. Our results show significantly higher knowledge retention in the VR group compared to the traditional group. Also, while there were no significant differences in immediate comprehension between the two groups, participants in the VR group spent substantially more time engaged with the learning content. These findings highlight the potential of visually enriched virtual environments to enhance the learning experience and improve knowledge retention of intangible scientific concepts.},
  archive      = {J_TVCG},
  author       = {M. Akif Akdag and Jean Botev and Steffen Rothkugel},
  doi          = {10.1109/TVCG.2025.3616826},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Enhancing learning and knowledge retention of abstract physics concepts with virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-illumination-interfered neural holography with expanded eyebox. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616793'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Holography has immense potential for near-eye displays in virtual and augmented reality (VR/AR), providing natural 3D depth cues through wavefront reconstruction. However, balancing the field of view (FOV) with the eyebox remains challenging, constrained by the étendue limitation. Additionally, holographic image quality is often compromised due to differences between actual wave propagation and simulation models. This study addresses these by expanding the eyebox via multi-angle illumination, and enhancing image quality with end-to-end pupil-aware hologram optimization. Further, energy efficiency is improved by incorporating higher-order diffractions and pupil constraints. We explore a Pupil-HOGD algorithm for multi-angle illumination and validate it with a dual-angle holographic display prototype. Integrated with camera calibration and tracked eye position, the developed Pupil-HOGD algorithm improves image quality and expands the eyebox by 50% horizontally. We envision this approach extends the space-bandwidth product (SBP) of holographic displays, enabling broader applications in immersive, high-quality visual computing.},
  archive      = {J_TVCG},
  author       = {Xinxing Xia and Pengfei Mi and Yiqing Tao and Xiangyu Meng and Wenbin Zhou and Yingjie Yu and Yifan Peng},
  doi          = {10.1109/TVCG.2025.3616793},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multi-illumination-interfered neural holography with expanded eyebox},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When LLMs recognize your space: Research on experiences with spatially aware LLM agents. <em>TVCG</em>, 1-9. (<a href='https://doi.org/10.1109/TVCG.2025.3616809'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) have evolved into LLM agents that can use the user conversation context and respond according to the roles of the LLM agents. Recent studies have suggested that LLM-based agents can be used as human-like partners in social interactions. However, the role of the environmental context, particularly spatial information of user space, in the interaction between humans and LLM agents has not been explored. In this study, participants engaged in counselling conversations under three different conditions based on their spatial awareness levels. The dependent measures included copresence, trust, therapist alliances, and self-disclosure. The results suggested that participants in the condition where the LLM actively reflected spatial information generally reported higher levels of user experience. Interestingly, when the LLM actively reflected the spatial context of the user, the participants tended to describe themselves and express their emotions more. These findings suggest that spatially aware LLM agents can contribute to better social interactions between humans and LLM agents. Our findings can be used to design future augmented reality applications in the counselling, education, and healthcare industries.},
  archive      = {J_TVCG},
  author       = {Seungwoo Oh and Nakyoung An and Youngwug Cho and Myeongul Jung and Kwanguk Kenny Kim},
  doi          = {10.1109/TVCG.2025.3616809},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {When LLMs recognize your space: Research on experiences with spatially aware LLM agents},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NAT: Neural acoustic transfer for interactive scenes in real time. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3617802'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous acoustic transfer methods rely on extensive precomputation and storage of data to enable real-time interaction and auditory feedback. However, these methods struggle with complex scenes, especially when dynamic changes in object position, material, and size significantly alter sound effects. These continuous variations lead to fluctuating acoustic transfer distributions, making it challenging to represent with basic data structures and render efficiently in real time. To address this challenge, we present Neural Acoustic Transfer, a novel approach that leverages implicit neural representations to encode acoustic transfer functions and their variations. This enables real-time prediction of dynamically evolving sound fields and their interactions with the environment under varying conditions. To efficiently generate high-quality training data for the neural acoustic field while avoiding reliance on mesh quality of a model, we develop a fast and efficient Monte-Carlo-based boundary element method (BEM) approximation, suitable for general scenarios with smooth Neumann boundary conditions. In addition, we devise strategies to mitigate potential singularities during the synthesis of training data, thereby enhancing its reliability. Together, these methods provide robust and accurate data that empower the neural network to effectively model complex sound radiation space. We demonstrate our method's numerical accuracy and runtime efficiency (within several milliseconds for 30s audio) through comprehensive validation and comparisons in diverse acoustic transfer scenarios. Our approach allows for efficient and accurate modeling of sound behavior in dynamically changing environments, which can benefit a wide range of interactive applications such as virtual reality, augmented reality, and advanced audio production.},
  archive      = {J_TVCG},
  author       = {Xutong Jin and Bo Pang and Chenxi Xu and Xinyun Hou and Guoping Wang and Sheng Li},
  doi          = {10.1109/TVCG.2025.3617802},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {NAT: Neural acoustic transfer for interactive scenes in real time},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Entering your space: How agent entrance styles shape social presence in AR. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616757'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Embodied conversational agents (ECAs) capable of non-verbal behaviors have been developed to address the limitations of voice-only assistants, with research exploring their use in mixed and augmented reality (AR), suggesting they may soon interact with us more naturally in physical spaces. Traditionally, AI voice assistants are activated through wake-up keywords, and since they are invisible, their method of appearance has not been a concern. However, for ECAs in AR, the question of how they should enter the user's space when summoned remains underexplored. In this paper, we focused on the plausibility of ECAs' entering action into the user's field of view in AR. We analyzed its impact on user experience, concentrating on perceived social presence and co-presence of the agent. Three entrance styles were chosen for comparison: an obviously impossible one, a possible one, and an intermediate one, alongside a voice-only condition. We designed and conducted a within-subjects study with 38 participants. Our results indicated that while the plausibility of the action had less impact on functionality compared to the embodiment itself, it significantly affected social/co-presence. These findings highlight the importance of entrance design for future AR agent experiences.},
  archive      = {J_TVCG},
  author       = {Junyeong Kum and Seungwon Kim and Myungho Lee},
  doi          = {10.1109/TVCG.2025.3616757},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Entering your space: How agent entrance styles shape social presence in AR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visuo-tactile feedback with hand outline styles for modulating affective roughness perception. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616805'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a visuo-tactile feedback method that combines virtual hand visualization and fingertip vibrations to modulate affective roughness perception in VR. While prior work has focused on object-based textures and vibrotactile feedback, the role of visual feedback on virtual hands remains underexplored. Our approach introduces affective visual cues including line shape, motion, and color applied to hand outlines, and examines their influence on both affective responses (arousal, valence) and perceived roughness. Results show that sharp contours enhanced perceived roughness, increased arousal, and reduced valence, intensifying the emotional impact of haptic feedback. In contrast, color affected valence only, with red consistently lowering emotional positivity. These effects were especially noticeable at lower haptic intensities, where visual cues extended affective modulation into mid-level perceptual ranges. Overall, the findings highlight how integrating expressive visual cues with tactile feedback can enrich affective rendering and offer flexible emotional tuning in immersive VR interactions.},
  archive      = {J_TVCG},
  author       = {Minju Baeck and Yoonseok Shin and Dooyoung Kim and Hyunjin Lee and Sang Ho Yoon and Woontack Woo},
  doi          = {10.1109/TVCG.2025.3616805},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visuo-tactile feedback with hand outline styles for modulating affective roughness perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visceral notices and privacy mechanisms for eye tracking in augmented reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616837'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head-worn augmented reality (AR) continues to evolve through critical advancements in power optimizations, AI capabilities, and naturalistic user interactions. Eye-tracking sensors play a key role in these advancements. At the same time, eye-tracking data is not well understood by users and can reveal sensitive information. Our work contributes visualizations based on visceral notice to increase privacy awareness of eye-tracking data in AR. We also evaluated user perceptions towards privacy noise mechanisms applied to gaze data visualized through these visceral interfaces. While privacy mechanisms have been evaluated against privacy attacks, we are the first to evaluate them subjectively and understand their influence on data-sharing attitudes. Despite our participants being highly concerned with eye-tracking privacy risks, we found 47% of our participants still felt comfortable sharing raw data. When applying privacy noise, 70% to 76% felt comfortable sharing their gaze data for the Weighted Smoothing and Gaussian Noise privacy mechanisms, respectively. This implies that participants are still willing to share raw gaze data even though overall data-sharing sentiments decreased after experiencing the visceral interfaces and privacy mechanisms. Our work implies that increased access and understanding of privacy mechanisms are critical for gaze-based AR applications; further research is needed to develop visualizations and experiences that relay additional information about how raw gaze data can be used for sensitive inferences, such as age, gender, and ethnicity. We intend to open-source our codebase to provide AR developers and platforms with the ability to better inform users about privacy concerns and provide access to privacy mechanisms. A pre-print of this paper and all supplemental materials are available at https://bmdj-vt.github.io/project_pages/privacy_notice.},
  archive      = {J_TVCG},
  author       = {Nissi Otoo and Kailon Blue and G. Nikki Ramirez and Evan Selinger and Shaun Foster and Brendan David-John},
  doi          = {10.1109/TVCG.2025.3616837},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visceral notices and privacy mechanisms for eye tracking in augmented reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ada: A distributed, power-aware, real-time scene provider for XR. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616835'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time scene provisioning—reconstructing and delivering scene data to requesting XR applications during runtime—is central to enabling spatial computing in modern XR systems. However, existing solutions struggle to balance latency, power and scene fidelity under XR device constraints, and often rely on designs that are either closed, application-specific designs, or both. We present Ada, the first open distributed, power-aware, application-agnostic real-time scene provisioning system. Through computation offloading along with algorithmic and system innovations, Ada provides high-fidelity scenes with stable performance across all evaluated scene sizes and with low power consumption. To isolate the benefits of Ada's algorithmic and design innovations over the closest prior work [82], which is on-device and CPU-based, we configure a comparable on-device, CPU-based variant of Ada (AdaLocal- CPU). We show this variant achieves up to 6.8× lower scene request latency and higher scene fidelity compared to the prior work. Furthermore, Ada's final distributed GPU-accelerated implementation reduces latency by an additional 2×, highlighting the benefits of GPU acceleration and distributed computing. Additionally, Ada also lowers the incremental power cost of scene provisioning by 24% compared to the best on-device variant (AdaLocal-GPU). Finally, Ada flexibly adapts to diverse latency, power, scene fidelity, and network bandwidth requirements.},
  archive      = {J_TVCG},
  author       = {Yihan Pang and Sushant Kondguli and Shenlong Wang and Sarita Adve},
  doi          = {10.1109/TVCG.2025.3616835},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Ada: A distributed, power-aware, real-time scene provider for XR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Handows: A palm-based interactive multi-window management system in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616843'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Window management in virtual reality (VR) remains a challenging task due to the spatial complexity and physical demands of current interaction methods. We introduce Handows, a palm-based interface that enables direct manipulation of spatial windows through familiar smartphone-inspired gestures on the user's non-dominant hand. Combining ergonomic layout design with body-centric input and passive haptics, Handows supports four core operations: window selection, closure, positioning, and scaling. We evaluate Handows in a user study (N = 15) against two common VR techniques (virtual hand and controller) across four core window operations. Results show that Handows significantly reduces physical effort and head movement while improving task efficiency and interaction precision. A follow-up case study (N = 8) demonstrates Handows' usability in realistic multitasking scenarios, highlighting user-adapted workflows and spontaneous layout strategies. Our findings also suggest the potential of embedding mobile-inspired metaphors into proprioceptive body-centric interfaces to support low-effort and spatially coherent interaction in VR.},
  archive      = {J_TVCG},
  author       = {Jin-Du Wang and Ke Zhou and Haoyu Ren and Per Ola Kristensson and Xiang Li},
  doi          = {10.1109/TVCG.2025.3616843},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Handows: A palm-based interactive multi-window management system in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). The effect of hand visibility in AR: Comparing dexterity and interaction with virtual and real objects. <em>TVCG</em>, 1-9. (<a href='https://doi.org/10.1109/TVCG.2025.3616868'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hand-tracking technologies allow us to use our own hands to interact with real and virtual objects in Augmented Reality (AR) environments. This enables us to explore the interplay between hand-visualizations and hand-object interactions. We present a user study that examines the effect of different hand visualizations (invisible, transparent, opaque) on manipulation performance when interacting with real and virtual objects. For this, we implemented video-see-through (VST) AR-based virtual building blocks and hot wire tasks with real one-to-one counterparts that require participants to use gross and fine motor hand movements. To evaluate manipulation performance, we considered three measures: task completion time, number of collisions (hot wire task), and percentage of object displacement (building block task). Additionally, we explored the sense of agency and subjective impressions (preference, ease of interaction, successful and awkwardness) evoked by the different hand-visualizations. The results show that (1) manipulation performance is significantly higher when interacting with real objects compared to virtual ones, (2) invisible hands lead to fewer errors, higher agency, higher perceived success and ease of interaction during fine manipulation tasks with real objects, and (3) having some visualization of the virtual hands (transparent or opaque) overlayed on the real hands is preferred when manipulating virtual objects even when there are no significant performance improvements. Our empirical findings about the differences when interacting with real and virtual objects can aid hand visualization choices for manipulation tasks in AR.},
  archive      = {J_TVCG},
  author       = {Jakob Hartbrich and Stephanie Arévalo Arboleda and Steve Göring and Alexander Raake},
  doi          = {10.1109/TVCG.2025.3616868},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {The effect of hand visibility in AR: Comparing dexterity and interaction with virtual and real objects},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal contrastive learning for cybersickness recognition using brain connectivity graph representation. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616797'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cybersickness significantly impairs user comfort and immersion in virtual reality (VR). Effective identification of cybersickness leveraging physiological, visual, and motion data is a critical prerequisite for its mitigation. However, current methods primarily employ direct feature fusion across modalities, which often leads to limited accuracy due to inadequate modeling of inter-modal relationships. In this paper, we propose a multimodal contrastive learning method for cybersickness recognition. First, we introduce Brain Connectivity Graph Representation (BCGR), an innovative graph-based representation that captures cybersickness-related connectivity patterns across modalities. We further develop three BCGR instances: E-BCGR, constructed based on EEG signals; MV-BCGR, constructed based on video and motion data; and S-BCGR, obtained through our proposed standardized decomposition algorithm. Then, we propose a connectivity-constrained contrastive fusion module, which aligns E-BCGR and MV-BCGR into a shared latent space via graph contrastive learning while utilizing S-BCGR as a connectivity constraint to enhance representation quality. Moreover, we construct a multimodal cybersickness dataset comprising synchronized EEG, video, and motion data collected in VR environments to promote further research in this domain. Experimental results demonstrate that our method outperforms existing state-of-the-art methods across four critical evaluation metrics: accuracy, sensitivity, specificity, and the area under the curve. Source code: https://github.com/PEKEW/cybersickness-bcgr.},
  archive      = {J_TVCG},
  author       = {Peike Wang and Ming Li and Ziteng Wang and Yong-Jin Liu and Lili Wang},
  doi          = {10.1109/TVCG.2025.3616797},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Multimodal contrastive learning for cybersickness recognition using brain connectivity graph representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). “Heart flows with zen”: Exploring multi-modal mixed reality to promote the inheritance and experience of cultural heritage. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616750'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The preservation of cultural heritage (CH) is a complex and promising field. Driven by technological advancements, digitization has emerged as a crucial approach for revitalizing tangible/intangible cultural heritage (TCH/ICH). However, current research and practice remain limited in their exploration of abstract forms of ICH, such as traditional philosophies and ideologies. In this study, utilizing Zen as a context, we designed an immersive mixed reality (MR) experience system, Flowing with Zen, based on formative study and cultural symbol analysis. The MR system integrates multi-modal interfaces, motion capture, environmental sensing, and generative computing, enabling users to engage with four scenarios through meditation, life appreciation, and experiential Zen practice, providing the embodied experience of Zen. Comparative user evaluation (N = 51) revealed that the MR system has significant advantages in eliciting engagement and interest from users, enhancing their aesthetic appreciation and cultural understanding, and increasing the accessibility of Zen. Our research proposes a novel approach and design inspiration for the digital inheritance of abstract ICH.},
  archive      = {J_TVCG},
  author       = {Wenchen Guo and Zhirui Chen and Guoyu Sun and Hailiang Wang},
  doi          = {10.1109/TVCG.2025.3616750},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {“Heart flows with zen”: Exploring multi-modal mixed reality to promote the inheritance and experience of cultural heritage},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GS-ProCams: Gaussian splatting-based projector-camera systems. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616841'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present GS-ProCams, the first Gaussian Splatting-based framework for projector-camera systems (ProCams). GSProCams is not only view-agnostic but also significantly enhances the efficiency of projection mapping (PM) that requires establishing geometric and radiometric mappings between the projector and the camera. Previous CNN-based ProCams are constrained to a specific viewpoint, limiting their applicability to novel perspectives. In contrast, NeRF-based ProCams support view-agnostic projection mapping, however, they require an additional co-located light source and demand significant computational and memory resources. To address this issue, we propose GS-ProCams that employs 2D Gaussian for scene representations, and enables efficient view-agnostic ProCams applications. In particular, we explicitly model the complex geometric and photometric mappings of ProCams using projector responses, the projection surface's geometry and materials represented by Gaussians, and the global illumination component. Then, we employ differentiable physically-based rendering to jointly estimate them from captured multi-view projections. Compared to state-of-the-art NeRF-based methods, our GS-ProCams eliminates the need for additional devices, achieving superior ProCams simulation quality. It also uses only 1/10 of the GPU memory for training and is 900 times faster in inference speed. Please refer to our project page for the code and dataset: https://realqingyue.github.io/GS-ProCams/.},
  archive      = {J_TVCG},
  author       = {Qingyue Deng and Jijiang Li and Haibin Ling and Bingyao Huang},
  doi          = {10.1109/TVCG.2025.3616841},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GS-ProCams: Gaussian splatting-based projector-camera systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). P-blend: Privacy- and utility-preserving blendshape perturbation against re-identification attacks in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616736'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose p-Blend, an efficient and effective blendshape perturbation mechanism designed to defend against both intra- and cross-app re-identification attacks in virtual reality. p-Blend provides privacy protection when streaming blendshape data to third-party applications on VR devices. In its design, we consider both privacy and utility. p-Blend not only perturbs blendshape values to resist re-identification attacks but also preserves the smoothness of facial animations and the naturalness of facial expressions, ensuring the continued usability of the data. We validate the effectiveness of p-Blend through extensive empirical evaluations and user studies. Quantitative experiments on a large-scale dataset collected from 45 participants demonstrate that p-Blend significantly reduces re-identification accuracy across a range of machine learning models. While pure-random perturbation fails to prevent attacks that exploit statistical features, p-Blend effectively mitigates these risks in both raw and statistical blendshape data. Additionally, user study results show that facial animations generated from p-Blend-perturbed blendshapes maintain greater smoothness and naturalness compared to those using purely random perturbation. The codes and dataset are available at https://github.com/jingwei1016/p-Blend.},
  archive      = {J_TVCG},
  author       = {Jingwei Liu and Lai Wei and Yan Hu and Guangrong Zhao and Qing Yang and Guangdong Bai and Yiran Shen},
  doi          = {10.1109/TVCG.2025.3616736},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {P-blend: Privacy- and utility-preserving blendshape perturbation against re-identification attacks in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LiteAT: A data-lightweight and user-adaptive VR telepresence system for remote education. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616747'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In educators' ongoing pursuit of enriching remote education, Virtual Reality (VR)-based telepresence has shown significant promise due to its immersive and interactive nature. Existing approaches often rely on point cloud or NeRF-based techniques to deliver realistic representations of teachers and classrooms to remote students. However, achieving low latency is non-trivial, and maintaining high-fidelity rendering under such constraints poses an even greater challenge. This paper introduces LiteAT, a data-lightweight and user-adaptive VR telepresence system, to enable real-time, immersive learning experiences. LiteAT employs a Gaussian Splatting-based reconstruction pipeline that integrates an SMPL-X–driven dynamic human model with a static classroom, supporting lightweight data transmission and high-quality rendering. To enable efficient and personalized exploration in the virtual classroom, we propose a user-adaptive viewpoint recommendation framework that dynamically suggests high-quality viewpoints tailored to user preferences. Candidate viewpoints are evaluated based on multiple visual quality factors and are continuously optimized based on recent user behavior and scene dynamics. Quantitative experiments and user studies validate the effectiveness of LiteAT across multiple evaluation metrics. LiteAT establishes a versatile and scalable foundation for immersive telepresence, potentially supporting real-time scenarios such as procedural teaching, multimodal instruction, and collaborative learning.},
  archive      = {J_TVCG},
  author       = {Yuxin Shen and Wei Liang and Jianzhu Ma},
  doi          = {10.1109/TVCG.2025.3616747},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LiteAT: A data-lightweight and user-adaptive VR telepresence system for remote education},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Behavioral and symbolic fillers as delay mitigation for embodied conversational agents in virtual reality. <em>TVCG</em>, 1-9. (<a href='https://doi.org/10.1109/TVCG.2025.3616865'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When communicating with embodied conversational agents (ECAs) in virtual reality, there might be delays in the responses of the agents lasting several seconds, for example, due to more extensive computations of the answers when large language models are used. Such delays might lead to unnatural or frustrating interactions. In this paper, we investigate filler types to mitigate these effects and lead to a more positive experience and perception of the agent. In a within-subject study, we asked 24 participants to communicate with ECAs in virtual reality, comparing four strategies displayed during the delays: a multimodal behavioral filler consisting of conversational and gestural fillers, a base condition with only idle motions, and two symbolic indicators with progress bars, one embedded as a badge on the agent, the other one external and visualized as a thinking bubble. Our results indicate that the behavioral filler improved perceived response time, three subscales of presence, humanlikeness, and naturalness. Participants looked away from the face more often when symbolic indicators were displayed, but the visualizations did not lead to a more positive impression of the agent or to increased presence. The majority of participants preferred the behavioral fillers, only 12.5% and 4.2% favored the symbolic embedded and external conditions, respectively.},
  archive      = {J_TVCG},
  author       = {Denmar Mojan Gonzales and Snehanjali Kalamkar and Sophie Jörg and Jens Grubert},
  doi          = {10.1109/TVCG.2025.3616865},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Behavioral and symbolic fillers as delay mitigation for embodied conversational agents in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring and modeling the effects of eye-tracking accuracy and precision on gaze-based steering in virtual environments. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in eye-tracking technology have positioned gaze as an efficient and intuitive input method for Virtual Reality (VR), offering a natural and immersive user experience. As a result, gaze input is now leveraged for fundamental interaction tasks such as selection, manipulation, crossing, and steering. Although several studies have modeled user steering performance across various path characteristics and input methods, our understanding of gaze-based steering in VR remains limited. This gap persists because the unique qualities of eye movements—involving rapid, continuous motions—and the variability in eye-tracking make findings from other input modalities nontransferable to a gaze-based context, underscoring the need for a dedicated investigation into gaze-based steering behaviors and performance. To bridge this gap, we present two user studies to explore and model gaze-based steering. In the first one, user behavior data are collected across various path characteristics and eye-tracking conditions. Based on this data, we propose four refined models that extend the classic Steering Law to predict users' movement time in gaze-based steering tasks, explicitly incorporating the impact of tracking quality. The best-performing model achieves an adjusted R2 of 0.956, corresponding to a 16% improvement in movement time prediction. This model also yields a substantial reduction in AIC (from 1550 to 1132) and BIC (from 1555 to 1142), highlighting improved model quality and better balance between goodness of fit and model complexity. Finally, data from a second study with varied settings, such as a different eye-tracking sampling rate, illustrate the strong robustness and predictability of our models. Finally, we present scenarios and applications that demonstrate how our models can be used to design enhanced gaze-based interactions in VR systems.},
  archive      = {J_TVCG},
  author       = {Xuning Hu and Yichuan Zhang and Yushi Wei and Liangyuting Zhang and Yue Li and Wolfgang Stuerzlinger and Hai-Ning Liang},
  doi          = {10.1109/TVCG.2025.3616824},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring and modeling the effects of eye-tracking accuracy and precision on gaze-based steering in virtual environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facilitating the exploration of linearly aligned objects in controller-free 3D environment with gaze and microgestures. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As Extended Reality (XR) environments increasingly involve large amounts of data and content, designing effective exploration techniques has become critical. Depth-based object exploration is a common but underexplored task in XR environments, especially in settings without physical devices. Prior studies have largely focused on horizontal or planar interactions, leaving depthoriented exploration relatively overlooked. To bridge this gap, we propose three linearly aligned layer transition techniques (Continuous Push, Push&Return, and Tilt&Return) specifically designed to support efficient, precise, and continuous object exploration along the depth axis within depth-based UIs. In a user study with 30 participants, we compared their performance, usability, and user preference across two different layer configurations (8-layer vs. 16-layer). The results highlight that Continuous Push enables faster exploration with lower effort, while Push&Return provides the highest accuracy and is most preferred by users. Based on these findings, we discuss design implications for depth-based interaction techniques in controller-free XR environments.},
  archive      = {J_TVCG},
  author       = {Jihyeon Lee and Jinwook Kim and Jeongmi Lee},
  doi          = {10.1109/TVCG.2025.3616833},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Facilitating the exploration of linearly aligned objects in controller-free 3D environment with gaze and microgestures},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LITFORAGER: Exploring multimodal literature foraging strategies in immersive sensemaking. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616732'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring and comprehending relevant academic literature is a vital yet challenging task for researchers, especially given the rapid expansion in research publications. This task fundamentally involves sensemaking—interpreting complex, scattered information sources to build understanding. While emerging immersive analytics tools have shown cognitive benefits like enhanced spatial memory and reduced mental load, they predominantly focus on information synthesis (e.g., organizing known documents). In contrast, the equally important information foraging phase—discovering and gathering relevant literature—remains underexplored within immersive environments, hindering a complete sensemaking workflow. To bridge this gap, we introduce LITFORAGER, an interactive literature exploration tool designed to facilitate information foraging of research literature within an immersive sensemaking workflow using network-based visualizations and multimodal interactions. Developed with WebXR and informed by a formative study with researchers, LITFORAGER supports exploration guidance, spatial organization, and seamless transition through a 3D literature network. An observational user study with 15 researchers demonstrated LITFORAGER's effectiveness in supporting fluid foraging strategies and spatial sensemaking through its multimodal interface.},
  archive      = {J_TVCG},
  author       = {Haoyang Yang and Elliott H. Faa and Weijian Liu and Shunan Guo and Duen Horng Chau and Yalong Yang},
  doi          = {10.1109/TVCG.2025.3616732},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {LITFORAGER: Exploring multimodal literature foraging strategies in immersive sensemaking},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Let's do it my way: Effects of personality and age of virtual characters. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616815'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing interactions between humans and virtual characters requires careful consideration of various human perceptions and user experiences. While numerous studies have explored the effects of several virtual characters' properties, the impacts of the virtual character's personality and age on human perceptions and experiences have yet to be thoroughly investigated. To address this gap, we conducted a within-group study (N = 28) following a 2 (personality: egoism vs. altruism) × 2 (age: child vs. adult) design to explore how the personality and age factors influence human perception and experience during interactions with virtual characters. In each condition of our study, our participants co-solved a jigsaw puzzle with a virtual character that embodied combinations of personality and age. After each condition, participants completed a survey. We also asked them to provide written feedback at the end of the study. Our statistical analyses revealed that the virtual character's personality and age significantly influenced participants' perceptions and experiences. The personality factor affected perceptions of altruism, anthropomorphism, likability, safety, and all aspects of user experience, including perceived collaboration, rapport, emotional reactivity, and the desire for future interaction. Additionally, the virtual character's age affected our participants' ratings of the uncanny valley and likability. We also identified an interaction effect between personality and age factors on the virtual character's anthropomorphism. Based on our findings, we offered guidelines and insights for researchers aiming to design collaborative experiences with virtual characters of different personalities and ages.},
  archive      = {J_TVCG},
  author       = {Minsoo Choi and Dixuan Cui and Siqi Guo and Dominic Kao and Christos Mousas},
  doi          = {10.1109/TVCG.2025.3616815},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Let's do it my way: Effects of personality and age of virtual characters},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic eyebox steering for improved pinlight AR near-eye displays. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616807'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An optical-see-through near-eye display (NED) for augmented reality (AR) allows the user to perceive virtual and real imagery simultaneously. Existing technologies for optical-see-through AR NEDs involve trade-offs between key metrics such as field of view (FOV), eyebox size, form factor, etc. We have enhanced an existing compact wide-FOV pinlight AR NED design with real-time 3D pupil localization in order to dynamically steer and thus effectively enlarge the usable eyebox. This is achieved with a dual-camera rig that captures stereoscopic views of the pupils. The 3D pupil location is used to dynamically calculate a display pattern that spatio-temporally modulates the light entering the wearer's eyes. We have built a demonstrable compact prototype and have conducted a user study that indicates the effectiveness of our eyebox steering method (e.g., without eyebox steering, in 10.5% of our tests, users were unable to perceive the test pattern correctly before experiment timeout; with eyebox steering, that fraction decreased dramatically to 1.25%). This is a small yet crucial step in making simple wide-FOV pinlight NEDs usable for human users and not just as demonstration prototypes filmed with a precisely positioned camera standing in for the user's eye. Further contributions of this paper include a detailed description of display design, calibration technique, and user study design, all of which may benefit other NED research.},
  archive      = {J_TVCG},
  author       = {Xinxing Xia and Zheye Yu and Dongyu Qiu and Andrei State and Tat-Jen Cham and Frank Guan and Henry Fuchs},
  doi          = {10.1109/TVCG.2025.3616807},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Dynamic eyebox steering for improved pinlight AR near-eye displays},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comparison of user performance and experience between light field and conventional AR glasses. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3617940'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Light field AR glasses can provide better visual comfort than conventional AR glasses; however, studies on user performance comparison between them are notably scarce. In this paper, we present a systematic method employing a serial visual search task without confounding factors to quantify and compare the user performance and experience between these two types of AR glasses at two different viewing distances, 30 cm and 60 cm, and in two modes, purely virtual VR mode and virtualreal integration AR mode. The results show that the light field AR glasses led to a significantly faster reaction speed and higher accuracy than the conventional AR glasses at 30 cm in the AR mode. The participant feedback also shows that the former led to better virtual-real integration. User performance and experience of the light field AR glasses remained consistent across different viewing distances. Although the conventional AR glasses had a better search efficiency than the light field AR glasses at 60 cm in both AR and VR modes, it had more negative feedback from the participants. Overall, the design of this experiment successfully allows us to quantify the effect of VAC and underscores the strength of the evaluation method},
  archive      = {J_TVCG},
  author       = {Wei-An Teng and Su-Ling Yeh and Homer H. Chen},
  doi          = {10.1109/TVCG.2025.3617940},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comparison of user performance and experience between light field and conventional AR glasses},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Designing hand and forearm gestures to control virtual forearm for user-initiated forearm deformation. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3616825'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thanks to the development of virtual reality (VR) technology, there is growing research on VR avatar body deformation effects. However, previous research mainly focused on passive body deformation expression, leaving users with limited methods to actively control their virtual bodies. To address this gap, we explored user-controlled forearm deformation by investigating how hand and forearm gestures can be mapped to various degrees of avatar forearm deformation. We conducted a gesture design workshop with six designers to generate gesture sets for different forearm deformations and deformation degrees, resulting in 15 gesture sets. Then, we selected the three highest-rated gesture sets and conducted a comparative study to evaluate the sense of embodiment and user performance across the three gesture sets. Our findings provide design suggestions for gesture-controlled forearm deformation in VR.},
  archive      = {J_TVCG},
  author       = {Yilong Lin and Han Shi and Weitao Jiang and Xuesong Zhang and Hye-Young Jo and Yoonji Kim and Seungwoo Je},
  doi          = {10.1109/TVCG.2025.3616825},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Designing hand and forearm gestures to control virtual forearm for user-initiated forearm deformation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MetaRoundWorm: A virtual reality escape room game for learning the lifecycle and immune response to parasitic infections. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616752'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Promoting public health is challenging owing to its abstract nature, and individuals may be apprehensive about confronting it. Recently, there has been an increasing interest in using the metaverse and gamification as novel educational techniques to improve learning experiences related to the immune system. Thus, we present MetaRoundWorm, an immersive virtual reality (VR) escape room game designed to enhance the understanding of parasitic infections and host immune responses through interactive, gamified learning. The application simulates the lifecycle of Ascaris lumbricoides and corresponding immunological mechanisms across anatomically accurate environments within the human body. Integrating serious game mechanics with embodied learning principles, MetaRoundWorm offers players a task-driven experience combining exploration, puzzle-solving, and immune system simulation. To evaluate the educational efficacy and user engagement, we conducted a controlled study comparing MetaRoundWorm against a traditional approach, i.e., interactive slides. Results indicate that MetaRoundWorm significantly improves immediate learning outcomes, cognitive engagement, and emotional experience, while maintaining knowledge retention over time. Our findings suggest that immersive VR gamification holds promise as an effective pedagogical tool for communicating complex biomedical concepts and advancing digital health education.},
  archive      = {J_TVCG},
  author       = {Xuanru Cheng and Xian Wang and Chi-lok Tai and Lik-Hang Lee},
  doi          = {10.1109/TVCG.2025.3616752},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MetaRoundWorm: A virtual reality escape room game for learning the lifecycle and immune response to parasitic infections},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AttentionPainter: An efficient and adaptive stroke predictor for scene painting. <em>TVCG</em>, 1-16. (<a href='https://doi.org/10.1109/TVCG.2025.3618184'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stroke-based Rendering (SBR) aims to decompose an input image into a sequence of parameterized strokes, which can be rendered into a painting that resembles the input image. Recently, Neural Painting methods that utilize deep learning and reinforcement learning models to predict the stroke sequences have been developed, but suffer from longer inference time or unstable training. To address these issues, we propose AttentionPainter, an efficient and adaptive model for single-step neural painting. First, we propose a novel scalable stroke predictor, which predicts a large number of stroke parameters within a single forward process, instead of the iterative prediction of previous Reinforcement Learning or auto-regressive methods, which makes AttentionPainter faster than previous neural painting methods. To further increase the training efficiency, we propose a Fast Stroke Stacking algorithm, which brings 13 times acceleration for training. Moreover, we propose Stroke-density Loss, which encourages the model to use small strokes for detailed information, to help improve the reconstruction quality. Finally, we design a Stroke Diffusion Model as an application of AttentionPainter, which conducts the denoising process in the stroke parameter space and facilitates stroke-based inpainting and editing applications helpful for human artists' design. Extensive experiments show that AttentionPainter outperforms the state-of-the-art neural painting methods.},
  archive      = {J_TVCG},
  author       = {Yizhe Tang and Yue Wang and Teng Hu and Ran Yi and Xin Tan and Lizhuang Ma and Yu-Kun Lai and Paul L. Rosin},
  doi          = {10.1109/TVCG.2025.3618184},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AttentionPainter: An efficient and adaptive stroke predictor for scene painting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EnVisionVR: A scene interpretation tool for visual accessibility in virtual reality. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3617147'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective visual accessibility in Virtual Reality (VR) is crucial for Blind and Low Vision (BLV) users. However, designing visual accessibility systems is challenging due to the complexity of 3D VR environments and the need for techniques that can be easily retrofitted into existing applications. While prior work has studied how to enhance or translate visual information, the advancement of Vision Language Models (VLMs) provides an exciting opportunity to advance the scene interpretation capability of current systems. This paper presents EnVisionVR, an accessibility tool for VR scene interpretation. Through a formative study of usability barriers, we confirmed the lack of visual accessibility features as a key barrier for BLV users of VR content and applications. In response, we used our findings from the formative study to inform the design and development of EnVisionVR, a novel visual accessibility system leveraging a VLM, voice input and multimodal feedback for scene interpretation and virtual object interaction in VR. An evaluation with 12 BLV users demonstrated that EnVisionVR significantly improved their ability to locate virtual objects, effectively supporting scene understanding and object interaction.},
  archive      = {J_TVCG},
  author       = {Junlong Chen and Rosella P. Galindo Esparza and Vanja Garaj and Per Ola Kristensson and John Dudley},
  doi          = {10.1109/TVCG.2025.3617147},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EnVisionVR: A scene interpretation tool for visual accessibility in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DenseSplat: Densifying gaussian splatting SLAM with neural radiance prior. <em>TVCG</em>, 1-14. (<a href='https://doi.org/10.1109/TVCG.2025.3617961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian SLAM systems excel in real-time rendering and fine-grained reconstruction compared to NeRF-based systems. However, their reliance on extensive keyframes is impractical for deployment in real-world robotic systems, which typically operate under sparse-view conditions that can result in substantial holes in the map. To address these challenges, we introduce DenseSplat, the first SLAM system that effectively combines the advantages of NeRF and 3DGS. DenseSplat utilizes sparse keyframes and NeRF priors for initializing primitives that densely populate maps and seamlessly fill gaps. It also implements geometry-aware primitive sampling and pruning strategies to manage granularity and enhance rendering efficiency. Moreover, DenseSplat integrates loop closure and bundle adjustment, significantly enhancing frame-to-frame tracking accuracy. Extensive experiments on multiple large-scale datasets demonstrate that DenseSplat achieves superior performance in tracking and mapping compared to current state-of-the-art methods.},
  archive      = {J_TVCG},
  author       = {Mingrui Li and Shuhong Liu and Tianchen Deng and Hongyu Wang},
  doi          = {10.1109/TVCG.2025.3617961},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DenseSplat: Densifying gaussian splatting SLAM with neural radiance prior},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WonderHuman: Hallucinating unseen parts in dynamic 3D human reconstruction. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3618268'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present WonderHuman to reconstruct dynamic human avatars from a monocular video for high-fidelity novel view synthesis. Previous dynamic human avatar reconstruction methods typically require the input video to have full coverage of the observed human body. However, in daily practice, one typically has access to limited viewpoints, such as monocular front-view videos, making it a cumbersome task for previous methods to reconstruct the unseen parts of the human avatar. To tackle the issue, we present WonderHuman, which leverages 2D generative diffusion model priors to achieve high-quality, photorealistic reconstructions of dynamic human avatars from monocular videos, including accurate rendering of unseen body parts. Our approach introduces a Dual-Space Optimization technique, applying Score Distillation Sampling (SDS) in both canonical and observation spaces to ensure visual consistency and enhance realism in dynamic human reconstruction. Additionally, we present a View Selection strategy and Pose Feature Injection to enforce the consistency between SDS predictions and observed data, ensuring pose-dependent effects and higher fidelity in the reconstructed avatar. In the experiments, our method achieves SOTA performance in producing photorealistic renderings from the given monocular video, particularly for those challenging unseen parts. The project page and source code can be found at https://wyiguanw.github.io/WonderHuman/.},
  archive      = {J_TVCG},
  author       = {Zilong Wang and Zhiyang Dou and Yuan Liu and Cheng Lin and Xiao Dong and Yunhui Guo and Chenxu Zhang and Xin Li and Wenping Wang and Xiaohu Guo},
  doi          = {10.1109/TVCG.2025.3618268},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {WonderHuman: Hallucinating unseen parts in dynamic 3D human reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). See what i mean? mobile eye-perspective rendering for optical see-through head-mounted displays. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616739'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based scene understanding allows Augmented Reality (AR) systems to provide contextual visual guidance in unprepared, real-world environments. While effective on video see-through (VST) head-mounted displays (HMDs), such methods suffer on optical see-through (OST) HMDs due to misregistration between the world-facing camera and the user's eye perspective. To approximate the user's true eye view, we implement and evaluate three software-based eye-perspective rendering (EPR) techniques on a commercially available, untethered OST HMD (Microsoft HoloLens 2): (1) Plane-Proxy EPR, projecting onto a fixed-distance plane; (2) Mesh-Proxy EPR, using SLAM-based reconstruction for projection; and (3) Gaze-Proxy EPR, a novel eye-tracking-based method that aligns the projection with the user's gaze depth. A user study on real-world tasks underscores the importance of accurate EPR and demonstrates gaze-proxy as a lightweight alternative to geometry-based methods. We release our EPR framework as open source.},
  archive      = {J_TVCG},
  author       = {Gerlinde Emsenhuber and Tobias Langlotz and Denis Kalkofen and Markus Tatzgern},
  doi          = {10.1109/TVCG.2025.3616739},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {See what i mean? mobile eye-perspective rendering for optical see-through head-mounted displays},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Portable silent room: Exploring VR design for anxiety and emotion regulation for neurodivergent women and non-binary individuals. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616828'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurodivergent individuals, particularly those with Autism and Attention Deficit Hyperactivity Disorder (ADHD), frequently experience anxiety, panic attacks, meltdowns, and emotional dysregulation due to societal pressures and inadequate accommodations. These challenges are especially pronounced for neurodivergent women and non-binary individuals navigating intersecting barriers of neurological differences and gender expectations. This research investigates virtual reality (VR) as a portable safe space for emotional regulation, addressing challenges of sensory overload and motion sickness while enhancing relaxation capabilities. Our mixed-methods approach included an online survey (N = 223) and an ideation workshop (N = 32), which provided key design elements for creating effective calming VR environments. Based on these findings, we developed and iteratively tested VR prototypes with neurodivergent women and non-binary participants (N = 12), leading to a final version offering enhanced adaptability to individual sensory needs. This final prototype underwent a comprehensive evaluation with 25 neurodivergent participants to assess its effectiveness as a regulatory tool. This research contributes to the development of inclusive, adaptive VR environments that function as personalized “portable silent rooms” offering neurodivergent individuals on-demand access to sensory regulation regardless of physical location.},
  archive      = {J_TVCG},
  author       = {Kinga Skierś and Yun Suen Pai and Marina Nakagawa and Kouta Minamizawa and Giulia Barbareschi},
  doi          = {10.1109/TVCG.2025.3616828},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Portable silent room: Exploring VR design for anxiety and emotion regulation for neurodivergent women and non-binary individuals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). “I was truly able to express the image of myself that i have within”: Exploring VR group therapy approaches with the LGBTQIA+ community. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616754'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Members of the LGBTQIA+ community are more likely to face mental health challenges. However, stigma and the fear of being outed often prevent them from seeking professional support. To address this, we collaborated with mental health professionals and LGBTQIA+ communities in Japan to develop a multi-user Virtual Reality (VR) platform that facilitates access to group therapy sessions. The system allows users to participate using personalized avatars and customized voices, preserving anonymity while enabling them to present themselves as they wish. We conducted a user study with 21 LGBTQIA+ participants and two qualified counselors to evaluate their experiences with VR-based therapy. Findings revealed that the created avatars enabled participants to express their chosen gender identity and increase confidence, acting as protective intermediaries. However, participants also noted how anonymity could affect trust, and suggested that better representation of body language and the introduction of trust-building activities could help compensate for such ambivalence. Overall, the platform fostered a strong sense of co-presence, and both counselors and LGBTQIA+ members felt that, with some ergonomic adjustment to improve the comfort of the headset during longer sessions, VR platforms could offer substantial opportunities for safe and representative access to mental health services.},
  archive      = {J_TVCG},
  author       = {Kinga Skierś and Danyang Peng and Anish Kundu and Tanner Person and Kenkichi Takase and Tamii Nagoshi and Sawako Nakayama and Yano Yuichiro and Tomoyuki Miyazaki and Kouta Minamizawa and Giulia Barbareschi},
  doi          = {10.1109/TVCG.2025.3616754},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {“I was truly able to express the image of myself that i have within”: Exploring VR group therapy approaches with the LGBTQIA+ community},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nanouniverse: Virtual instancing of structural detail and adaptive shell mapping. <em>TVCG</em>, 1-18. (<a href='https://doi.org/10.1109/TVCG.2025.3618914'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rendering huge biological scenes with atomistic detail presents a significant challenge in molecular visualization due to the memory limitations inherent in traditional rendering approaches. In this paper, we propose a novel method for the interactive rendering of massive molecular scenes based on hardware-accelerated ray tracing. Our approach circumvents GPU memory constraints by introducing virtual instantiation of full-detail scene elements. Using instancing significantly reduces memory consumption while preserving the full atomistic detail of scenes comprising trillions of atoms, with interactive rendering performance and completely free user exploration. We utilize coarse meshes as proxy geometries to approximate the overall shape of biological compartments, and access all atomistic detail dynamically during ray tracing. We do this via a novel adaptive technique utilizing a volumetric shell layer of prisms extruded around proxy geometry triangles, and a virtual volume grid for the interior of each compartment. Our algorithm scales to enormous molecular scenes with minimal memory consumption and the potential to accommodate even larger scenes. Our method also supports advanced effects such as clipping planes and animations. We demonstrate the efficiency and scalability of our approach by rendering tens of instances of Red Blood Cell and SARS-CoV-2 models theoretically containing more than 20 trillion atoms.},
  archive      = {J_TVCG},
  author       = {Ruwayda Alharbi and Ondřej Strnad and Markus Hadwiger and Ivan Viola},
  doi          = {10.1109/TVCG.2025.3618914},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Nanouniverse: Virtual instancing of structural detail and adaptive shell mapping},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EgoTrigger: Toward audio-driven image capture for human memory enhancement in all-day energy-efficient smart glasses. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616866'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {All-day smart glasses are likely to emerge as platforms capable of continuous contextual sensing, uniquely positioning them for unprecedented assistance in our daily lives. Integrating the multi-modal AI agents required for human memory enhancement while performing continuous sensing, however, presents a major energy efficiency challenge for all-day usage. Achieving this balance requires intelligent, context-aware sensor management. Our approach, EgoTrigger, leverages audio cues from the microphone to selectively activate power-intensive cameras, enabling efficient sensing while preserving substantial utility for human memory enhancement. EgoTrigger uses a lightweight audio model (YAMNet) and a custom classification head to trigger image capture from hand-object interaction (HOI) audio cues, such as the sound of a drawer opening or a medication bottle being opened. In addition to evaluating on the QA-Ego4D dataset, we introduce and evaluate on the Human Memory Enhancement Question-Answer (HME-QA) dataset. Our dataset contains 340 human-annotated first-person QA pairs from full-length Ego4D videos that were curated to ensure that they contained audio, focusing on HOI moments critical for contextual understanding and memory. Our results show EgoTrigger can use 54% fewer frames on average, significantly saving energy in both power-hungry sensing components (e.g., cameras) and downstream operations (e.g., wireless transmission), while achieving comparable performance on datasets for an episodic memory task. We believe this context-aware triggering strategy represents a promising direction for enabling energy-efficient, functional smart glasses capable of all-day use — supporting applications like helping users recall where they placed their keys or information about their routine activities (e.g., taking medications).},
  archive      = {J_TVCG},
  author       = {Akshay Paruchuri and Sinan Hersek and Lavisha Aggarwal and Qiao Yang and Xin Liu and Achin Kulshrestha and Andrea Colaco and Henry Fuchs and Ishan Chatterjee},
  doi          = {10.1109/TVCG.2025.3616866},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EgoTrigger: Toward audio-driven image capture for human memory enhancement in all-day energy-efficient smart glasses},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How do we read texts in VR?: The effects of text segment quantity and social distraction on text readability in virtual museum contexts. <em>TVCG</em>, 1-10. (<a href='https://doi.org/10.1109/TVCG.2025.3616803'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual museums are increasingly used to present cultural and educational content, often relying on textual descriptions to convey essential information. However, when users interact with text objects in virtual environments, the optimal text segment quantity for readability and comprehension remains unclear, especially when social distractions such as other visitors are present. This study investigated the effects of text segment quantity and the presence of other visitors on text readability and comprehension in the context of virtual museums. Participants read exhibit descriptions under four text-segment length conditions (1, 2, 4, and 8 lines) with or without simulated visitor agents. The results indicated that readability and comprehension were maximized when text was presented in intermediate segmentation lengths (2 and 4 Lines), while both excessively short (1 line) and long (8 lines) text segments hindered reading performance. Additionally, a significant interaction between text length and the presence of visitors was observed. Specifically, the presence of visitors led to increased comprehension task completion times only in the intermediate segmentation conditions, suggesting that social presence imposes an additional cognitive demand as social distractions in optimal text segment conditions. These findings provide empirical guidelines for designing effective text-based information systems in virtual museums, optimizing both user engagement and learning outcomes in immersive cultural environments.},
  archive      = {J_TVCG},
  author       = {Jungmin Lee and Hyuckjin Jang and Jeongmi Lee},
  doi          = {10.1109/TVCG.2025.3616803},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {How do we read texts in VR?: The effects of text segment quantity and social distraction on text readability in virtual museum contexts},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Casual-VRAuth: A design framework bridging focused and casual interactions for behavioral authentication in virtual reality. <em>TVCG</em>, 1-11. (<a href='https://doi.org/10.1109/TVCG.2025.3616834'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current behavioral authentication systems in Virtual Reality (VR) require sustained focused interaction during task execution - an assumption frequently incompatible with real-world constraints across two factors: (1) physical limitations (e.g., restricted hand/eye mobility), and (2) psychological barriers (e.g., task-switching fatigue or break-in-presence). To address this attentional gap, we propose a design framework bridging focused and casual interactions in behavior-based VR authentication (Casual-VRAuth). Based on this framework, we designed an authentication prototype using a modified ball-and-tunnel task (propelling a ball along a circular path), supporting three interaction modes: baseline Touch, and two eyes-free options (Hover/Tapping). Experimental results demonstrate that our framework effectively guides the design of authentication systems with varying interaction engagement levels (Touch >Hover >Tapping) to accommodate scenarios requiring casual interaction (e.g., multitasking or eyes-free operation). Furthermore, we revealed that reducing interaction engagement enhances resistance to mimicry attacks while decreasing cognitive workload and error rates in multitasking or eyes-free environments. However, this approach compromises the average classification accuracy of Interaction behavior under different algorithms (including InceptionTime, FCN, ResNet, CNN, MLP, and MCDCNN). Notably, moderate reduction of interaction engagement enhances authentication speed, while excessive reduction may conversely slow it down. Overall, our work establishes a novel design paradigm for VR authentication that supports casual interactions and offers valuable insights into balancing usability and security.},
  archive      = {J_TVCG},
  author       = {GuanYu Ye and BoYu Gao and Huawei Tu},
  doi          = {10.1109/TVCG.2025.3616834},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Casual-VRAuth: A design framework bridging focused and casual interactions for behavioral authentication in virtual reality},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SERES: Semantic-aware neural reconstruction from sparse views. <em>TVCG</em>, 1-12. (<a href='https://doi.org/10.1109/TVCG.2025.3619144'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a semantic-aware neural reconstruction method to generate 3D high-fidelity models from sparse images. To tackle the challenge of severe radiance ambiguity caused by mismatched features in sparse input, we enrich neural implicit representations by adding patch-based semantic logits that are optimized together with the signed distance field and the radiance field. A novel regularization based on the geometric primitive masks is introduced to mitigate shape ambiguity. The performance of our approach has been verified in experimental evaluation. The average chamfer distances of our reconstruction on the DTU dataset can be reduced by 44% for SparseNeuS and 20% for VolRecon. When working as a plugin for those dense reconstruction baselines such as NeuS and Neuralangelo, the average error on the DTU dataset can be reduced by 69% and 68% respectively.},
  archive      = {J_TVCG},
  author       = {Bo Xu and Yuhu Guo and Yuchao Wang and Wenting Wang and Yeung Yam and Charlie C.L. Wang and Xinyi Le},
  doi          = {10.1109/TVCG.2025.3619144},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SERES: Semantic-aware neural reconstruction from sparse views},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
