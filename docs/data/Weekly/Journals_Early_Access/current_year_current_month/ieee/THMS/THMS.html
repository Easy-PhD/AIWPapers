<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms">THMS - 7</h2>
<ul>
<li><details>
<summary>
(2025). Balancing exploration and cybersickness: Investigating curiosity-driven behavior in virtual environments. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3602125'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality offers the opportunity for immersive exploration, yet it is often undermined by cybersickness. However, how individuals strike a balance between exploration and discomfort remains unclear. Existing method (e.g., reinforcement learning (RL)) often fail to fully capture the complexities of navigation and decision-making patterns. This study investigates how curiosity influences users’ navigation behavior, particularly how users strike a balance between exploration and discomfort. We propose curiosity as a key factor driving irrational decision-making and apply the free energy principle to model the relationship between curiosity and user behavior quantitatively. Our findings indicate that users generally adopt conservative strategies when navigating. Also, curiosity levels tend to rise when the virtual environment changes. These results illustrate the dynamic interplay between exploration and discomfort. In addition, it offers a new perspective on how curiosity drives behavior in immersive environments, providing a foundation for designing adaptive VR environments. Future research will further refine this model by incorporating additional psychological and environmental factors to improve prediction accuracy.},
  archive      = {J_THMS},
  author       = {Tangyao Li and Yuyang Wang},
  doi          = {10.1109/THMS.2025.3602125},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Balancing exploration and cybersickness: Investigating curiosity-driven behavior in virtual environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SHA-SCP: A UI element spatial hierarchy aware smartphone user click behavior prediction method. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3601578'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting user click behavior and making relevant recommendations based on the user’s historical click behavior are critical to simplifying operations and improving user experience. Modeling User Interface (UI) elements is essential to user click behavior prediction, while the complexity and variety of the UI make it difficult to adequately capture the information of different scales. In addition, the lack of relevant datasets also presents difficulties for such studies. In response to these challenges, we construct a fine-grained smartphone usage behavior dataset containing 3 664 325 clicks of 100 users and propose a UI element Spatial Hierarchy Aware Smartphone user Click behavior Prediction method (SHA-SCP). SHA-SCP builds element groups by clustering the elements according to their spatial positions and uses attention mechanisms to perceive the UI at the element level and the element group level to fully capture the information of different scales. Experiments are conducted on the fine-grained smartphone usage behavior dataset, and the results show that our method outperforms the best baseline by an average of 18.35$\%$, 13.86$\%$, and 11.97$\%$ in Top-1 Accuracy, Top-3 Accuracy, and Top-5 Accuracy, respectively.},
  archive      = {J_THMS},
  author       = {Ling Chen and Qian Chen and Yiyi Peng and Kai Qian and Hongyu Shi and Xiaofan Zhang},
  doi          = {10.1109/THMS.2025.3601578},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {SHA-SCP: A UI element spatial hierarchy aware smartphone user click behavior prediction method},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatiotemporal view-reset deep learning with attentional GRUs for skeleton-based human action recognition. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3601386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based human action recognition has attracted significant attention. However, the skeleton spatial invariance and temporal context modeling are recent challenges for most existing methods. This work proposes a view-reset network, which integrates two important branches: spatial view-reset module (SVRM) and temporal attention module (TAM). In the SVRM, the skeleton model from different perspectives is reset in a unified coordinate system, eliminating the influence of viewpoint changes. In the TAM, the perception of temporal features is jointly enhanced by weighting each frame’s importance in the context. Furthermore, the pretrained residual network (ResNet) is used for prediction. The sample size is increased through data augmentation to improve the robustness of the model. The SVRM, TAM, and ResNet form an end-to-end learning network. The ablation study proved that the model could record the key skeletons and frames in the sequence and then reset the human body to a new position, making it easy for learning. The proposed model is evaluated on four challenging benchmarks based on the performance of the cross-view evaluation metrics. Experiments prove that the proposed model has superior performance and surpasses many state-of-the-art algorithms, with an increase of 1.91% over the top ten on the NTU RGB+D 60 dataset.},
  archive      = {J_THMS},
  author       = {Tianyu Ma and Xuna Wang and Hongwei Gao and Zide Liu and Jiahui Yu and Zhaojie Ju},
  doi          = {10.1109/THMS.2025.3601386},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Spatiotemporal view-reset deep learning with attentional GRUs for skeleton-based human action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neuroergonomics in digital operating rooms: Applying the two-competitor model of attention to the surgical context. <em>THMS</em>, 1-12. (<a href='https://doi.org/10.1109/THMS.2025.3601222'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to high workload and often excessive working hours, team members in operating rooms perform surgical procedures under difficult conditions and often without sufficient breaks. Despite this, the team must deal with incomplete information and unexpected distractions. This requires a suitable level of attention and the ability to balance the demands of the task with available cognitive resources. Advances in measurement technology and data analysis in neurotechnology open up new possibilities for the assessment of attention processes. Increasingly complex and demanding surgeries, especially, could benefit from the application of neuroergonomic automated assistants to minimize distraction, stress, and fatigue, and to facilitate interactions between team members. Such assistants could improve performance via monitoring of cognitive and affective states as well as the implementation of suitable interventions strategies. Understanding the impact of distractions on performance, enhancing individuals’ resilience to distractions, and potentially employing artificial assistants to mitigate their effects are critical future goals. In order to support such future developments, a standard taxonomy of attention in the operating theater is needed, as is a broader consensus regarding the nature of distraction. Ideally, such a model would serve as a basis for comparison between studies conducted in different laboratories, and in principle could also be used to bridge the gap between the laboratory and the real scenario. Here, we propose the adoption of a model of attention previously shown to be effective for modeling levels of attention in immersion and describe its application in the surgical context.},
  archive      = {J_THMS},
  author       = {David Thinnes and Alexander L. Francis and Volkan Sayman and Daniel Guagnin and Matthias W. Laschke and Michael D. Menger and Jonas Roller and Daniel J. Strauss},
  doi          = {10.1109/THMS.2025.3601222},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Neuroergonomics in digital operating rooms: Applying the two-competitor model of attention to the surgical context},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EEG neurofeedback-based gait motor imagery training in lokomat enhances motor rhythms in complete spinal cord injury. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3603548'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic interventions combining neurofeedback (NFB) and motor imagery (MI) are emerging strategies to promote cortical reorganization and functional training in individuals with complete spinal cord injury (SCI). This study proposes an electroencephalogram-based NFB approach for MI training, designed to teach the MI-related brain rhythmics modulation in Lokomat. For the purposes of this study, NFB is defined as a visual feedback training scheme. The proposed system introduces a formulation to minimize the default cortical effects that Lokomat produces on the individual’s activity during passive walking. Two individuals with complete SCI tested the proposed NFB system, in order to relearn the modulation of Mu ($\mu$ : 8–12 Hz) and Beta ($\beta$ : 13–30 Hz) rhythms over Cz, while receiving gait training with full weight support across 12 sessions. Each session consisted of the following three stages: 1) 2 min walking without MI (baseline); 2) 5 min walking with MI and True NFB; and 3) 5 min walking with MI and Sham NFB. The latter two stages were randomized session-by-session. The findings suggest that the proposed NFB approach may promote cortical reorganization and support the restoration of sensorimotor functions. Significant differences were observed between cortical patterns during True NFB and Sham NFB, particularly in the last intervention sessions. These results confirm the positive impact of the NFB system on gait motor training by enabling individuals with complete SCI to learn how to modulate their motor rhythms in specific cortical areas.},
  archive      = {J_THMS},
  author       = {Ericka R. da Silva Serafini and Cristian D. Guerrero-Mendez and Douglas M. Dunga and Teodiano F. Bastos-Filho and Anibal Cotrina Atencio and André F. O. de Azevedo Dantas and Caroline C. do Espírito Santo and Denis Delisle-Rodriguez},
  doi          = {10.1109/THMS.2025.3603548},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {EEG neurofeedback-based gait motor imagery training in lokomat enhances motor rhythms in complete spinal cord injury},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Ergodic imitation with corrections: Learning from implicit information in human feedback. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3603434'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the prevalence of collaborative robots increases, physical interactions between humans and robots are inevitable—presenting an opportunity for robots to not only maintain safe working parameters with humans but also learn from these interactions. To develop adaptive robots, we first aim to analyze human responses to different errors through a study in which users are asked to correct any errors that the robot makes in various tasks. With this characterization of corrections, we can treat physical human–robot interactions as informative instead of ignoring physical interactions or leaving robots to return to the originally planned behaviors when interactions end. We incorporate physical corrections into existing learning from demonstration (LfD) frameworks, which allow robots to learn new skills by observing human demonstrations. We demonstrate that learning from physical interactions can improve task-specific performance metrics. The results reveal that including information about the behavior being corrected in the update improves task performance significantly compared to adding corrected trajectories alone. In a user study with an optimal control-based LfD framework, we also find that users are able to provide less feedback to the robot after each interaction update to the robot’s behavior. Utilizing corrections could enable advanced LfD techniques to be integrated into commercial applications for collaborative robots by enabling end-users to customize the robot’s behavior through intuitive interactions rather than by modifying the behavior in software.},
  archive      = {J_THMS},
  author       = {Junru Pang and Quentin Anderson-Watson and Kathleen Fitzsimons},
  doi          = {10.1109/THMS.2025.3603434},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Ergodic imitation with corrections: Learning from implicit information in human feedback},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bimanual manipulation of steady-hand eye robots with adaptive sclera force control: Cooperative versus teleoperation strategies. <em>THMS</em>, 1-11. (<a href='https://doi.org/10.1109/THMS.2025.3605011'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performing retinal vein cannulation (RVC) as a potential treatment for retinal vein occlusion without the assistance of a surgical robotic system is very challenging to do safely. The main limitation is the physiological hand tremor of surgeons. Robot-assisted eye surgery technology may resolve the problems of hand tremors and fatigue and improve the safety and precision of RVC. The steady-hand eye robot (SHER) is an admittance-based robotic system that can filter out hand tremors and enables ophthalmologists to manipulate a surgical instrument inside the eye cooperatively. However, the admittance-based cooperative control mode does not safely minimize the contact force between the surgical instrument and the sclera to prevent tissue damage. In addition, features such as haptic feedback or hand motion scaling, which can improve the safety and precision of surgery, require a teleoperation control framework. This work presents, for the first time in the field of robot-assisted retinal microsurgery research, a registration-free bimanual adaptive teleoperation (BMAT) control framework using SHER 2.0 and SHER 2.1 robotic systems. Both SHERs are integrated with an adaptive force control algorithm that dynamically and automatically minimizes the tool–sclera interaction forces, enforcing them within a safe limit. The scleral forces are measured using two fiber Bragg grating-based force-sensing tools. The performance of the proposed BMAT control framework is evaluated by comparison with a bimanual adaptive cooperative framework in a vessel-following experiment conducted under a surgical microscope. Experimental results demonstrate the effectiveness of the BMAT control framework in performing a safe bimanual telemanipulation of the eye without overstretching it, even in the absence of registration between the two robots.},
  archive      = {J_THMS},
  author       = {Mojtaba Esfandiari and Peter Gehlbach and Russell H. Taylor and Iulian I. Iordachita},
  doi          = {10.1109/THMS.2025.3605011},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {9},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Bimanual manipulation of steady-hand eye robots with adaptive sclera force control: Cooperative versus teleoperation strategies},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
