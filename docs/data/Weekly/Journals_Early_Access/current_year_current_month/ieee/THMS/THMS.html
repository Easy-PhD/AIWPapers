<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms">THMS - 3</h2>
<ul>
<li><details>
<summary>
(2025). ST-GCN-AltFormer: Gesture recognition with spatial-temporal alternating transformer. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3607961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In skeleton-based gesture recognition tasks, existing approaches based on graph convolutional networks (GCNs) struggle to capture the synergistic actions of nonadjacent graph nodes and the information conveyed by their long-range dependencies. Combining spatial and temporal transformers is a promising solution to address the limitation, inspired by the advantage of transformer in assessing nonadjacent long-range dependencies, but there lacks an effective strategy to integrate the spatial and temporal information extracted by these transformers. Therefore, this article proposes the spatial-temporal alternating graph convolution transformer (ST-GCN-AltFormer), which connects the spatial-temporal graph convolutional network (ST-GCN) with the spatial-temporal alternating transformer (AltFormer) architecture. In the AltFormer architecture, the spatial-temporal transformer branch employs a spatial transformer to capture information from specific frames, and uses a temporal transformer to analyze its evolution over the entire temporal range. Meanwhile, the temporal-spatial transformer branch extracts temporal information from specific nodes using a temporal transformer, and integrates it with a spatial transformer. The fusion enhances accurate spatial-temporal information extraction. Our method achieves superior performance compared to state-of-the-art methods, achieving accuracies of 97.5%, 95.8%, 94.3%, 92.8%, and 98.31% on the large-scale 3D hand gesture recognition (SHREC’17 Track), Dynamic Hand Gesture 14-28 (DHG-14/28), and leap motion dynamic hand gesture (LMDHG) dynamic gesture datasets, respectively.},
  archive      = {J_THMS},
  author       = {Qing Pan and Jintao Zhu and Lingwei Zhang and Gangmin Ning and Luping Fang},
  doi          = {10.1109/THMS.2025.3607961},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {ST-GCN-AltFormer: Gesture recognition with spatial-temporal alternating transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Understanding and predicting temporal visual attention influenced by dynamic highlights in monitoring task. <em>THMS</em>, 1-11. (<a href='https://doi.org/10.1109/THMS.2025.3614364'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monitoring interfaces are crucial for dynamic, high-stakes tasks where effective user attention is essential. Visual highlights can guide attention effectively, but may also introduce unintended disruptions. To investigate this, we examined how visual highlights affect users’ gaze behavior in a drone monitoring task, focusing on when, how long, and how much attention they draw. We found that highlighted areas exhibit distinct temporal characteristics compared to nonhighlighted ones, quantified using normalized saliency (NS) metrics. We found that highlights elicited immediate responses, with NS peaking quickly, but this shift came at the cost of reduced search efforts elsewhere, potentially impacting situational awareness. To predict these dynamic changes and support interface design, we developed the Highlight-Informed Saliency Model, which provides granular predictions of NS over time. These predictions enable evaluations of highlight effectiveness and inform the optimal timing and deployment of highlights in future monitoring interface designs, particularly for time-sensitive tasks.},
  archive      = {J_THMS},
  author       = {Zekun Wu and Anna Maria Feit},
  doi          = {10.1109/THMS.2025.3614364},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Understanding and predicting temporal visual attention influenced by dynamic highlights in monitoring task},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning model with fine-tuning for generalized few-shot activity recognition. <em>THMS</em>, 1-13. (<a href='https://doi.org/10.1109/THMS.2025.3613773'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem we focused on in this article is sensor-based generalized few-shot activity recognition. In this problem, each of the predefined activity classes (i.e., base classes) has substantial training instances, while each of the new activity classes (i.e., novel classes) just has a few training instances. Both the base and the novel classes need to be recognized. Currently, just a few works focus on this problem, and no formal statement of the problem is provided. In this article, we provide a formal definition of the problem, and propose a method to address it. In the proposed method, adopting the strategy of fine-tuning deep learning models, a deep learning model is first learned with the base-class training instances, and then fine-tuned with resampled training instances from both the base and the novel classes. We evaluate our method with three publicly available datasets on 1-shot, 5-shot, and 10-shot learning tasks. The results on the evaluation metric of harmonic mean of the average per-class accuracy for the base classes and that for the novel classes show that, our method could outperform state-of-the-art methods. In addition, the time and resource cost of our method is moderate.},
  archive      = {J_THMS},
  author       = {Wei Wang and Qingzhong Li},
  doi          = {10.1109/THMS.2025.3613773},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {Deep learning model with fine-tuning for generalized few-shot activity recognition},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
