<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>THMS</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="thms">THMS - 1</h2>
<ul>
<li><details>
<summary>
(2025). ST-GCN-AltFormer: Gesture recognition with spatial-temporal alternating transformer. <em>THMS</em>, 1-10. (<a href='https://doi.org/10.1109/THMS.2025.3607961'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In skeleton-based gesture recognition tasks, existing approaches based on graph convolutional networks (GCNs) struggle to capture the synergistic actions of nonadjacent graph nodes and the information conveyed by their long-range dependencies. Combining spatial and temporal transformers is a promising solution to address the limitation, inspired by the advantage of transformer in assessing nonadjacent long-range dependencies, but there lacks an effective strategy to integrate the spatial and temporal information extracted by these transformers. Therefore, this article proposes the spatial-temporal alternating graph convolution transformer (ST-GCN-AltFormer), which connects the spatial-temporal graph convolutional network (ST-GCN) with the spatial-temporal alternating transformer (AltFormer) architecture. In the AltFormer architecture, the spatial-temporal transformer branch employs a spatial transformer to capture information from specific frames, and uses a temporal transformer to analyze its evolution over the entire temporal range. Meanwhile, the temporal-spatial transformer branch extracts temporal information from specific nodes using a temporal transformer, and integrates it with a spatial transformer. The fusion enhances accurate spatial-temporal information extraction. Our method achieves superior performance compared to state-of-the-art methods, achieving accuracies of 97.5%, 95.8%, 94.3%, 92.8%, and 98.31% on the large-scale 3D hand gesture recognition (SHRECâ€™17 Track), Dynamic Hand Gesture 14-28 (DHG-14/28), and leap motion dynamic hand gesture (LMDHG) dynamic gesture datasets, respectively.},
  archive      = {J_THMS},
  author       = {Qing Pan and Jintao Zhu and Lingwei Zhang and Gangmin Ning and Luping Fang},
  doi          = {10.1109/THMS.2025.3607961},
  journal      = {IEEE Transactions on Human-Machine Systems},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Human-Mach. Syst.},
  title        = {ST-GCN-AltFormer: Gesture recognition with spatial-temporal alternating transformer},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
