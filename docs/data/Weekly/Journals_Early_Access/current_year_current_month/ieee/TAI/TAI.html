<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAI</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tai">TAI - 6</h2>
<ul>
<li><details>
<summary>
(2025). Ent-BAM: A generative framework for biomedical argument mining enriched with entity information. <em>TAI</em>, 1-10. (<a href='https://doi.org/10.1109/TAI.2025.3616076'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomedical Argument Mining (BAM) aims to identify and extract argumentative structures, specifically, Argumentative Components (ACs) and Argumentative Relations (ARs), within biomedical texts. Primary challenges in BAM arise from the rich presence of specialized terms and a diverse range of entities. These entities often occur within ACs and co-occur in ARs, providing valuable cues for identifying ACs and ARs. Most of the existing approaches do not leverage these cues. This paper presents Ent-BAM, a BAM framework that incorporates entity (co-)occurrence information in a text-to-text generation paradigm. Plain text serves as the input, and the output is formatted in Augmented Natural Language (ANL). First, we employ a prompt-based strategy using a large language model (LLM) to extract (co-)occurred entities from the standard BAM dataset. Then, we construct the ANL output by embedding AC and AR labels, along with those extracted entities, within a single sequence. Ent-BAM significantly outperforms existing state-of-the-art (SoTA) methods by an average task margin of up to 4.86% Micro F1 Score, highlighting the strong potential of our approach.},
  archive      = {J_TAI},
  author       = {Nilmadhab Das and Yash Sunil Choudhary and V. Vijaya Saradhi and Ashish Anand},
  doi          = {10.1109/TAI.2025.3616076},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Ent-BAM: A generative framework for biomedical argument mining enriched with entity information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Securing IoT: Unveiling attacks with multiview-multitask learning. <em>TAI</em>, 1-13. (<a href='https://doi.org/10.1109/TAI.2025.3615565'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid expansion and use of day-to-day IoT applications, cybercriminals are exploiting an increasingly wide attack surface and are capable of executing many successful attacks, leading to significant losses for individuals and organizations. The existing defense mechanisms often rely on single-view, single-task models that utilize a single feature set to perform one specific task. However, modern IoT systems are multifaceted, heterogeneous, and resource-constrained, posing considerable challenges for developing unified and scalable defense solutions. To address this, we propose a novel hybrid model called M2VT that integrates Multi-View Learning (MVL) and Multi-Task Learning (MTL) for effective cyber-attack defense. The model simultaneously processes three distinct subsets of relevant features (views) to perform three interrelated tasks: attack detection, attack category classification, and attack type classification. The model leverages AutoEncoder (AE) and Long Short-Term Memory (LSTM) networks to extract task-specific spatial and temporal features, enhancing both efficiency and cost-effectiveness. The M2VT model is evaluated using four publicly available IoT benchmark datasets and one testbed dataset. Across all tasks and datasets, the model achieves over 96% accuracy, consistently outperforming state-of-the-art approaches. The parallel execution of MVL and MTL, combined with taskspecific feature subsets, significantly boosts performance. The implementation of M2VT is publicly available in our code repository 1.},
  archive      = {J_TAI},
  author       = {Urkhimbam Boby Clinton and Nazrul Hoque and Shahid Raza and Monowar Bhuyan},
  doi          = {10.1109/TAI.2025.3615565},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Securing IoT: Unveiling attacks with multiview-multitask learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HEAL: Brain-inspired hyperdimensional efficient active learning. <em>TAI</em>, 1-15. (<a href='https://doi.org/10.1109/TAI.2025.3617929'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drawing inspiration from the outstanding learning capability of our human brains, Hyperdimensional Computing (HDC) emerges as a novel computing paradigm, and it leverages high-dimensional vector representation and operations for brain-like lightweight Machine Learning (ML). Practical deployments of HDC have significantly enhanced the learning efficiency compared to current deep ML methods on a broad spectrum of applications. However, boosting the data efficiency of HDC classifiers in supervised learning remains an open question. In this paper, we introduce Hyperdimensional Efficient Active Learning (HEAL), a novel Active Learning (AL) framework tailored for HDC classification. HEAL proactively annotates unlabeled data points via uncertainty and diversity-guided acquisition, leading to a more efficient dataset annotation and lowering labor costs. Unlike conventional AL methods that only support classifiers built upon deep neural networks (DNN), HEAL operates without the need for gradient or probabilistic computations. This allows it to be effortlessly integrated with any existing HDC classifier architecture. The key design of HEAL is a novel approach for uncertainty estimation in HDC classifiers through a lightweight HDC ensemble with prior hypervectors. Additionally, by exploiting hypervectors as prototypes (i.e., compact representations), we develop a sample acquisition strategy for HEAL to select diverse samples within each batch for annotation. Our evaluation shows that HEAL surpasses a diverse set of baselines in AL quality and achieves notably faster acquisition than many existing state-of-the-art AL methods, recording 12× to 45,700× speedup in acquisition runtime per batch.},
  archive      = {J_TAI},
  author       = {Yang Ni and Zhuowen Zou and Wenjun Huang and Hanning Chen and William Youngwoo Chung and Samuel Cho and Ranganath Krishnan and Pietro Mercati and Mohsen Imani},
  doi          = {10.1109/TAI.2025.3617929},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {HEAL: Brain-inspired hyperdimensional efficient active learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QuanCrypt-FL: Quantized homomorphic encryption with pruning for secure federated learning. <em>TAI</em>, 1-16. (<a href='https://doi.org/10.1109/TAI.2025.3612906'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning has emerged as a leading approach for decentralized machine learning, enabling multiple clients to collaboratively train a shared model without exchanging private data. While federated learning enhances data privacy, it remains vulnerable to inference attacks, such as gradient inversion and membership inference, during both the training and inference phases. Specifically, QuanCrypt-FL is designed to defend against adversaries aiming to reconstruct sensitive client data through gradient leakage or membership inference in FL settings. Homomorphic Encryption provides a promising solution by encrypting model updates to protect against such attacks, but it introduces substantial communication overhead, slowing down training and increasing computational costs. To address these challenges, we propose QuanCrypt-FL, a novel algorithm that integrates fully homomorphic encryption with low-bit quantization and pruning techniques to enhance security against inference attacks while significantly reducing computational overhead, making federated learning more efficient and resilient. Further, we propose and implement mean-based clipping to mitigate quantization overflow or errors. By integrating these methods, QuanCrypt-FL creates a communication-efficient FL framework that ensures privacy protection with minimal impact on model accuracy, thereby improving both computational efficiency and attack resilience. We validate our approach on MNIST, HAM10000, CIFAR-10, and CIFAR-100 datasets, demonstrating superior performance compared to state-of-the-art methods. QuanCrypt-FL consistently outperforms existing method and matches Vanilla-FL in terms of accuracy across varying clients. Further, QuanCrypt-FL achieves up to 9× faster encryption, 16× faster decryption, and 1.5× faster inference compared to existing method, with training time reduced by up to 3×.},
  archive      = {J_TAI},
  author       = {Md Jueal Mia and M Hadi Amini},
  doi          = {10.1109/TAI.2025.3612906},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {QuanCrypt-FL: Quantized homomorphic encryption with pruning for secure federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DDD-GenDT: Dynamic data-driven generative digital twin framework. <em>TAI</em>, 1-15. (<a href='https://doi.org/10.1109/TAI.2025.3612920'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital twin (DT) technology enables real-time simulation, prediction, and optimization of physical systems, but its practical deployment often faces challenges related to high data requirements, proprietary data constraints, and limited adaptability to evolving system conditions. This work introduces DDD-GenDT, a dynamic data-driven generative digital twin framework grounded in the Dynamic Data-Driven Application Systems (DDDAS) paradigm. The proposed architecture comprises of the Physical Twin Observation Graph (PTOG) for representing operational states of the physical twin (PT), an Observation Window Extraction process for capturing relevant temporal state sequences, a Data Preprocessing Pipeline within an LLM-Based Behavior Prediction Engine for sensor data structuring and filtering, and an LLM ensemble that performs zero-shot predictive inference. By leveraging generative artificial intelligence, DDD-GenDT reduces the need for extensive historical datasets, enabling DT construction in data-scarce environments while maintaining privacy for proprietary industrial processes. The DDDAS-driven feedback mechanism allows the DT to autonomically adapt its predictive behavior in alignment with PT-specific wear and degradation patterns, thereby supporting DT-aging, which is the progressive synchronization of the DT with the evolving physical system. The proposed framework is validated using the NASA CNC milling dataset, with spindle motor current as the monitored variable. In a zero-shot prediction setting, the GPT-4-based DT achieves an average RMSE of 0.479 A (4.79% of the maximum 10 A spindle current), accurately modeling both nonlinear process dynamics and changes arising from PT aging without retraining. These results demonstrate that DDD-GenDT provides a generalizable, data-efficient, and adaptive DT modeling approach, bridging the generative AI capabilities with the performance and reliability requirements of industrial DT applications.},
  archive      = {J_TAI},
  author       = {Yu-Zheng Lin and Qinxuan Shi and Zhanglong Yang and Banafsheh Saber Latibari and Shalaka Satam and Sicong Shao and Soheil Salehi and Pratik Satam},
  doi          = {10.1109/TAI.2025.3612920},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {DDD-GenDT: Dynamic data-driven generative digital twin framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Physical reservoir computing for optical stethoscope-based heart sound biometric identification. <em>TAI</em>, 1-14. (<a href='https://doi.org/10.1109/TAI.2025.3617382'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart sound signal has emerged as a promising solution to biometric identification. In this paper, we use an optical flow algorithm to retrieve optical stethoscope-based heart sound signals from a laser-camera system. We apply physical reservoir computing (RC) for the classification algorithm. As a bio-inspired algorithm, physical RC has attracted growing research interests in recent years. We aim to create an efficient identification system by applying a recently proposed physical RC model called rotating neuron reservoir (RNR) as the processing core. Unlike conventional machine learning classifiers, RNR is a hardware-based neuromorphic model that preserves the majority of computing in the analogue domain, holding the promise of a next-generation machine learning accelerator. At the same time, the RNR, as a recurrent neural network (RNN), is suitable for time series data processing. The proposed system is verified by an experimentally collected heart sound dataset by laser-camera system achieving an overall accuracy of 89.03% in identifying twelve testing subjects. Additionally, the elevated heart sound from 8 subjects have been blended with their normal heart sounds to assess the robustness of the proposed system. The classification accuracy reaches over 83% in this mixed test. Furthermore, the identification system was assessed under individuals with different types of heart murmurs and abnormal heart sounds, achieving an overall accuracy of around 90%. The successful demonstration promises a novel application of physical RC for future biometric identification.},
  archive      = {J_TAI},
  author       = {Yuqi Ding and Haobo Li and Xiangpeng Liang and Marija Vaskeviciute and Daniele Faccio and Hadi Heidari},
  doi          = {10.1109/TAI.2025.3617382},
  journal      = {IEEE Transactions on Artificial Intelligence},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Artif. Intell.},
  title        = {Physical reservoir computing for optical stethoscope-based heart sound biometric identification},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
