<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TKDE</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tkde">TKDE - 6</h2>
<ul>
<li><details>
<summary>
(2025). Approximately unimodal likelihood models for ordinal regression. <em>TKDE</em>, 1-11. (<a href='https://doi.org/10.1109/TKDE.2025.3617386'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ordinal regression (OR, also called ordinal classification) is classification of ordinal data, in which the underlying target variable is categorical and considered to have a natural ordinal relation for the underlying explanatory variable. A key to successful OR models is to find a data structure ‘natural ordinal relation’ common to many ordinal data and reflect that structure into the design of those models. A recent OR study found that many real-world ordinal data show a tendency that the conditional probability distribution (CPD) of the target variable given a value of the explanatory variable will often be unimodal. Several previous studies thus developed unimodal likelihood models, in which a predicted CPD is guaranteed to become unimodal. However, it was also observed experimentally that many real-world ordinal data partly have values of the explanatory variable where the underlying CPD will be non-unimodal, and hence unimodal likelihood models may suffer from a bias for such a CPD. Therefore, motivated to mitigate such a bias, we propose approximately unimodal likelihood models, which can represent up to a unimodal CPD and a CPD that is close to be unimodal. We also verify experimentally that a proposed model can be effective for statistical modeling of ordinal data and OR tasks.},
  archive      = {J_TKDE},
  author       = {Ryoya Yamasaki},
  doi          = {10.1109/TKDE.2025.3617386},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Approximately unimodal likelihood models for ordinal regression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ASSM: Adaptive subject-focused modeling for multimodal summarization via semantic matching. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3610544'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Summarization aims to use multimodal data to generate accurate and concise summaries for long sentences. While previous work has achieved promising success, they have overlooked the mismatching among multimodal semantics and lacked subject information guidance for adaptive referential images. Motivated by this observation, we propose ASSM, an Adaptive Subject-focused modeling for multimodal summarization via Semantic Matching. The novelty of ASSM lies in two aspects. First, we propose a multimodal semantic matching module that projects multimodal inputs into a shared joint embedding semantic space to determine whether the semantics between multimodalities are mismatching. Second, we propose an adaptive subject-focused guide module, which adaptively references images to learn subject tokens based on the multimodal semantic matching results. With these subject tokens, we are able to focus on the subject information, providing precise guidance for summary generation. We conduct extensive experiments on two standard benchmarks and compare ASSM with 17 existing models. The experimental results regarding ROUGE, BERTScore, and MoverScore show that the proposed ASSM model outperforms all competitors, achieving state-of-the-art performance and suggesting the effectiveness of our proposal. In addition, we provide a case study to further demonstrate the usability of ASSM.},
  archive      = {J_TKDE},
  author       = {Xujian Zhao and Chuanpeng Deng and Peiquan Jin},
  doi          = {10.1109/TKDE.2025.3610544},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {ASSM: Adaptive subject-focused modeling for multimodal summarization via semantic matching},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A survey of large language models on generative graph analytics: Query, learning, and applications. <em>TKDE</em>, 1-20. (<a href='https://doi.org/10.1109/TKDE.2025.3609877'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graph is a fundamental data model to represent various entities and their complex relationships in society and nature, such as social networks, transportation networks, financial networks, and biomedical systems. Recently, large language models (LLMs) have showcased a strong generalization ability to handle various natural language processing tasks to answer users' arbitrary questions and generate specific-domain content. Compared with graph learning models, LLMs enjoy superior advantages in addressing the challenges of generalizing graph tasks by eliminating the need for training graph learning models and reducing the cost of manual annotation. However, LLMs are sequential models for textual data, but graphs are non-sequential topological data. It is challenging to adapt LLMs to tackle graph analytics tasks. In this survey, we conduct a comprehensive investigation of existing LLM studies on graph data, which summarizes the relevant graph analytics tasks solved by advanced LLM models and points out the existing challenges and future directions. Specifically, we study the key problems of LLM-based generative graph analytics (LLM-GGA) in terms of three categories: LLM-based graph query processing (LLM-GQP), LLM-based graph inference and learning (LLM-GIL), and graph-LLM-based applications. LLM-GQP focuses on an integration of graph analytics techniques and LLM prompts, including graph understanding and knowledge graphs and LLMs, while LLM-GIL focuses on learning and reasoning over graphs, including graph learning, graph-formed reasoning, and graph representation. We summarize the useful prompts incorporated into LLM to handle different graph downstream tasks. Moreover, we give a summary of LLM model evaluation, benchmark datasets/tasks, and a deep pro and cons analysis of the discussed LLM-GGA models. We also explore open problems and future directions in this exciting interdisciplinary research area of LLMs and graph analytics.},
  archive      = {J_TKDE},
  author       = {Wenbo Shang and Xin Huang},
  doi          = {10.1109/TKDE.2025.3609877},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A survey of large language models on generative graph analytics: Query, learning, and applications},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive hyper-box granulation with justifiable granularity for feature selection. <em>TKDE</em>, 1-15. (<a href='https://doi.org/10.1109/TKDE.2025.3617583'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering as a fundamental technique in data mining and machine learning, aims to partition data into meaningful groups based on the inherent relationships among data. However, traditional clustering algorithms typically assume convex hyperspherical geometry of data, where the clusters have clearly defined boundaries and do not overlap. In contrast, real-world data often exhibits complex and non-convex geometries, which makes these assumptions ineffective and lead to inaccurate clustering results that fail to capture the intrinsic structure. To address this challenge, the paper proposes a novel granular clustering based on an enhanced granularity representation, which further refines the principle of justifiable granularity. By introducing a more precise and flexible hyper-box granulation mechanism, the method dynamically adapts to the topology of data, thereby improving clustering accuracy. By defining the degree of aggregation and discreteness between data points, the importance of attributes in the feature space is quantified, leading to the design of a novel hyper-box feature selection (HBFS) algorithm. This algorithm integrates the granular clustering principle to optimize the feature selection process, reducing the impact of redundant features and noise, thus improving clustering efficiency and interpretability. To validate the superiority and effectiveness of the proposed method, extensive experiments were conducted on fifteen publicly available datasets, comparing the performance of HBFS algorithm with classical and state-of-art feature selection methods. The results and the statistical significance tests show that HBFS significantly outperforms existing feature selection methods across various evaluation metrics.},
  archive      = {J_TKDE},
  author       = {Wentao Li and Bowen Yang and Witold Pedrycz and Chao Zhang and Tao Zhan},
  doi          = {10.1109/TKDE.2025.3617583},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive hyper-box granulation with justifiable granularity for feature selection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph2Region: Efficient graph similarity learning with structure and scale restoration. <em>TKDE</em>, 1-13. (<a href='https://doi.org/10.1109/TKDE.2025.3617461'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph similarity is critical in graph-related tasks such as graph retrieval, where metrics like maximum common subgraph (MCS) and graph edit distance (GED) are commonly used. However, exact computations of these metrics are known to be NP-Hard. Recent neural network-based approaches approximate the similarity score in embedding spaces to alleviate the computational burden, but they either involve expensive pairwise node comparisons or fail to effectively utilize structural and scale information of graphs. To tackle these issues, we propose a novel geometric-based graph embedding method called Graph2Region (G2R). G2R represents nodes as closed regions and recovers their adjacency patterns within graphs in the embedding space. By incorporating the node features and adjacency patterns of graphs, G2R summarizes graph regions, i.e., graph embeddings, where the shape captures the underlying graph structures and the volume reflects the graph size. Consequently, the overlap between graph regions can serve as an approximation of MCS, signifying similar node regions and adjacency patterns. We further analyze the relationship between MCS and GED and propose using disjoint parts as a proxy for GED similarity. This analysis enables concurrent computation of MCS and GED, incorporating local and global structural information. Experimental evaluation highlights G2R's competitive performance in graph similarity computation. It achieves up to a 60.0% relative accuracy improvement over state-of-the-art methods in MCS similarity learning, while maintaining efficiency in both training and inference. Moreover, G2R showcases remarkable capability in predicting both MCS and GED similarities simultaneously, providing a holistic assessment of graph similarity.},
  archive      = {J_TKDE},
  author       = {Zhouyang Liu and Yixin Chen and Ning Liu and Jiezhong He and Dongsheng Li},
  doi          = {10.1109/TKDE.2025.3617461},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph2Region: Efficient graph similarity learning with structure and scale restoration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning accurate representation to nonstandard tensors via a mode-aware tucker network. <em>TKDE</em>, 1-14. (<a href='https://doi.org/10.1109/TKDE.2025.3617894'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A nonstandard tensor is frequently adopted to model a large-sale complex dynamic network. A Tensor Representation Learning (TRL) model enables extracting valuable knowledge form a dynamic network via learning low-dimensional representation of a target nonstandard tensor. Nevertheless, the representation learning ability of existing TRL models are limited for a nonstandard tensor due to its inability to accurately represent the specific nature of the nonstandard tensor, i.e., mode imbalance, high-dimension, and incompleteness. To address this issue, this study innovatively proposes a Mode-Aware Tucker Networkbased Tensor Representation Learning (MTN-TRL) model with three-fold ideas: a) designing a mode-aware Tucker network to accurately represent the imbalanced mode of a nonstandard tensor, b) building an MTN-based high-efficient TRL model that fuses both data density-oriented modeling principle and adaptive parameters learning scheme, and c) theoretically proving the MTN-TRL model's convergence. Extensive experiments on eight nonstandard tensors generating from real-world dynamic networks demonstrate that MTN-TRL significantly outperforms state-of-the-art models in terms of representation accuracy.},
  archive      = {J_TKDE},
  author       = {Hao Wu and Qu Wang and Xin Luo and Zidong Wang},
  doi          = {10.1109/TKDE.2025.3617894},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning accurate representation to nonstandard tensors via a mode-aware tucker network},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
