<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc">TAFFC - 19</h2>
<ul>
<li><details>
<summary>
(2025). A multi-scale feature refinement and dual-attention enhanced dynamic convolutional network for speech-based depression and ADHD assessment. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3604562'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the area of affective computing, speech has been identified as a promising biomarker for assessing depression and attention deficit hyperactivity disorder (ADHD). These disorders manifest as abnormalities in speech across various frequency bands and exhibit temporal variations. Most existing work on speech features relies on the magnitude spectrogram, which discards phase information and also does not consider the impact of different frequency bands on depression and ADHD detection. Inspired by these, we propose a novel multi-scale complex feature refinement and dynamic convolution attention-aware network to enhance speech-based assessment of depression and ADHD. Our approach incorporates three key components: multi-scale complex feature refinement (MSFR), dynamic convolutional neural network (Dy-CNN), and dual-attention feature enhancement (DAFE) module. The MSFR module utilizes depth-wise convolutional networks to process both magnitude and phase input, selectively emphasizing frequency bands associated with depression and ADHD. Importantly, the Dy-CNN module employs an attention mechanism to autonomously generate multiple convolution kernels that adapt to input features and capture relevant temporal dynamics linked to depression and ADHD. Additionally, the DAFE module enhances feature representation and detection performance by incorporating channel shuffle attention (CSA) and spatial axial attention (SAA) mechanisms, which leverage both inter- and intra-channel relationships and examine time-frequency characteristics of the feature map. Extensive experiments conducted on four publicly available datasets, i.e., AVEC2013, AVEC2014, E-DAIC, and a self-collected authentic ADHD dataset demonstrated that the proposed method outperforms previous approaches and exhibits superior generalization capabilities across different language settings (i.e., English, German) for speech-based depression and ADHD assessment.},
  archive      = {J_TAFFC},
  author       = {Shuanglin Li and Siyang Song and Syed Mohsen Naqvi},
  doi          = {10.1109/TAFFC.2025.3604562},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {A multi-scale feature refinement and dual-attention enhanced dynamic convolutional network for speech-based depression and ADHD assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypercomplex neural network and cross-modal attention for multi-modal emotion recognition using physiological signals. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3605133'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal emotion recognition plays a crucial role in human-computer interaction. Nowadays, many studies have developed fusion algorithms for this purpose. However, two challenges are still present, i.e., insufficient cross-modal information sharing and weak fusion feature representations. To this end, we develop a novel framework, namely CH-Net, for multi-modal emotion recognition with physiological signals. It is based on cross-modal attention and hypercomplex domain fusion. First, our learnable cross-modal attention mechanism adaptively aligns features across modalities, enhancing both complementarity and modality-specific discrepancies. Second, a hypercomplex fusion module encodes these features, yielding more robust representations while reducing parameter overhead. Two benchmark datasets, i.e., MAHNOB-HCI and DEAP, are utilized to train and test our model. Extensive experiments demonstrate that CH-Net is effective and outperforms state-of-the-art (SOTA) methods. Our code will be available at https://github.com/xuxusky/CH-Net.},
  archive      = {J_TAFFC},
  author       = {Xu Xu and Junxin Chen and Chong Fu and Zhihan Lyu},
  doi          = {10.1109/TAFFC.2025.3605133},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Hypercomplex neural network and cross-modal attention for multi-modal emotion recognition using physiological signals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SarcasmBench: Towards evaluating large language models on sarcasm understanding. <em>TAFFC</em>, 1-20. (<a href='https://doi.org/10.1109/TAFFC.2025.3604806'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of large language models (LLMs), tasks associated with “System I” cognition—those that are fast, automatic, and intuitive, such as sentiment analysis and text classification—are often considered effectively solved. However, sarcasm remains a persistent challenge. As a subtle and complex linguistic phenomenon, sarcasm frequently involves rhetorical devices such as hyperbole and figurative language to express implicit sentiments and intentions, demanding a higher level of abstraction and pragmatic reasoning than standard sentiment analysis. This raises concerns about whether current claims of LLM success extend robustly to the domain of sarcasm understanding. To systematically investigate this issue, we introduce a new high-quality multi-modal sarcasm detection dataset, termed AMSD, and construct a comprehensive evaluation benchmark, SarcasmBench. Our benchmark encompasses 16 state-of-the-art (SOTA) LLMs and 8 strong pretrained language models (PLMs), evaluated across six widely-used textual sarcasm datasets and three multi-modal sarcasm benchmarks. We adopt three popular prompting paradigms: zero-shot input/output (IO) prompting, few-shot IO prompting, and chain-of-thought (CoT) prompting. Our extensive experiments yield three key findings: (1) current LLMs underperform supervised PLMs based sarcasm detection baselines. This suggests that significant efforts are still required to improve LLMs' understanding of human sarcasm. (2) GPT-4 and Gemini 2.0 consistently and significantly outperforms other LLMs across various prompting methods. (3) Few-shot IO prompting method outperforms the other two methods: zero-shot IO and few-shot CoT. We hope this benchmark will serve as a valuable resource for the research community and inspire future work toward more robust and human-aligned sarcasm understanding.},
  archive      = {J_TAFFC},
  author       = {Yazhou Zhang and Chunwang Zou and Zheng Lian and Prayag Tiwari and Jing Qin},
  doi          = {10.1109/TAFFC.2025.3604806},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {SarcasmBench: Towards evaluating large language models on sarcasm understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PainFormer: A vision foundation model for automatic pain assessment. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3605475'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pain is a manifold condition that impacts a significant percentage of the population. Accurate and reliable pain evaluation for the people suffering is crucial to developing effective and advanced pain management protocols. Automatic pain assessment systems provide continuous monitoring and support decision-making processes, ultimately aiming to alleviate distress and prevent functionality decline. This study introduces PainFormer, a vision foundation model based on multi-task learning principles trained simultaneously on 14 tasks/datasets with a total of 10.9 million samples. Functioning as an embedding extractor for various input modalities, the foundation model provides feature representations to the Embedding-Mixer, a transformer-based module that performs the final pain assessment. Extensive experiments employing behavioral modalities–including RGB, synthetic thermal, and estimated depth videos–and physiological modalities such as ECG, EMG, GSR, and fNIRS revealed that PainFormer effectively extracts high-quality embeddings from diverse input modalities. The proposed framework is evaluated on two pain datasets, BioVid and AI4Pain, and directly compared to 75 different methodologies documented in the literature. Experiments conducted in unimodal and multimodal settings demonstrate state-of-the-art performances across modalities and pave the way toward general-purpose models for automatic pain assessment. The foundation model's architecture (code) and weights are available at https://github.com/GkikasStefanos/PainFormer.},
  archive      = {J_TAFFC},
  author       = {Stefanos Gkikas and Raul Fernandez Rojas and Manolis Tsiknakis},
  doi          = {10.1109/TAFFC.2025.3605475},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {PainFormer: A vision foundation model for automatic pain assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LibEER: A comprehensive benchmark and algorithm library for EEG-based emotion recognition. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3605833'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {EEG-based emotion recognition (EER) has gained significant attention due to its potential for understanding and analyzing human emotions. While recent advancements in deep learning techniques have substantially improved EER, the field lacks a convincing benchmark and comprehensive open-source libraries. This absence complicates fair comparisons between models and creates reproducibility challenges for practitioners, which collectively hinder progress. To address these issues, we introduce LibEER, a comprehensive benchmark and algorithm library designed to facilitate fair comparisons in EER. LibEER carefully selects popular and powerful baselines, harmonizes key implementation details across methods, and provides a standardized codebase in PyTorch. By offering a consistent evaluation framework with standardized experimental settings, LibEER enables unbiased assessments of seventeen representative deep learning models for EER across the six most widely used datasets. Additionally, we conduct a thorough, reproducible comparison of model performance and efficiency, providing valuable insights to guide researchers in the selection and design of EER models. Moreover, we make observations and in-depth analysis on the experiment results and identify current challenges in this community. We hope that our work will not only lower entry barriers for newcomers to EEG-based emotion recognition but also contribute to the standardization of research in this domain, fostering steady development. The library and source code are publicly available at https://github.com/XJTU-EEG/LibEER.},
  archive      = {J_TAFFC},
  author       = {Huan Liu and Shusen Yang and Yuzhe Zhang and Mengze Wang and Fanyu Gong and Chengxi Xie and Guanjian Liu and Zejun Liu and Yong-Jin Liu and Bao-Liang Lu and Dalin Zhang},
  doi          = {10.1109/TAFFC.2025.3605833},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {LibEER: A comprehensive benchmark and algorithm library for EEG-based emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distinguishing depression and bipolar disorder from social media data utilizing intensity of emotions and interpretable deep learning models. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3606887'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depression and bipolar disorder (BD) are mental disorders that are often misdiagnosed as each other because their symptoms often overlap. Depression is characterized by prolonged negative emotion, which is persistent feelings of sadness. In contrast, BD is characterized by repeated extreme changes of emotions, consisting of manic (very happy) and depression (very sad) episodes. Because of overlapping symptoms, changes of emotions indicating depression or BD are difficult to recognize from individual posts. We propose two deep learning models to detect depression and BD from user posts, considering individual posts and multiple posts. A novel method to extract emotion features is designed by fine-tuning a transformer model to extract the intensity of emotions of the posts. Text features are extracted using word embedding models that have been fine tuned on mental health data, and we further consider topic features by using a topic modeling method. Multiple posts are considered by grouping them based on a time interval between the posts. The features of each group are concatenated, input into a convolution neural network (CNN), and go through a long short-term memory (LSTM). The attention mechanism is used to pay attention to important groups. The most important groups can be observed further to explain why the model classified depression and BD users. Our proposed model outperforms other models, achieving an F1-Score of 0.9589. Based on our experiment, the changes of emotions within four days are necessary for distinguishing depression and BD, which aligns with considerations in diagnosing symptoms of depression and BD.},
  archive      = {J_TAFFC},
  author       = {Syauki Aulia Thamrin and Arbee L.P. Chen},
  doi          = {10.1109/TAFFC.2025.3606887},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Distinguishing depression and bipolar disorder from social media data utilizing intensity of emotions and interpretable deep learning models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCMIB-PNS: Balancing sufficiency and necessity with probabilistic causality and cross-modal uncertainty in multimodal sentiment analysis. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3606964'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sentiment analysis aims to accurately identify sentiment orientations by integrating information from multiple modalities such as text, audio, and video. However, a key challenge in multimodal fusion is effectively balancing the sufficiency and necessity of information across modalities. Traditional models often fail to qualify and capture this balance due to the presence of noise and redundant information in multimodal data, leading to suboptimal performance in sentiment analysis. To address this issue, we propose a novel multimodal sentiment analysis method called UCMIB-PNS, which is guided by information bottleneck and probabilistic causality. The method employs an Uncertain Cross-Modal Information Bottleneck (UCMIB) module to reduce redundant information within modalities and maximize discriminative information. The UCMIB utilizes codebooks to dynamically record the distributions of samples and employs random sampling to conduct uncertain modeling across different modalities. It integrates uncertainty-aware contrastive learning and KL divergence for dynamic comparison and compression of information from different modalities. Moreover, UCMIB-PNS uses differentiable Probability of Necessity and Sufficiency (PNS) estimators to estimate and re-weight the sufficiency and necessity of modalities by constructing several counterfactual scenarios through end-to-end learning. Experiments conducted on four publicly available multimodal sentiment analysis datasets demonstrate that UCMIB-PNS achieves optimal performance on both clean and noisy data. Extended experiments further validate the method's robustness under different types of noise.},
  archive      = {J_TAFFC},
  author       = {Jili Chen and Yihua Zhong and Qionghao Huang and Changqin Huang and Fan Jiang and Xiaodi Huang and Xun Wang},
  doi          = {10.1109/TAFFC.2025.3606964},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {UCMIB-PNS: Balancing sufficiency and necessity with probabilistic causality and cross-modal uncertainty in multimodal sentiment analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mitigating symptom heterogeneity in multimodal depression estimation via level separation and deviation regression. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3606949'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Depression Estimation (MDE) aims to infer individual depression scores by analyzing various signals, such as visual, auditory, and language signals etc. Compared to Multimodal Depression Detection (MDD) methods that only provide discrete labels, MDE can provide a more refined score evaluation. However, symptom heterogeneity leads to differences in external behaviors among patients with similar depressive states, which limits the performance of direct regression MDE methods. To address this issue, we propose a Combined Depression Level and Deviation (CDLD) method for MDE, which separates samples at different depression levels and further analyzes subtle deviations within same level to improve estimation performance. Specifically, the Multilevel Depression Separation module constructs depression levels with inherent commonalities based on psychological theories and models the ordinality of these levels, thereby separating samples with different levels of depression. Building on this, the Level-specific Deviation Regression module contrasts sample features relative to level-specific anchors, regressing the subtle depression deviation. Finally, the depression level and deviation are integrated to infer the depression score more accurately. Experiments on the DAIC-WOZ, CMDC, SEARCH, and AVEC 2014 datasets demonstrate that the proposed coarse-to-fine method effectively mitigates the impact of symptom heterogeneity on depression estimation performance, showing significant advantages in MDE tasks. The code is publicly available at https://github.com/LIU70KG/CDLD.},
  archive      = {J_TAFFC},
  author       = {Chengguang Liu and Shanmin Wang and Qingshan Liu and Yang Wang and Fei Wang},
  doi          = {10.1109/TAFFC.2025.3606949},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Mitigating symptom heterogeneity in multimodal depression estimation via level separation and deviation regression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Brain-machine enhanced intelligence for semi-supervised facial emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3607025'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning, particularly deep learning, typically achieves high facial emotion image recognition accuracy benefiting from multiple labeled data. However, the datasets usually contain insufficient labeled samples and numerous unlabeled data since human labeling is a costly endeavor. For semi-supervised learning of these datasets, self-training procedure solely based on the visual features of images fails to comprehensively understand the intricate high-level semantic features. Since EEG signals contain not only visual information related to the visual stimulus but also emotional information related to brain activity, they are highly suitable as supervisory signals for labeling unlabeled facial emotion images. In this study, we specifically employ EEG signals evoked by visual image stimuli in conjunction with EEGNet3D to learn a discriminative EEG class representation manifold of brain activity. The one-hot class label is replaced with the EEG class representation as the supervisory to train the base model. Then, better pseudo-labeling is achieved using the base model in the EEG class representation manifold. Based on pseudo-labeling results, the utilization of unlabeled data is further improved. Interestingly, our findings reveal that when utilizing EEG class representations as supervisory information for the base model, the base model demonstrates a learning pattern that involves focusing more on the eye area when making judgments about emotions. This behavior closely resembles how the human brain decodes emotions. Experiments show that the performance of the proposed method can be effectively enhanced by combining labeled and pseudo-labeled images. Further experiments demonstrate that our method exhibits strong generalization abilities when applied to new image datasets and other visual networks.},
  archive      = {J_TAFFC},
  author       = {Dongjun Liu and Weichen Dai and Hangjie Yi and Honggang Liu and Jianting Cao and Qibin Zhao and Fabio Babiloni and Wanzeng Kong},
  doi          = {10.1109/TAFFC.2025.3607025},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Brain-machine enhanced intelligence for semi-supervised facial emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIND-EEG: Multi-granularity integration network with discrete codebook for EEG-based emotion recognition. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3608571'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition using electroencephalogram (EEG) signals has broad potential across various domains. EEG signals have ability to capture rich spatial information related to brain activity, yet effectively modeling and utilizing these spatial relationships remains a challenge. Existing methods struggle with simplistic spatial structure modeling, failing to capture complex node interactions, and lack generalizable spatial connection representations, failing to balance the dynamic nature of brain networks with the need for discriminative and generalizable features. To address these challenges, we propose the Multi-granularity Integration Network with Discrete Codebook for EEG-based Emotion Recognition (MIND-EEG). The framework employs a multi-granularity approach, integrating global and regional spatial information through a Global State Encoder, an Intra-Regional Functionality Encoder, and an Inter-Regional Interaction Encoder to comprehensively model brain activity. Additionally, we introduce a discrete codebook mechanism for constructing network structures via vector quantization, ensuring compact and meaningful brain network representations while mitigating over-smoothing and enhancing model generalization. The proposed framework effectively captures the dynamic and diverse nature of EEG signals, enabling robust emotion recognition. Extensive comparisons and analyses demonstrate the effectiveness of MIND-EEG, and the source code is publicly available at https://github.com/XJTU-EEG/MIND_EEG.},
  archive      = {J_TAFFC},
  author       = {Yuzhe Zhang and Chengxi Xie and Huan Liu and Yuhan Shi and Guanjian Liu and Dalin Zhang},
  doi          = {10.1109/TAFFC.2025.3608571},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MIND-EEG: Multi-granularity integration network with discrete codebook for EEG-based emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multimodal affect perception with large language model enhancement network. <em>TAFFC</em>, 1-17. (<a href='https://doi.org/10.1109/TAFFC.2025.3607727'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal Sentiment Analysis (MSA) plays a vital role in understanding emotional content from social media and multimedia data. However, existing methods often rely on large-scale labeled datasets, leading to high annotation costs and poor adaptability. They also suffer from modality imbalance and suboptimal feature fusion. To address these issues, we propose MapleNet—a Multimodal Affect Perception framework enhanced by Large Language Models. MapleNet integrates a prototype-guided fusion strategy and a dynamic modality balancing mechanism to improve alignment and collaboration between text and image features. Specifically, a shared-space encoder combined with prompt optimization ensures semantic consistency across modalities. Within the prototype learning framework, the model dynamically adjusts modalityspecific learning by aligning features with class prototypes, thus mitigating imbalance and uncovering complementary affective cues. In addition, MapleNet employs a similaritybased sample retrieval module to construct contextual prompts, enriching sentiment understanding in few-shot settings. Experiments on six benchmark datasets show that MapleNet consistently outperforms state-of-the-art methods, especially under few-shot conditions, achieving superior accuracy and generalization. The relevant code is available at https://github.com/YFanLuo/MapleNet.},
  archive      = {J_TAFFC},
  author       = {Kaixiang Yang and Yifan Luo and Zongyan Zhang and C. L. Philip Chen and Tong Zhang},
  doi          = {10.1109/TAFFC.2025.3607727},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Multimodal affect perception with large language model enhancement network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MCGC-net: Multi-scale controllable graph convolutional network on music emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3608830'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel dynamic graph learning approach for frequency graphs, underpinned by a suite of baseline methodologies and the Multi-scale Controllable Graph Convolutional Network (MCGC-net), a multi-channel dynamic graph learning framework. This approach addresses the limitations of temporal graphs in capturing global and local correlations in music signals for music emotion recognition tasks. Frequency graphs integrate timbre and chroma audio features to capture both local and global correlations in music, enhancing emotion classification and segmentation. The graph feature extractor module improves the dissemination of hierarchical feature information through control units and enhances the depiction of remote node attributes via shortcut connections. Additionally, the multi-channel feature extractor module expands the receptive field, enhancing inter-channel feature extraction. The integration of these modules enables the model to proficiently extract musical frequency features, uncovering latent correlations among various frequencies and their impact on emotion recognition. Empirical validation across the 1000songs, PMEmo, and Memo2496 datasets demonstrates the strategy's efficacy, with MCGC-net achieving classification accuracies of 79.03% for Arousal and 70.05% for Valence on 1000songs; 85.81% and 75.42% on PMEmo; and 79.52% and 78.61% on Memo2496, respectively. Complementary ablation studies and T-order robustness assessments further validate the unique contributions of model components and the performance enhancements provided by shortcut connections.},
  archive      = {J_TAFFC},
  author       = {Tong Zhang and Qilin Li and Xueyue Yang and C. L. Philip Chen},
  doi          = {10.1109/TAFFC.2025.3608830},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MCGC-net: Multi-scale controllable graph convolutional network on music emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing emotions and engagement during cognitive stimulation group training with the pepper robot. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3609855'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive Stimulation Therapy (CST) is an evidence-based intervention that involves group or individual sessions demonstrating cognitive benefits in Mild Cognitive Impairment. Recent research suggests that social robots can effectively assist therapists during cognitive stimulation sessions for people with MCI. Building on these findings, we conducted an experimental study to evaluate the impact of a Socially Assistive Robot, on engagement and emotional responses during cognitive interventions for a group of elderly people. Each session was video-recorded for subsequent analysis. The findings suggest that the use of a Social Robot as a mediating tool in CST interventions is associated with high levels of participant engagement and predominantly positive-valenced emotional responses, as detected by both human and automatic evaluations.},
  archive      = {J_TAFFC},
  author       = {Berardina De Carolis and Nicola Macchiarulo and Giuseppe Palestra and Olimpia Pino},
  doi          = {10.1109/TAFFC.2025.3609855},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Analyzing emotions and engagement during cognitive stimulation group training with the pepper robot},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal group constrained transformer with deformable landmark attention for video dimensional emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3610185'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video dimensional emotion recognition aims to map human affect into the dimensional emotion space based on visual signals. Recent works notice that it is beneficial to locate key facial regions related to human emotion perception, as well as establish long-term temporal dependencies. While preliminary attempts have been made, there still exists much space for further improvements. In this paper, to better exploit key facial regions, we propose the Temporal cue guided Deformable Landmark Spatial (TDLS) transformer which attends to key facial regions in a data-dependent manner. We also propose the temporal cue guided frame representation learning to learn the spatial representation of each frame by considering features of other frames together. To better model temporal dependencies, we propose the Multi-layer Group Constrained Temporal (MGCT) transformer to summarize features of frames to multi-layer groups, perform group-to-group communications, and let group-level features guide the frame-level emotion recognition. We also introduce cross-clip representation learning to generate consistent results across different clips and videos. Extensive experiments are conducted on two benchmark datasets and superior results are achieved by our method compared to state-of-the-art approaches.},
  archive      = {J_TAFFC},
  author       = {Weixin Li and Xiangjing Meng and Linmei Hu and Xuan Dong},
  doi          = {10.1109/TAFFC.2025.3610185},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Temporal group constrained transformer with deformable landmark attention for video dimensional emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STRFLNet: Spatio-temporal representation fusion learning network for EEG-based emotion recognition. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3611173'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG)-based emotion recognition is essential for medical assistance and human-computer interaction. Although deep learning-based emotion recognition methods have demonstrated high performance, several challenges remain: 1) How to effectively utilize the complex dynamic-static spatial patterns inherent in emotion-related EEG signals. 2) How to hierarchically learn the latent correlations among multi-domain features. To address these challenges, a spatio-temporal representation fusion learning network (STRFLNet) is proposed to improve both the accuracy and robustness of emotion recognition using EEG signals. Specifically, dynamic-static graph topologies are constructed to capture comprehensive brain functional connectivity states, and a continuous dynamic-static graph ordinary differential equations is introduce to reveal continuous spatial patterns within EEG signals. Additionally, a hierarchical transformer fusion module is developed to fully leverage the latent correlations among multi-domain features to obtain the fused spatio-temporal representation. The experimental results on the SEED, SEED-IV, and DREAMER public EEG emotion datasets, under both subject-independent and subject-dependent settings, demonstrate that STRFLNet outperforms state-of-the-art methods in emotion recognition tasks. We further validate the effectiveness of the proposed model through interpretability analysis, which reveals the associations between the activated brain regions and corresponding emotional states. Our work highlights the significance of continuous spatial pattern learning and spatio-temporal feature fusion in emotion recognition, providing new insights for EEG-based emotion modeling.},
  archive      = {J_TAFFC},
  author       = {Fo Hu and Kailun He and Can Wang and Qinxu Zheng and Bin Zhou and Gang Li and Yu Sun},
  doi          = {10.1109/TAFFC.2025.3611173},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {STRFLNet: Spatio-temporal representation fusion learning network for EEG-based emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Audio-visual feature disentanglement and fusion network for automatic depression severity prediction. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3611238'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to achieve early screening and assist clinical decision-making, automatic depression assessment based on multimodal data are highly anticipated. However, the existed methods often suffer from semantic gap and information redundancy due to heterogeneity among modalities. To address this challenge, this paper investigates a novel Feature Disentanglement and Fusion Network (FDFNet) for predicting depression severity from audio-visual cues. Firstly, we design the shared and private encoders to disentangle modality-shared and modalityprivate representations. The former representation that acquires joint information is subjected by similarity constraints between modalities to ensure their distributions as close as possible. The latter that can capture unique features of each modality is restrained by independence constraints for keeping their distributions distinct. The decoder is then developed to reconstruct unimodal representation with constraints to minimize information loss. Finally, an efficient fusion strategy through addition and concatenation is ultilized for aggregating information. Experimental results on four benchmark datasets demonstrate that the proposed FDFNet consistently outperforms several stateof-the-art methods, with the competitive MAE/RMSE values of 6.22/7.58 on AVEC2013, 5.21/6.49 on AVEC2014, 4.25/5.34 on DAIC-WOZ, and 4.41/5.10 on E-DAIC, indicating that multimodal deep learning based on audio-visual is an attractive solution for objectively evaluating the depression severity.},
  archive      = {J_TAFFC},
  author       = {Shihao Li and Zhuhong Shao and Rongyin Qin and Yongzhen Huang and Peipeng Liang and Xiaobai Li and Yinan Jiang and Yanhe Deng and Tie Liu and Xiaohui Tan},
  doi          = {10.1109/TAFFC.2025.3611238},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Audio-visual feature disentanglement and fusion network for automatic depression severity prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Building and using state-anxiety-oriented graph for student state anxiety assessment in online classroom scenarios. <em>TAFFC</em>, 1-16. (<a href='https://doi.org/10.1109/TAFFC.2025.3611369'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State anxiety is a temporary reaction to external stressors. In online classroom scenarios, severe state anxiety over a long period significantly affects the physical and mental health of high school students. Researchers often use students' facial videos to assess their state anxiety. However, videobased assessment methods face challenges with data and clues, limiting their effectiveness in assessing student state anxiety. First, previous datasets mainly focused on the general population and there was a lack of a domain-specific dataset. Second, a single video provides limited clues, making it challenging for assessment methods to make accurate judgments when the student consistently displays a neutral expression. To address the data challenge, we collected the first state anxiety dataset containing 3,701 video clips from 106 high school students. To solve the clue challenge, we constructed a student-level stateanxiety-oriented graph and proposed a graph-based assessment method for student state anxiety at the video-level. This method incorporates course information, event information, students' academic and mental health statuses, and the earlier video information. Three well-designed attention modules are used to fuse these clues for better performance. Experimental results on the collected dataset demonstrate that our method is highly accurate in assessing students' state anxiety, with minimal errors (MSE = 0.1380, MAE = 0.2768).},
  archive      = {J_TAFFC},
  author       = {Lei Cao and Qi Li and Yaming Hang and Huijun Zhang and Zihan Wei and Fang Luo and Zhihong Qiao},
  doi          = {10.1109/TAFFC.2025.3611369},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Building and using state-anxiety-oriented graph for student state anxiety assessment in online classroom scenarios},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Phy-FusionNet: A memory-augmented transformer for multimodal emotion recognition with periodicity and contextual attention. <em>TAFFC</em>, 1-13. (<a href='https://doi.org/10.1109/TAFFC.2025.3609046'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate emotion recognition from physiological signals is critical for applications in healthcare, autonomous systems, and human-computer interaction. However, prevailing methods often fail to model long-term dependencies and overlook periodic patterns inherent in physiological data. To address these challenges, we propose Phy-FusionNet, a novel memory-augmented transformer architecture for multimodal emotion recognition. Phy-FusionNet introduces a Memory Stream Module with FIFO-queue and decay-based updates to preserve long-term contextual information. It further integrates Fourier-based positional encoding and frequency-aware attention, enabling robust detection of periodic emotional cues. An Adaptive Temporal Attention Module enhances computational efficiency and enables dynamic relevance in temporal feature extraction. For cross-modal fusion, we employ a transformer-based Multimodal Binding Learning framework that balances modality-specific and shared features. Extensive experiments on five public datasets—WESAD, CL-Drive, PPB-Emo, PhyMER, and EEG-VUI—demonstrate that Phy-FusionNet outperforms state-of-the-art models, achieving up to 16.3% improvement in accuracy and superior robustness across diverse emotional states and noisy environments. Notably, the model maintains low performance variance across emotion classes, with F1-Score differences under 2.5%, indicating stable recognition even for subtle or overlapping emotions. Our results underscore the importance of integrating memory, frequency, and adaptive attention for effective affective computing. The code will be publicly available on GitHub.},
  archive      = {J_TAFFC},
  author       = {Tianyi Wu and Erick Purwanto and Yongrun Huang and Su Yang},
  doi          = {10.1109/TAFFC.2025.3609046},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Phy-FusionNet: A memory-augmented transformer for multimodal emotion recognition with periodicity and contextual attention},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spoken in jest, detected in earnest: A systematic review of sarcasm recognition - Multimodal fusion, challenges, and future prospects. <em>TAFFC</em>, 1-20. (<a href='https://doi.org/10.1109/TAFFC.2025.3612205'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sarcasm, a common feature of human communication, poses challenges in interpersonal interactions and human-machine interactions. Linguistic research has highlighted the importance of prosodic cues, such as variations in pitch, speaking rate, and intonation, in conveying sarcastic intent. Although previous work has focused on text-based sarcasm detection, the role of speech data in recognizing sarcasm has been underexplored. Recent advancements in speech technology emphasize the growing importance of leveraging speech data for automatic sarcasm recognition, which can enhance social interactions for individuals with neurodegenerative conditions and improve machine understanding of complex human language use, leading to more nuanced interactions. This systematic review is the first to focus on speech-based sarcasm recognition, charting the evolution from unimodal to multimodal approaches. It covers datasets, feature extraction, and classification methods, and aims to bridge gaps across diverse research domains. The findings include limitations in datasets for sarcasm recognition in speech, the evolution of feature extraction techniques from traditional acoustic features to deep learning-based representations, and the progression of classification methods from unimodal approaches to multimodal fusion techniques. In so doing, we identify the need for greater emphasis on cross-cultural and multilingual sarcasm recognition, as well as the importance of addressing sarcasm as a multimodal phenomenon, rather than a text-based challenge.},
  archive      = {J_TAFFC},
  author       = {Xiyuan Gao and Shekhar Nayak and Matt Coler},
  doi          = {10.1109/TAFFC.2025.3612205},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {9},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Spoken in jest, detected in earnest: A systematic review of sarcasm recognition - Multimodal fusion, challenges, and future prospects},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
