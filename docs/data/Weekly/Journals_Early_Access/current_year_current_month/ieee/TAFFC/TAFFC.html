<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc">TAFFC - 8</h2>
<ul>
<li><details>
<summary>
(2025). How many raters do we need? analyses of uncertainty in estimating ambiguity-aware emotion labels. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3616071'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and expressing emotions is subjective, leading to varying levels of ambiguity in how emotions are perceived. Humans naturally handle this ambiguity, and it's crucial for machines to do the same for natural human-machine interaction. Efforts are being made to develop emotion recognition systems that can handle ambiguity by modelling emotions as distributions of arousal/valence labels. However, existing approaches often assume that an underlying distribution can be inferred from ground truth ratings. Yet, the underlying distribution is never observed and the inferred distribution from the ratings can accurately represent the true distribution only when enough raters are involved. Our study investigates how the number of raters impacts the uncertainty in inferring distributions from ground truth ratings in the context of modelling emotion ambiguity. We then explore how many raters may be sufficient to effectively approximate the ambiguity present in the emotion annotations. Using the Belief Mismatch Coefficient (BMC) for analysis, we compare distributions to different numbers of ground truth ratings leading to quantitative and interpretable inferences. Experimental analysis was conducted using both simulated ratings and the real emotion (both arousal and valence) ratings collected from MSP-Conversation dataset. Our results suggest that using 5 raters may be sufficient to approximate a stable distribution representing the underlying emotional ambiguity when time continuous emotion ratings are captured and annotated from speech. Our study provides insights and practical advice for collecting emotion labels. Additionally, the analyses methods presented in this paper are expected to be relevant in any field of study dealing with labelling of subjective quantities that can vary from person to person.},
  archive      = {J_TAFFC},
  author       = {Jingyao Wu and Ting Dang and Vidhyasaharan Sethu and Eliathamby Ambikairajah},
  doi          = {10.1109/TAFFC.2025.3616071},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {How many raters do we need? analyses of uncertainty in estimating ambiguity-aware emotion labels},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). STREL - Naturalistic dataset and methods for studying mental stress and relaxation patterns in critical leading roles. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3616270'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate mental stress and relaxation patterns in professionals occupying leadership roles in emergency care and special police units. A key finding is that on days that involve critical missions, these individuals experience negative mental stress (i.e., distress) that escalates as the day unfolds. In contrast, on non-leadership workdays, mental distress remains relatively stable, while on non-workdays, participants exhibit positive mental stress (i.e., eustress) that subsides over time, facilitating relaxation. These findings stem from a four-day naturalistic study of $n=24$ professionals, during which we collected physiological, mobility, and psychometric data. Using participant debriefings and sensor-based validation triggers (e.g., GPS, cadence), we labeled activities in 5-minute intervals and focused our analysis on sedentary periods. We defined stress during these periods as a normalized heart rate that exceeds two standard deviations above a personalized baseline, thus isolating mental stress uncontaminated by physical exertion. A logistic regression model based on this stress labeling method yielded results largely consistent with those obtained from Kubios' SNS Index, reinforcing its validity. In cases of disagreement, our method aligned better with participant reports and established literature, highlighting advantages in interpretability and specificity. Overall, our work makes three contributions: (a) to affective science, by quantifying the mentally stressful nature of leadership in high-stakes environments; (b) to affective computing, by proposing a wearable compatible method for estimating mental stress during sedentary activity in the wild; (c) to data science, by introducing a well-annotated, multimodal dataset suitable for machine learning benchmarking in stress detection.},
  archive      = {J_TAFFC},
  author       = {Corinna Rott and Fettah Kiran and Mien Segers and Piet Van den Bossche and Ioannis Pavlidis},
  doi          = {10.1109/TAFFC.2025.3616270},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {STREL - Naturalistic dataset and methods for studying mental stress and relaxation patterns in critical leading roles},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MPFNet: A multi-prior fusion network with a progressive training strategy for micro-expression recognition. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3617652'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-expression recognition (MER), a critical subfield of affective computing, presents greater challenges than macro-expression recognition due to its brief duration and low intensity. While incorporating prior knowledge has been shown to enhance MER performance, existing methods predominantly rely on simplistic, singular sources of prior knowledge, failing to fully exploit multi-source information. This paper introduces the Multi-Prior Fusion Network (MPFNet), leveraging a progressive training strategy to optimize MER tasks. We propose two complementary encoders: the Generic Feature Encoder (GFE) and the Advanced Feature Encoder (AFE), both based on Inflated 3D ConvNets (I3D) with Coordinate Attention (CA) mechanisms, to improve the model's ability to capture spatiotemporal and channel-specific features. Inspired by developmental psychology, we present two variants of MPFNet—MPFNet-P and MPFNet-C—corresponding to two fundamental modes of infant cognitive development: parallel and hierarchical processing. These variants enable the evaluation of different strategies for integrating prior knowledge. Extensive experiments demonstrate that MPFNet significantly improves MER accuracy while maintaining balanced performance across categories, achieving accuracies of 0.811, 0.924, and 0.857 on the SMIC, CASME II, and SAMM datasets, respectively. To the best of our knowledge, our approach achieves state-of-the-art performance on the SMIC and SAMM datasets. The source code is available at: https://github.com/Mac0504/MPFNet.},
  archive      = {J_TAFFC},
  author       = {Chuang Ma and Shaokai Zhao and Dongdong Zhou and Yu Pei and Zhiguo Luo and Liang Xie and Ye Yan and Erwei Yin},
  doi          = {10.1109/TAFFC.2025.3617652},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {MPFNet: A multi-prior fusion network with a progressive training strategy for micro-expression recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive key role guided hierarchical relation inference for enhanced group-level emotion recognition. <em>TAFFC</em>, 1-14. (<a href='https://doi.org/10.1109/TAFFC.2025.3618148'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel hierarchical relational network, termed Key Role Guided Hierarchical Relation Inference (KR-HRI), for enhanced group-level emotion recognition (GER). Unlike existing methods that adopt a coarse-grained approach to model interactions among all individuals, our approach adaptively identifies and emphasizes key individuals who play a crucial role in conveying group-level emotions. By integrating coarse-grained relationship modeling with fine-grained key individual enhancement and leveraging global scene information, our method effectively refines discriminative feature generation while minimizing irrelevant interference. We introduce a Multi-branch Interaction Module (MIM) to dynamically fuse features from both the global scene and local individual branches using a localized mask integration strategy. This comprehensive approach enhances the interaction between global and local features, resulting in robust group-level emotion representations. Extensive experiments on three widely adopted GER datasets demonstrate that our framework consistently outperforms state-of-the-art methods, validating the effectiveness and robustness of our proposed approach.},
  archive      = {J_TAFFC},
  author       = {Qing Zhu and Qirong Mao and Wenlong Dong and Xiuyan Shao and Xiaohua Huang and Wenming Zheng},
  doi          = {10.1109/TAFFC.2025.3618148},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {10},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Adaptive key role guided hierarchical relation inference for enhanced group-level emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid-supervised hypergraph-enhanced transformer for micro-gesture based emotion recognition. <em>TAFFC</em>, 1-15. (<a href='https://doi.org/10.1109/TAFFC.2025.3618639'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-gestures are unconsciously performed body gestures that can convey the emotion states of humans and start to attract more research attention in the fields of human behavior understanding and affective computing as an emerging topic. However, the modeling of human emotion based on micro-gestures has not been explored sufficiently. In this work, we propose to recognize the emotion states based on the micro-gestures by reconstructing behavioral patterns with a hypergraph-enhanced Transformer in a hybrid-supervised framework. In the framework, hypergraph Transformer based encoder and decoder are separately designed by stacking the hypergraph-enhanced self-attention and multiscale temporal convolution modules. Especially, to better capture the subtle motion of micro-gestures, we construct a decoder with additional upsampling operations for a reconstruction task in a self-supervised learning manner. We further propose a hypergraph-enhanced self-attention module where the hyperedges between skeleton joints are gradually updated to present the relationships of body joints for modeling the subtle local motion. Lastly, for exploiting the relationship between the emotion states and local motion of micro-gestures, an emotion recognition head from the output of encoder is designed with a shallow architecture and learned in a supervised way. The end-to-end framework is jointly trained in a one-stage way by comprehensively utilizing self-reconstruction and supervision information. The proposed method is evaluated on two publicly available datasets, namely iMiGUE and SMG, and achieves the best performance under multiple metrics, which is superior to the existing methods. The code is available on Github (https://github.com/xiazhaoqiang/H2OFormerMicroGestureRec).},
  archive      = {J_TAFFC},
  author       = {Zhaoqiang Xia and Hexiang Huang and Haoyu Chen and Xiaoyi Feng and Guoying Zhao},
  doi          = {10.1109/TAFFC.2025.3618639},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {10},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Hybrid-supervised hypergraph-enhanced transformer for micro-gesture based emotion recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How expression, context and perspective determine judgments of emotion. <em>TAFFC</em>, 1-12. (<a href='https://doi.org/10.1109/TAFFC.2025.3618588'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the field of Affective Computing, facial expressions were historically assigned emotion labels by annotators without knowledge of the context in which the expressions were produced. Often these 3rd-person impressions were used instead of 1st-person judgments about the target's feelings. The field now appreciates that 1st- and 3rd-person judgments can differ dramatically and that context plays a central role in explaining these differences. More recently, there is growing appreciation of the importance of a 2nd-person perspective. When people are engaged in a social task, impressions of their partner's emotions differ from the impressions of detached bystanders. In this paper, we explore how facial expressions, contexts and perspective (1st, 2nd-, and 3rd-person) interact to determine judgments of emotion. We explore this using automatic facial expression analysis of natural expressions produced in the context of a social task (the prisoner's dilemma). Our findings suggest that expression and context combine to determine judgments, but the way they combine differs depending on perspective. We discuss the implication of these findings for automatic emotion recognition methods and the inferences such methods can support.},
  archive      = {J_TAFFC},
  author       = {Bin Han and Jessie Hoegen and Su Lei and Gale Lucas and Danielle Shore and Brian Parkinson and Jonathan Gratch},
  doi          = {10.1109/TAFFC.2025.3618588},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {10},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {How expression, context and perspective determine judgments of emotion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal multimodal multitask attention for affective state estimation in a stressful environment. <em>TAFFC</em>, 1-10. (<a href='https://doi.org/10.1109/TAFFC.2025.3618385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotional stress significantly impacts mental and physical health, motivating the need for computational methods that can model subtle affective states. This study introduces a novel multimodal multitask architecture for the simultaneous regression of arousal and valence, two key emotional dimensions correlated with emotional stress. Utilizing the ULM-TSST dataset, the proposed model integrates video, audio, text, and physiological data through a combination of LSTM and Transformer Encoder networks, employing class tokens for task-specific representations. Experimental results demonstrate the model's effectiveness, achieving an average Concordance Correlation Coefficient (CCC) of 60% for valence and 61% for arousal, outperforming existing approaches by a 4% CCC margin. Ablation studies highlight the importance of each modality, confirming that the best performance is achieved only when all modalities are included. Additionally, comparative analysis between the multitask and single-task versions of the architecture confirms that the multitask approach outperforms single-task models in both arousal and valence prediction. This improvement underscores the benefits of shared representations and joint learning of related affective dimensions within a unified framework. The code for this project is publicly available at https://github.com/cosbidev/Temporal-Multimodal-Multitask-Attention.},
  archive      = {J_TAFFC},
  author       = {Giustino Marino and Alessandro Bria and Giuseppe Labanca and Paolo Soda and Rosa Sicilia},
  doi          = {10.1109/TAFFC.2025.3618385},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {10},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Temporal multimodal multitask attention for affective state estimation in a stressful environment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automated boredom recognition using multimodal physiological signals. <em>TAFFC</em>, 1-18. (<a href='https://doi.org/10.1109/TAFFC.2025.3619979'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing access to affordable physiological sensors and computing devices has bolstered the development of emotionaware systems that recognize human emotions. However, to date, there has been little research testing machine-driven approaches to recognizing boredom, particularly in learning contexts. To address that, this study tests the automatic recognition of boredom induced during a video lecture using a multimodal recognition system relying on physiological signals. Electroencephalogram (EEG), electrocardiogram (ECG), galvanic skin response (GSR), and eye gaze data were recorded from 84 healthy adults (mean age = 26.90 ± 5.29) as they watched both non-boring and boring educational videos to control and induce boredom. Signal features were extracted from the physiological data and validated using Wilcoxon signed-rank tests prior to evaluating their utility for boredom recognition with three separate machine-learning classification techniques, namely Extreme Gradient Boosting (XGB), Random Forest (RF), and Gradient Boosting (GB), with leave-one-out cross-validation. Both unimodal and multimodal recognition systems were assessed by evaluating model performance using each physiological signal separately or in combination (using decision fusion). Multimodal recognition was found to enhance model performance when compared to any single modality, with the highest average boredom recognition accuracy of 88.56% ± 0.82% recorded for EEG + eye gaze modal fusion with RF.},
  archive      = {J_TAFFC},
  author       = {Rajamanickam Yuvaraj and Sampathraman Samyuktha and Jack Fogarty and Jun Song Huang and Samuel Tan and Wong Teck Kiong},
  doi          = {10.1109/TAFFC.2025.3619979},
  journal      = {IEEE Transactions on Affective Computing},
  month        = {10},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Affect. Comput.},
  title        = {Automated boredom recognition using multimodal physiological signals},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
