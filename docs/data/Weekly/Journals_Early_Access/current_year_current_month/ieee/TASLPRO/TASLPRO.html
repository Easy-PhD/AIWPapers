<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TASLPRO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taslpro">TASLPRO - 13</h2>
<ul>
<li><details>
<summary>
(2025). Robust detection of partially spoofed audio using semantic-aware inconsistency learning. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Partially spoofed technology subtly manipulates interested parts in an audio to alter the original meaning, with its fine-grained forgery posing great challenges to existing fully spoofed detection countermeasures. Existing partially spoofed audio detection methods have shown excellent effectiveness in distinguishing clean and long-duration spoofed segments. However, their robustness remains limited when malicious attackers manipulate a finer-grained segment (e.g., only a single phoneme) and employ post-processing operations to reduce detectable discontinuities. To face these challenges, we propose the Semantic-Aware Inconsistency Learning (SAIL) method for robust frame-level detection. It incorporates a robust augmentation module (RAM), a Multi-Scale Semantic Inconsistency Learning (MSIL) module, and a Semantic Separation Module (SSM) to learn robust discriminative features by capturing multi-segment discontinuities and semantic inconsistencies introduced by partially spoofed manipulations. Specifically, the RAM is applied to suppress the model's erroneous attention to additional interference caused by post-processing operations on the subtle spoofed artifacts. Then, the MSIL module is proposed to extract semantic inconsistency features after manipulations, using attention mechanisms at different scales to highlight forgery differences at various granularities. Finally, the SSM is devised to refine these features for robust frame-level detection, utilizing contrastive learning to ensure a clear distinction of inconsistent semantic features in the feature space. Extensive experiments are conducted on three public datasets, including ASVS2019PS, HAD, and LAV-DF, showing that our proposed method achieves the best performance under various noisy scenarios.},
  archive  = {J},
  author   = {Jialu Cao and Hui Tian and Peng Tian and Haizhou Li and Jianzong Wang},
  doi      = {10.1109/TASLPRO.2025.3617241},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {Robust detection of partially spoofed audio using semantic-aware inconsistency learning},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VPVID: Variance-preserving velocity-guided interpolant diffusion for speech enhancement and dereverberation. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Diffusion-based generative models for speech enhancement often face challenges in balancing performance and inference efficiency. To address this, we propose a model of Variance-Preserving Velocity-guided Interpolant Diffusion (VPVID), a novel framework that achieves competitive enhancement performance while maintaining high computational efficiency. Our approach incorporates a scalable interpolant framework that reconstructs the reverse diffusion process using velocity terms and state variables. Unlike traditional score-matching objectives, we employ a velocity-based loss function that directly estimates the instantaneous rate of change, providing more stable training and efficient data distribution learning. We further combine stochastic diffusion sampling with probability flow ordinary differential equations, augmented by an adaptive corrector mechanism, creating a flexible sampling strategy that balances quality and efficiency. Extensive experiments on VoiceBank-DEMAND and WSJ0-CHiME3 datasets demonstrate that VPVID significantly outperforms existing baselines across multiple metrics, particularly excelling in noise separation with SI-SIR improvement up to 4.7 dB. Furthermore, VPVID achieves up to 7× faster inference than existing diffusion-based methods while maintaining excellent speech enhancement and dereverberation performance.},
  archive  = {J},
  author   = {Gang Yang and Yangjie Wei and Ben Niu and Yuqiao Wang},
  doi      = {10.1109/TASLPRO.2025.3617254},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {VPVID: Variance-preserving velocity-guided interpolant diffusion for speech enhancement and dereverberation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MASKSER: A robust emotion recognition model based on voice data and noisy transcripts. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, emotion recognition has become an increasingly vital tool for enhancing customer service applications. Especially in telephonic interactions, detecting emotions accurately is crucial for improving human-computer interaction experiences. Despite significant advances in deep learning, current emotion recognition systems that integrate voice and text face challenges such as noise interference in transcripts and inadequate multimodal fusion, which hinder precise emotion detection. In this paper, we introduce MASKSER, a methodology that combines vocal signals and transcribed text in a robust manner. Our approach involves pretraining noisy transcripts with ChatGPT-4 using few-shot learning based on techniques such as masking and sentiment word replacement. This enhances emotion discernment significantly by leveraging the strengths of both modalities. To address the challenges posed by noisy data, we propose a mask-based noise generation model and use it to pretrain the transcript-based model, which helps mitigate inaccuracies. Additionally, we introduce a novel loss function that evaluates the Kullback-Leibler divergence between text and voice encoder distributions, ensuring balanced contributions from both modalities. Experiments are conducted in both English and Korean to validate the language independence and robustness of the proposed approach in different linguistic contexts. The results demonstrate substantial improvements in emotion recognition capabilities, achieving high performance metrics while reducing reliance on costly speech recognition resources.},
  archive  = {J},
  author   = {Yeo-Chan Yoon and Sookyun Kim},
  doi      = {10.1109/TASLPRO.2025.3617234},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {MASKSER: A robust emotion recognition model based on voice data and noisy transcripts},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-lingual embedding clustering for hierarchical softmax in low-resource multilingual speech recognition. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a novel approach centered on the decoding stage of Automatic Speech Recognition (ASR) that enhances multilingual performance, especially for low-resource languages. It utilizes a cross-lingual embedding clustering method to construct a hierarchical Softmax (H-Softmax) decoder, which enables similar tokens across different languages to share similar decoder representations. It addresses the limitations of the previous Huffman-based H-Softmax method, which relied on shallow features in token similarity assessments. Through experiments on a downsampled dataset of 15 languages, we demonstrate the effectiveness of our approach in improving low-resource multilingual ASR accuracy.},
  archive  = {J},
  author   = {Zhengdong Yang and Qianying Liu and Sheng Li and Fei Cheng and Chenhui Chu},
  doi      = {10.1109/TASLPRO.2025.3617233},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Cross-lingual embedding clustering for hierarchical softmax in low-resource multilingual speech recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring width-adaptive transformers for automatic speech recognition. <em>TASLPRO</em>, 1-16. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Transformer architectures with multiple heads with wide attention dimensions (widths) are over-parameterized. This leads to parameter redundancy and high correlations across attention heads with only a minority of heads actively contributing to the task. In this study, we quantitatively analyze the parameter redundancy by comparing the linear centered kernel alignment (CKA) similarity of learned representations extracted across attention layers and heads. Observing that widening the network can exacerbate these correlations, leading to representations with high CKA similarity, we question the design choice with uniform attention widths across all attention heads or layers and investigate how this choice impacts correlations across heads in the same layer. We design a width-adaptive training method to dynamically tune the model to keep the main contributing widths in each attention head and layer while no knowledge distillation or re-training process is needed. Experimental results on both English and Dutch corpora show our adaptive training method effectively reduces cross-head correlations and improves accuracy in automatic speech recognition. We also demonstrate the effectiveness of width-adaptive training by finetuning the OWSM speech foundation model.},
  archive  = {J},
  author   = {Pu Wang and Hugo Van Hamme},
  doi      = {10.1109/TASLPRO.2025.3617232},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-16},
  title    = {Exploring width-adaptive transformers for automatic speech recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhance the saliency: Synthesize text noise samples for few-shot out-of-distribution intent detection. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Few-shot out-of-distribution (OOD) detection is a critical yet under explored scenario in dialogue systems. Existing data augmentation techniques either incorporate external data or generate hard negative samples within the feature space, which often leads to issues such as introducing knowledge bias, failing to align with the discrete nature of text, and inadequately addressing the problem of under-representation caused by in-distribution (IND) overfitting. Motivated by the recent findings that enhancing intra-class discrimination can mitigate IND overfitting, and the class of a sentence is predominantly determined by salient words, we propose EnSal, a method designed to strengthen the features of salient words in order to enhance the correlation between intent features and their corresponding classes. To achieve this, we jointly train k-nearest neighbors contrastive learning (KCL) alongside cross-entropy (CE) to improve the intra-class discrimination of intent features. Salient words are identified using both the k-nearest neighbors condition and the prediction probability condition. These words are retained as templates for synthesizing text samples, thereby avoiding the introduction of knowledge bias while preserving consistency with the discrete characteristics of text. Furthermore, we treat the synthetic text as noise samples associated with their corresponding training samples and perform denoising autoencoder (DAE) training on the augmented dataset. This process enables the identification of common and significant class features, effectively alleviating the under-representation issue. Extensive experimental results demonstrate that our method surpasses the current state-of-the-art in few-shot OOD intent detection. The code and models will be made available at https://github.com/wangpei2009job/EnSal.},
  archive  = {J},
  author   = {Pei Wang and Jiangtao Ren},
  doi      = {10.1109/TASLPRO.2025.3617229},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Enhance the saliency: Synthesize text noise samples for few-shot out-of-distribution intent detection},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A temporal-spatial joint high-gain beamforming method in the STFT domain based on kronecker product filters. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Superdirective beamformers are highly appealing for their superior directivity and effectiveness in suppressing diffuse noise. However, their sensitivity to sensor noise and array imperfections poses significant challenges in practice. Achieving higher robustness often necessitates a trade-off in directivity, thereby reducing their ability to suppress directional and diffuse noises. A key concern, therefore, is how to improve noise suppression while maintaining robustness. To address this, we propose in this paper a novel temporal-spatial joint high-gain beamforming method based on a Kronecker product decomposition, making use of the inter-frame correlation to improve performance. The signal model in the proposed work uses recent pairs of time frames and employs the Kronecker product of the steering vector with a frequency- and angle-dependent inter-frame correlation vector. The high-gain beamformers are formulated as Kronecker product filters, where the temporal filter is optimized to maximize the white noise gain (WNG) and the spatial filter is optimized to enhance the directivity factor (DF). With accurate estimation of the correlation vector, Kronecker product high-gain beamformers can simultaneously improve both WNG and DF. The proposed method offers flexibility and can be extended to design other types of beamformers, with a maximum WNG (MWNG) beamformer presented as an example within the same framework. This paper also explores three approaches to estimating the correlation vector: time-invariant, time-varying, and data-driven estimations. Simulation results show notable improvements in noise suppression performance across various scenarios, highlighting the practical effectiveness of the proposed method.},
  archive  = {J},
  author   = {Xiaoran Yang and Hanchen Pei and Jacob Benesty and Gongping Huang and Jingdong Chen},
  doi      = {10.1109/TASLPRO.2025.3617242},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {A temporal-spatial joint high-gain beamforming method in the STFT domain based on kronecker product filters},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Retrieval-augmented prompt learning for pre-trained foundation models. <em>TASLPRO</em>, 1-12. (<a href='https://doi.org/10.1109/TASLPRO.2025.3608936'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The pre-trained foundation models (PFMs) have become essential for facilitating large-scale multimodal learning. Researchers have effectively employed the “pre-train, prompt, and predict” paradigm through prompt learning to induce improved few-shot performance. However, prompt learning approaches for PFMs still follow a parametric learning paradigm. As such, the stability of generalization in memorization and rote learning can be compromised. More specifically, conventional prompt learning might face difficulties in fully utilizing atypical instances and avoiding overfitting to shallow patterns with limited data during the process of fully-supervised training. To overcome these constraints, we present our approach, named RETROPROMPT, which aims to achieve a balance between memorization and generalization by decoupling knowledge from mere memorization. Unlike traditional prompting methods, RETROPROMPT leverages a publicly accessible knowledge base generated from the training data and incorporates a retrieval mechanism throughout the input, training, and inference stages. This enables the model to actively retrieve relevant contextual information from the corpus, thereby enhancing the available cues. We conduct comprehensive experiments on a variety of datasets across natural language processing and computer vision tasks to demonstrate the superior performance of our proposed approach, RETROPROMPT, in both zero-shot and few-shot scenarios. Through detailed analysis of memorization patterns, we observe that RETROPROMPT effectively reduces the reliance on rote memorization, leading to enhanced generalization.},
  archive  = {J},
  author   = {Xiang Chen and Yixin Ou and Quan Feng and Lei Li and Piji Li and Haibo Ye and Sheng-Jun Huang and Shuofei Qiao and Shumin Deng and Huajun Chen and Ningyu Zhang},
  doi      = {10.1109/TASLPRO.2025.3608936},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-12},
  title    = {Retrieval-augmented prompt learning for pre-trained foundation models},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-path state-space modeling with cross-domain interaction for multichannel speech enhancement. <em>TASLPRO</em>, 1-15. (<a href='https://doi.org/10.1109/TASLPRO.2025.3618543'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a novel dual-path state-space model within an encoder-decoder architecture for multichannel speech enhancement that leverages joint temporal and spectral modeling to significantly improve speech quality in noisy and reverberant environments. At the core of our framework is the S5 state-space model, which efficiently captures complex temporal dependencies across multiple speech channels by modeling both short and long-term dynamics. To effectively integrate spatial and spectral features, our encoder employs S3Conv layers that extract salient characteristics from the raw input, while a dedicated cross-domain interaction mechanism facilitates a dynamic exchange of information between two parallel data streams used for coarse magnitude estimation and complex spectral refinement, respectively. This dual-path design enables the network to jointly enhance amplitude and phase information, resulting in improved perceptual quality and intelligibility. Extensive experiments on public datasets demonstrate that our model outperforms state-of-the-art methods across multiple evaluation metrics. Ablation studies further validate the effectiveness of each component in the overall architecture, confirming that the integration of state-space-based temporal sequence modeling and cross-domain feature fusion is critical for robust, high-quality speech enhancement. Our results also indicate that the proposed framework is well-suited for real-world applications where computational efficiency and superior performance are essential.},
  archive  = {J},
  author   = {Xingyu Shen and Runze Wang and Wei-Ping Zhu and Benoit Champagne},
  doi      = {10.1109/TASLPRO.2025.3618543},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-15},
  title    = {Dual-path state-space modeling with cross-domain interaction for multichannel speech enhancement},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MuFFIN: Multifaceted pronunciation feedback model with interactive hierarchical neural modeling. <em>TASLPRO</em>, 1-16. (<a href='https://doi.org/10.1109/TASLPRO.2025.3619765'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Computer-assisted pronunciation training (CAPT) manages to facilitate second-language (L2) learners to practice pronunciation skills by offering timely and instructive feedback. To examine pronunciation proficiency from multiple facets, existing methods for CAPT broadly fall into two categories: mispronunciation detection and diagnosis (MDD) as well as automatic pronunciation assessment (APA). The former aims to pinpoint phonetic pronunciation errors and provide diagnostic feedback, while the latter seeks instead to quantify pronunciation proficiency pertaining to various aspects. Despite the natural complementarity between MDD and APA, researchers and practitioners, however, often treat them as independent tasks with disparate modeling paradigms. In light of this, we in this paper first introduce MuFFIN, a Multi-Faceted pronunciation Feedback model with an Interactive hierarchical Neural architecture, to jointly address the tasks of MDD and APA. To better capture the nuanced distinctions between phonemes in the feature space, a novel phoneme-contrastive ordinal regularization mechanism is then put forward to optimize the proposed model to generate more phoneme-discriminative features while factoring in the ordinality of the aspect scores. In addition, to address the intricate data imbalance problem in MDD, we design a simple yet effective training objective, which is specifically tailored to perturb the outputs of a phoneme classifier with the phoneme-specific variations, so as to better render the distribution of predicted phonemes meanwhile considering their mispronunciation characteristics. A series of experiments conducted on the Speechocean762 benchmark dataset demonstrates the efficacy of our method in relation to several cutting-edge baselines, showing state-of-the-art performance on both the APA and MDD tasks.},
  archive  = {J},
  author   = {Bi-Cheng Yan and Ming-Kang Tsai and Berlin Chen},
  doi      = {10.1109/TASLPRO.2025.3619765},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-16},
  title    = {MuFFIN: Multifaceted pronunciation feedback model with interactive hierarchical neural modeling},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bayesian sound field reconstruction using partial boundary information. <em>TASLPRO</em>, 1-12. (<a href='https://doi.org/10.1109/TASLPRO.2025.3619822'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The problem of reconstructing a spatial sound field from microphone signals and a coarse, partial, and/or uncertain point cloud representation of the boundaries of the room is considered. This problem has downstream applications within sound field control for which precise reconstruction is essential. Typical for these applications is that only microphone measurements are considered, resulting in poor reconstruction in a large spatial region and at high frequencies when few microphones are available. In contrast, in an idealistic setting, where boundary geometry and acoustic properties are known, the sound field can be simulated as a forward problem. However, since the acquisition of such information can be costly and time-consuming, we consider the intermediate setting where partial information of the boundary geometry is available. We formulate the problem in a Bayesian setting, where the boundary information is used to form a prior distribution on the sound field. The paper extends our preliminary work in [1] by allowing for multiple impedance boundary conditions and by introducing a weighting of the boundary points. A scheme for finding an optimal weighting is introduced to reduce the influence of points far from the region of interest or points not consistent with the microphone measurements. Finally, extensive numerical simulation experiments are performed to understand the properties of the boundary-informed regularizer. To further validate the performance and robustness on real data in relation to commonly used regularizers, we release the Field LAser-calibrated Impulse Response (FLAIR) dataset. This dataset consists of 135 microphone measurements along with a laser calibrated, millimeter accurate point cloud of the room geometry and microphone positions that is aimed at stimulating further research in this domain.},
  archive  = {J},
  author   = {David Sundström and Filip Elvander and Andreas Jakobsson},
  doi      = {10.1109/TASLPRO.2025.3619822},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-12},
  title    = {Bayesian sound field reconstruction using partial boundary information},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Less means more: Single stream audio-visual sound source localization via shared-parameter network. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3619850'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The architecture of two-stream network has been widely adopted in the task of audio-visual learning, especially for sound source localization. With a common way to separately process the different modalities, most current approaches establish the audio-visual correlation by maximizing the cosine similarity of representations from two streams. Unfortunately, the challenge of abundant inference parameters still limits this scheme to be further developed mainly because the parameter of modality-specific networks cannot be reused. Inspired by the mechanism of model averaging, in this study, an Iterative Multi-Modal Parameters Fusion (IMP-Fusion) strategy is proposed to fuse the network parameters during the training phase. By integrating the audio and visual knowledge into a unified architecture, a single-stream network is proposed to handle both modalities in the same time-round. Substantial experiments conducted on challenging benchmarks have validated a superior performance, even with only half of the inference parameter in comparison to the other state-of-the-art works. As a plug-and-play mechanism, the proposed IMP-Fusion strategy is also promising to benefit the design of future audio-visual networks.},
  archive  = {J},
  author   = {Tianyu Liu and Peng Zhang and Junwen Xiong and Chuanyue Li and Yue Huo and Wei Huang and Yufei Zha and Lei Xie and Yanning Zhang},
  doi      = {10.1109/TASLPRO.2025.3619850},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Less means more: Single stream audio-visual sound source localization via shared-parameter network},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Combining deterministic enhanced conditions with dual-streaming encoding for diffusion-based speech enhancement. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3619824'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Score-based diffusion models for speech enhancement (SE) need to incorporate correct prior knowledge as reliable conditions to generate accurate predictions. However, providing reliable conditions using noisy features is challenging. One solution is to use features enhanced by deterministic methods as conditions. However, the information distortion and loss caused by deterministic methods might affect the diffusion process. In this paper, we first investigate the effects of using different deterministic SE models as conditions for diffusion. We validate two conditions depending on whether the noisy feature was used as part of the condition: one using only the deterministic feature (deterministic-only), and the other using both deterministic and noisy features (deterministic-noisy). Preliminary investigation found that using deterministic enhanced conditions improves DNSMOS and UTMOS on real data, while the choice between using deterministic-only or deterministic-noisy conditions depends on the deterministic models. Based on these findings, we propose the deterministic-diffusion unified model for SE to more effectively utilize both conditions. Moreover, we found that fine-grained deterministic models have greater potential in objective evaluation metrics, while UNet-based deterministic models provide more stable diffusion performance. Therefore, we also propose a deterministic model that combines coarse- and fine-grained processing for the diffusion. Experimental results on CHiME4 show that the proposed models effectively leverage deterministic models to achieve better SE evaluation scores on the DNSMOS and UTMOS for real evaluation sets. In addition, the proposed deterministic model proves to be more stable than other deterministic models when it is used for diffusion.},
  archive  = {J},
  author   = {Hao Shi and Xugang Lu and Kazuki Shimada and Tatsuya Kawahara},
  doi      = {10.1109/TASLPRO.2025.3619824},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Combining deterministic enhanced conditions with dual-streaming encoding for diffusion-based speech enhancement},
  year     = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
