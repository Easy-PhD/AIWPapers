<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TASLPRO</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taslpro">TASLPRO - 7</h2>
<ul>
<li><details>
<summary>
(2025). Robust detection of partially spoofed audio using semantic-aware inconsistency learning. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617241'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Partially spoofed technology subtly manipulates interested parts in an audio to alter the original meaning, with its fine-grained forgery posing great challenges to existing fully spoofed detection countermeasures. Existing partially spoofed audio detection methods have shown excellent effectiveness in distinguishing clean and long-duration spoofed segments. However, their robustness remains limited when malicious attackers manipulate a finer-grained segment (e.g., only a single phoneme) and employ post-processing operations to reduce detectable discontinuities. To face these challenges, we propose the Semantic-Aware Inconsistency Learning (SAIL) method for robust frame-level detection. It incorporates a robust augmentation module (RAM), a Multi-Scale Semantic Inconsistency Learning (MSIL) module, and a Semantic Separation Module (SSM) to learn robust discriminative features by capturing multi-segment discontinuities and semantic inconsistencies introduced by partially spoofed manipulations. Specifically, the RAM is applied to suppress the model's erroneous attention to additional interference caused by post-processing operations on the subtle spoofed artifacts. Then, the MSIL module is proposed to extract semantic inconsistency features after manipulations, using attention mechanisms at different scales to highlight forgery differences at various granularities. Finally, the SSM is devised to refine these features for robust frame-level detection, utilizing contrastive learning to ensure a clear distinction of inconsistent semantic features in the feature space. Extensive experiments are conducted on three public datasets, including ASVS2019PS, HAD, and LAV-DF, showing that our proposed method achieves the best performance under various noisy scenarios.},
  archive  = {J},
  author   = {Jialu Cao and Hui Tian and Peng Tian and Haizhou Li and Jianzong Wang},
  doi      = {10.1109/TASLPRO.2025.3617241},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {Robust detection of partially spoofed audio using semantic-aware inconsistency learning},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). VPVID: Variance-preserving velocity-guided interpolant diffusion for speech enhancement and dereverberation. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617254'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Diffusion-based generative models for speech enhancement often face challenges in balancing performance and inference efficiency. To address this, we propose a model of Variance-Preserving Velocity-guided Interpolant Diffusion (VPVID), a novel framework that achieves competitive enhancement performance while maintaining high computational efficiency. Our approach incorporates a scalable interpolant framework that reconstructs the reverse diffusion process using velocity terms and state variables. Unlike traditional score-matching objectives, we employ a velocity-based loss function that directly estimates the instantaneous rate of change, providing more stable training and efficient data distribution learning. We further combine stochastic diffusion sampling with probability flow ordinary differential equations, augmented by an adaptive corrector mechanism, creating a flexible sampling strategy that balances quality and efficiency. Extensive experiments on VoiceBank-DEMAND and WSJ0-CHiME3 datasets demonstrate that VPVID significantly outperforms existing baselines across multiple metrics, particularly excelling in noise separation with SI-SIR improvement up to 4.7 dB. Furthermore, VPVID achieves up to 7Ã— faster inference than existing diffusion-based methods while maintaining excellent speech enhancement and dereverberation performance.},
  archive  = {J},
  author   = {Gang Yang and Yangjie Wei and Ben Niu and Yuqiao Wang},
  doi      = {10.1109/TASLPRO.2025.3617254},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {VPVID: Variance-preserving velocity-guided interpolant diffusion for speech enhancement and dereverberation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MASKSER: A robust emotion recognition model based on voice data and noisy transcripts. <em>TASLPRO</em>, 1-14. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617234'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, emotion recognition has become an increasingly vital tool for enhancing customer service applications. Especially in telephonic interactions, detecting emotions accurately is crucial for improving human-computer interaction experiences. Despite significant advances in deep learning, current emotion recognition systems that integrate voice and text face challenges such as noise interference in transcripts and inadequate multimodal fusion, which hinder precise emotion detection. In this paper, we introduce MASKSER, a methodology that combines vocal signals and transcribed text in a robust manner. Our approach involves pretraining noisy transcripts with ChatGPT-4 using few-shot learning based on techniques such as masking and sentiment word replacement. This enhances emotion discernment significantly by leveraging the strengths of both modalities. To address the challenges posed by noisy data, we propose a mask-based noise generation model and use it to pretrain the transcript-based model, which helps mitigate inaccuracies. Additionally, we introduce a novel loss function that evaluates the Kullback-Leibler divergence between text and voice encoder distributions, ensuring balanced contributions from both modalities. Experiments are conducted in both English and Korean to validate the language independence and robustness of the proposed approach in different linguistic contexts. The results demonstrate substantial improvements in emotion recognition capabilities, achieving high performance metrics while reducing reliance on costly speech recognition resources.},
  archive  = {J},
  author   = {Yeo-Chan Yoon and Sookyun Kim},
  doi      = {10.1109/TASLPRO.2025.3617234},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-14},
  title    = {MASKSER: A robust emotion recognition model based on voice data and noisy transcripts},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-lingual embedding clustering for hierarchical softmax in low-resource multilingual speech recognition. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We present a novel approach centered on the decoding stage of Automatic Speech Recognition (ASR) that enhances multilingual performance, especially for low-resource languages. It utilizes a cross-lingual embedding clustering method to construct a hierarchical Softmax (H-Softmax) decoder, which enables similar tokens across different languages to share similar decoder representations. It addresses the limitations of the previous Huffman-based H-Softmax method, which relied on shallow features in token similarity assessments. Through experiments on a downsampled dataset of 15 languages, we demonstrate the effectiveness of our approach in improving low-resource multilingual ASR accuracy.},
  archive  = {J},
  author   = {Zhengdong Yang and Qianying Liu and Sheng Li and Fei Cheng and Chenhui Chu},
  doi      = {10.1109/TASLPRO.2025.3617233},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Cross-lingual embedding clustering for hierarchical softmax in low-resource multilingual speech recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring width-adaptive transformers for automatic speech recognition. <em>TASLPRO</em>, 1-16. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617232'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Transformer architectures with multiple heads with wide attention dimensions (widths) are over-parameterized. This leads to parameter redundancy and high correlations across attention heads with only a minority of heads actively contributing to the task. In this study, we quantitatively analyze the parameter redundancy by comparing the linear centered kernel alignment (CKA) similarity of learned representations extracted across attention layers and heads. Observing that widening the network can exacerbate these correlations, leading to representations with high CKA similarity, we question the design choice with uniform attention widths across all attention heads or layers and investigate how this choice impacts correlations across heads in the same layer. We design a width-adaptive training method to dynamically tune the model to keep the main contributing widths in each attention head and layer while no knowledge distillation or re-training process is needed. Experimental results on both English and Dutch corpora show our adaptive training method effectively reduces cross-head correlations and improves accuracy in automatic speech recognition. We also demonstrate the effectiveness of width-adaptive training by finetuning the OWSM speech foundation model.},
  archive  = {J},
  author   = {Pu Wang and Hugo Van Hamme},
  doi      = {10.1109/TASLPRO.2025.3617232},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-16},
  title    = {Exploring width-adaptive transformers for automatic speech recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhance the saliency: Synthesize text noise samples for few-shot out-of-distribution intent detection. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617229'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Few-shot out-of-distribution (OOD) detection is a critical yet under explored scenario in dialogue systems. Existing data augmentation techniques either incorporate external data or generate hard negative samples within the feature space, which often leads to issues such as introducing knowledge bias, failing to align with the discrete nature of text, and inadequately addressing the problem of under-representation caused by in-distribution (IND) overfitting. Motivated by the recent findings that enhancing intra-class discrimination can mitigate IND overfitting, and the class of a sentence is predominantly determined by salient words, we propose EnSal, a method designed to strengthen the features of salient words in order to enhance the correlation between intent features and their corresponding classes. To achieve this, we jointly train k-nearest neighbors contrastive learning (KCL) alongside cross-entropy (CE) to improve the intra-class discrimination of intent features. Salient words are identified using both the k-nearest neighbors condition and the prediction probability condition. These words are retained as templates for synthesizing text samples, thereby avoiding the introduction of knowledge bias while preserving consistency with the discrete characteristics of text. Furthermore, we treat the synthetic text as noise samples associated with their corresponding training samples and perform denoising autoencoder (DAE) training on the augmented dataset. This process enables the identification of common and significant class features, effectively alleviating the under-representation issue. Extensive experimental results demonstrate that our method surpasses the current state-of-the-art in few-shot OOD intent detection. The code and models will be made available at https://github.com/wangpei2009job/EnSal.},
  archive  = {J},
  author   = {Pei Wang and Jiangtao Ren},
  doi      = {10.1109/TASLPRO.2025.3617229},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {Enhance the saliency: Synthesize text noise samples for few-shot out-of-distribution intent detection},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A temporal-spatial joint high-gain beamforming method in the STFT domain based on kronecker product filters. <em>TASLPRO</em>, 1-13. (<a href='https://doi.org/10.1109/TASLPRO.2025.3617242'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Superdirective beamformers are highly appealing for their superior directivity and effectiveness in suppressing diffuse noise. However, their sensitivity to sensor noise and array imperfections poses significant challenges in practice. Achieving higher robustness often necessitates a trade-off in directivity, thereby reducing their ability to suppress directional and diffuse noises. A key concern, therefore, is how to improve noise suppression while maintaining robustness. To address this, we propose in this paper a novel temporal-spatial joint high-gain beamforming method based on a Kronecker product decomposition, making use of the inter-frame correlation to improve performance. The signal model in the proposed work uses recent pairs of time frames and employs the Kronecker product of the steering vector with a frequency- and angle-dependent inter-frame correlation vector. The high-gain beamformers are formulated as Kronecker product filters, where the temporal filter is optimized to maximize the white noise gain (WNG) and the spatial filter is optimized to enhance the directivity factor (DF). With accurate estimation of the correlation vector, Kronecker product high-gain beamformers can simultaneously improve both WNG and DF. The proposed method offers flexibility and can be extended to design other types of beamformers, with a maximum WNG (MWNG) beamformer presented as an example within the same framework. This paper also explores three approaches to estimating the correlation vector: time-invariant, time-varying, and data-driven estimations. Simulation results show notable improvements in noise suppression performance across various scenarios, highlighting the practical effectiveness of the proposed method.},
  archive  = {J},
  author   = {Xiaoran Yang and Hanchen Pei and Jacob Benesty and Gongping Huang and Jingdong Chen},
  doi      = {10.1109/TASLPRO.2025.3617242},
  journal  = {IEEE Transactions on Audio, Speech and Language Processing},
  month    = {10},
  pages    = {1-13},
  title    = {A temporal-spatial joint high-gain beamforming method in the STFT domain based on kronecker product filters},
  year     = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
