<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PIEEE</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pieee">PIEEE - 2</h2>
<ul>
<li><details>
<summary>
(2025). Cooperative perception for automated driving: A survey of algorithms, applications, and future directions. <em>PIEEE</em>, 1-27. (<a href='https://doi.org/10.1109/JPROC.2025.3608874'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cooperative perception (CP) has emerged as an important research topic in connected and automated transportation systems, significantly enhancing situational awareness in challenging conditions such as limited communication bandwidth, harsh weather, and poor lighting. By sharing and aggregating data from multiple sources—including vehicles, roadside units (RSUs), and other infrastructure—CP addresses critical issues like occlusion and long-range perception in complex environments. This article provides a comprehensive review of existing CP methodologies, evaluating their effectiveness across diverse scenarios and fusion layers. We investigate approaches ranging from early to late collaboration strategies, highlighting how multimodal sensing at the network level can improve detection accuracy, system robustness, and adaptability. Additionally, we review publicly available datasets, identifying key gaps in their coverage of heterogeneous sensors, adverse conditions, and large-scale spatial scenarios. Finally, we propose actionable future research directions, including advanced algorithms for adverse environments, adaptive fusion strategies for heterogeneous sensors, and the integration of emerging technologies such as high-definition (HD) maps, large language models (LLMs), and end-to-end frameworks. These advancements aim to ensure reliability, interoperability, and scalability for real-world connected and automated vehicle (CAV) applications.},
  archive      = {J_PIEEE},
  author       = {Chuheng Wei and Guoyuan Wu and Matthew J. Barth},
  doi          = {10.1109/JPROC.2025.3608874},
  journal      = {Proceedings of the IEEE},
  month        = {10},
  pages        = {1-27},
  shortjournal = {Proc. IEEE},
  title        = {Cooperative perception for automated driving: A survey of algorithms, applications, and future directions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep-learning-empowered super resolution: A comprehensive survey and future prospects. <em>PIEEE</em>, 1-41. (<a href='https://doi.org/10.1109/JPROC.2025.3613233'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Super-resolution (SR) has garnered significant attention within the computer vision community, driven by advances in deep learning (DL) techniques and the growing demand for high-quality visual applications. With the expansion of this field, numerous surveys have emerged. Most existing surveys focus on specific domains, lacking a comprehensive overview of this field. Here, we present an in-depth review of diverse SR methods, encompassing single-image SR (SISR), video SR (VSR), stereo SR (SSR), and light field SR (LFSR). We extensively cover over 150 SISR methods, nearly 70 VSR approaches, and approximately 30 techniques for SSR and LFSR. We analyze methodologies, datasets, evaluation protocols, empirical results, and complexity. In addition, we conducted a taxonomy based on each backbone structure according to the diverse purposes. We also explore valuable yet understudied open issues in the field. We believe that this work will serve as a valuable resource and offer guidance to researchers in this domain. To facilitate access to related work, we created a dedicated repository available at https://github.com/AVC2-UESTC/Holistic-Super-Resolution-Review},
  archive      = {J_PIEEE},
  author       = {Le Zhang and Ao Li and Qibin Hou and Ce Zhu and Yonina C. Eldar},
  doi          = {10.1109/JPROC.2025.3613233},
  journal      = {Proceedings of the IEEE},
  month        = {10},
  pages        = {1-41},
  shortjournal = {Proc. IEEE},
  title        = {Deep-learning-empowered super resolution: A comprehensive survey and future prospects},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
