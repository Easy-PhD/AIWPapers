<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TEVC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tevc">TEVC - 3</h2>
<ul>
<li><details>
<summary>
(2025). Enhanced evolution of parallel algorithm portfolio for vehicle routing problem via transfer optimization. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3616385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel Algorithm Portfolio (PAP), comprising several component solvers with complementary capabilities, emerges as a cutting-edge computational technique for addressing computationally hard problems. Automatic construction of PAPs can develop high-performance PAPs without human intervention. It is natural to believe that the component solvers share common useful building blocks, thus knowledge transfer among them could be beneficial during the evolution. However, this has been neglected by existing studies. To fill this gap, we propose a transfer optimization framework for automatically co-evolving high-performance PAPs. Specifically, we develop a novel performance-oriented dynamic instance grouping strategy to divide problem instances into groups, each of which is associated with a subpopulation of individuals tasked with evolving a component solver. Additionally, the framework incorporates an adaptive knowledge transfer strategy that automatically identifies when and how to transfer knowledge among instance groups. We conducted extensive experiments on the well-known Vehicle Routing Problem (VRP), a famously challenging NP-hard combinatorial optimization problem. The comprehensive experimental results from three public benchmarks demonstrate that our proposed framework significantly outperforms existing state-of-the-art VRP solvers and automatic PAP construction methods.},
  archive      = {J_TEVC},
  author       = {Tong Guo and Yi Mei and Mengjie Zhang and Ke Tang and Kaiquan Cai and Wenbo Du},
  doi          = {10.1109/TEVC.2025.3616385},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Enhanced evolution of parallel algorithm portfolio for vehicle routing problem via transfer optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-learning inspired single-step generative model for expensive multitask optimization problems. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3617343'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In expensive multitask optimization problems (ExMTOPs), multiple complex tasks must be optimized simultaneously under limited computational budgets. Existing approaches, often based on surrogate models, aim to approximate objective functions but struggle to generalize across heterogeneous tasks, depend on task-specific sampling, and require frequent retraining. To address these challenges, we propose the Multifactorial Evolutionary Algorithm–Single Step Generative Model (MFEA-SSG), a meta-learning-inspired framework that learns to generate high-quality solutions across tasks. Inspired by meta-learning, we treat each random shuffle of the decision variables as a unique pseudo-task, training the model on a distribution of these tasks to learn a task-agnostic prior about the structure of elite solutions. This process disrupts task-specific dependencies, allowing the model to learn transferable structures from recomposed samples. We then adopt a diffusion-based generative model to learn the distribution of optimal solutions, enabling knowledge transfer across tasks without directly approximating objective functions. To reduce inference cost, we introduce a student model distilled from the diffusion process. Unlike conventional diffusion models that denoise iteratively, the student generates solutions in a single forward pass, significantly reducing inference time. Comprehensive experiments on both general multitask benchmarks and a real-world protein mutation prediction scenario demonstrate that MFEA-SSG achieves high-quality solutions with fast convergence and low computational cost under limited evaluation budgets, outperforming state-of-the-art general and ExMTOPs algorithms.},
  archive      = {J_TEVC},
  author       = {Ruilin Wang and Xiang Feng and Huiqun Yu and Yang Tan and Edmund M-K Lai},
  doi          = {10.1109/TEVC.2025.3617343},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Meta-learning inspired single-step generative model for expensive multitask optimization problems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-light: Automatic lightweight you-only-look-once generation in different scenarios through NeuroEvolution. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3617095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {You-Only-Look-Once (YOLO) represents the state-of-the-art in object detection models. With the emergence of various applications utilizing small domain-specific datasets and limited computing resources for extensive model training and deployment, there is an increasing demand for customized lightweight YOLO architectures. In this paper, we propose a general NeuroEvolution-based method, termed YOLO-Light, designed to automatically create lightweight variants of YOLO architectures tailored to object detection tasks across diverse scenarios. For a given task, YOLO-Light first initializes a population of minimal YOLO architectures and subsequently evolves these models within a novel parallel-chain evolutionary space. This process employs a diversity-protecting evolutionary search strategy until some architectures meet the expected performance standards. During evolution, YOLO-Light incorporates a dynamic evolution regulation mechanism to adjust the evolutionary configuration, thereby enhancing efficiency based on the current evolutionary state. We applied YOLO-Light to generate lightweight YOLOv5, YOLOv8, and YOLOv10 architectures for object detection on the Roboflow 100 small dataset collection, which comprises 100 diverse datasets spanning 7 distinct imagery domains, with a total of 224,714 images and 829 classes. Our experiments focused on 20 datasets ranging from 105 to 8,992 images and 1 to 53 classes. The experimental results show that YOLO-Light reduced the number of parameters by 54–95%, while maintaining or improving mean Average Precision (mAP) compared to standard YOLO architectures. These results demonstrate the effectiveness of YOLO-Light in generating lightweight, task-specific YOLO architectures for resource-constrained object detection tasks. The code repository of YOLO-Light is available on GitHub at https://github.com/BruceShine/YOLO-Light.},
  archive      = {J_TEVC},
  author       = {Zhenhao Shuai and Tao Yu and Bo Zhang and Chufan Ren and Liansheng Wang and Xiaoming Jiang and Linjin Li and Jianwei Shuai},
  doi          = {10.1109/TEVC.2025.3617095},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {YOLO-light: Automatic lightweight you-only-look-once generation in different scenarios through NeuroEvolution},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
