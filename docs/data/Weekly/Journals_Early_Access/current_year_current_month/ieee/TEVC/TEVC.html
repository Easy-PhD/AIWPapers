<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TEVC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tevc">TEVC - 12</h2>
<ul>
<li><details>
<summary>
(2025). Competitive multitasking sampling with deep reinforcement learning for surrogate-assisted constrained multiobjective optimization. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3605406'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When solving expensive constrained multiobjective optimization problems by surrogate-assisted evolutionary algorithms, the evolutionary sampling method is utilized to select valuable candidate solutions for expensive evaluations, therefore playing a vital role. Most existing sampling methods always prioritize feasible solutions over infeasible solutions during the whole evolutionary process. However, sampling infeasible solutions in some cases can not only provide search directions but also enhance diversity. To tackle this issue, a novel competitive multitasking sampling-based framework is proposed. Specifically, three complementary sampling subtasks are constructed: constraint-prioritized subtask, objective-constraint equivalence subtask, and objective-prioritized subtask. The main task is to find the constrained Pareto front. Different sampling subtasks exhibit different performances on various expensive problems. During each iteration, one of the sampling subtasks is selected by hierarchical deep reinforcement learning technique. This technique can effectively learn the relationship between the environmental states and the performances of different subtasks. Based on the selected subtask, the candidate points are generated via surrogate-assisted evolution. Subsequently, after these points undergo expensive evaluations, they can be regarded as knowledge and transferred to both the subtasks and the main task. Experimental results on three benchmark test suites and a real-world application have demonstrated the framework’s superiority over other state-of-the-art competitors.},
  archive      = {J_TEVC},
  author       = {Yuhang Ma and Bo Shen and Anqi Pan},
  doi          = {10.1109/TEVC.2025.3605406},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Competitive multitasking sampling with deep reinforcement learning for surrogate-assisted constrained multiobjective optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spike-driven lightweight large language model with evolutionary computation. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3606613'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, but their deployment in resource-constrained environments remains challenging due to substantial memory and computational requirements. Benefiting from the sparse event-driven computation paradigm of Spiking Neural Networks (SNNs), some research has focused on designing spike-based language models. However, existing spike-based language models achieve only partial computational efficiency gains and fail to address memory constraints comprehensively. In this paper, we propose an evolved and quantized spike-driven language model (EQ-SpikeLM) to address identified challenges. This model incorporates two primary innovations. First, inspired by the artificial bee colony algorithm in evolutionary computation, we propose an architecture evolution method, namely ABC-Arc. This method optimizes network topology by systematically removing redundant neural pathways. Second, a dynamic post-training quantization (DynPTQ) strategy is developed for the evolved SpikeLM, facilitating the conversion of floating-point parameters to lower-bit precision without requiring model retraining. By combining these two methods, EQ-SpikeLM significantly reduces storage and computational demands while preserving model performance. Experimental evaluation on the GLUE benchmark demonstrates EQ-SpikeLM’s ability to maintain performance equivalent to its uncompressed counterpart, with a substantial reduction in both model size and power consumption. These results position EQ-SpikeLM as a viable approach for deploying large language models in resource-constrained edge computing scenarios.},
  archive      = {J_TEVC},
  author       = {Malu Zhang and Wenjie Wei and Zijian Zhou and Wanlong Liu and Jie Zhang and Ammar Belatreche and Yang Yang},
  doi          = {10.1109/TEVC.2025.3606613},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Spike-driven lightweight large language model with evolutionary computation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Nanorobots-assisted in vivo computational methodology for tumor lesion exploitation and lesion boundary exploration. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3608196'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An innovative in vivo computational model that reframes the challenge of early tumor detection as an optimization problem has been developed recently. In this scope, the tumor lesion is viewed as the optimal solution that to be found by the computing agents (i.e., nanorobots). The tumor-triggered biological gradient field provides the information for the real-time path planning and fitness evaluation of nanorobots. The main emphasis of previous work has been on engineering tumor-targeting methodologies that address the challenges imposed by in vivo biological conditions. In this paper, we investigate the issue of tumor lesion exploitation by controlling the motion of nanorobotic swarm, which aims to enhance the operational efficiency of nanorobots deployed for tumor lesion identification. Thus, an exploitation approach about the settings of nanorobots moving direction and step size is developed. Furthermore, inspired by the spontaneous motion property of nanorobots, we propose cooperation and competition mechanisms to overcome the challenge of lesion boundary exploration in this paper. Some computational simulations are performed to validate the efficacy of this novel methodology.},
  archive      = {J_TEVC},
  author       = {Shaolong Shi and Zeyuan Xu and Jiawei Zou and Weiqi Liu and Qiujun Huang},
  doi          = {10.1109/TEVC.2025.3608196},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Nanorobots-assisted in vivo computational methodology for tumor lesion exploitation and lesion boundary exploration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Language model evolutionary algorithms for recommender systems: Benchmarks and algorithm comparisons. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3609058'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolutionary computing community, the remarkable language-handling capabilities and reasoning power of large language models (LLMs) have significantly enhanced the functionality of evolutionary algorithms (EAs), enabling them to tackle optimization problems involving structured language or program code. Although this field is still in its early stages, its impressive potential has led to the development of various LLM-based EAs. To effectively evaluate the performance and practical applicability of these LLM-based EAs, benchmarks with real-world relevance are essential. In this paper, we focus on LLM-based recommender systems (RSs) and introduce a benchmark problem set, named RSBench, specifically designed to assess the performance of LLM-based EAs in recommendation prompt optimization. RSBench emphasizes session-based recommendations, aiming to discover a set of Pareto optimal prompts that guide the recommendation process, providing accurate, diverse, and fair recommendations. We develop three LLM-based EAs based on established EA frameworks and experimentally evaluate their performance using RSBench. Our study offers valuable insights into the application of EAs in LLM-based RSs. Additionally, we explore key components that may influence the overall performance of the RS, providing meaningful guidance for future research on the development of LLM-based EAs in RSs.},
  archive      = {J_TEVC},
  author       = {Jiao Liu and Zhu Sun and Shanshan Feng and Caishun Chen and Yew-Soon Ong},
  doi          = {10.1109/TEVC.2025.3609058},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Language model evolutionary algorithms for recommender systems: Benchmarks and algorithm comparisons},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An evolution strategy with adaptive fitness sharing for multimodal multiobjective optimization. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3609902'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unlike multiobjective optimization (MOO), multimodal multiobjective optimization (MMMOO) should approximate the entire Pareto set, even if a portion of it maps onto the entire Pareto front. This study introduces a novel evolution strategy with adaptive fitness sharing (AFS) for MMMOO. The method, called, AFS-MMMO-ES, calculates an overall fitness for each solution in the selection pool based on its rank-wise hypervolume contribution, Pareto rank, and niche count in the decision space. Since the optimal niche radius is problem-dependent, this study introduces a novel strategy for on-the-fly adaptation of the niche radius. Simulations on meticulously designed test problems are performed to confirm the efficacy and reliability of this strategy in learning the optimal niche radius, as well as its significant impact on enhancing robustness and performance. Furthermore, AFS-MMMO-ES can easily reflect the relative importance of decision space diversity based on the decision-maker’s preference, a practically important feature that has been overlooked in this research field. Finally, the performance of AFS-MMMO-ES is assessed and compared with several successful MMMOO methods on a widely accepted test suite for MMMOO. Comparisons of numerical results reveal the robustness and superiority of AFS-MMMO-ES over its competitors.},
  archive      = {J_TEVC},
  author       = {Ali Ahrari and Ruhul Sarker and Mike Preuss},
  doi          = {10.1109/TEVC.2025.3609902},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {An evolution strategy with adaptive fitness sharing for multimodal multiobjective optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A study on the dynamics and effectiveness of the deflate geometric semantic mutation. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3611226'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Geometric Semantic Genetic Programming (GSGP) is a variant of Genetic Programming (GP) that induces an error surface without local minima for supervised learning tasks. However, GSGP is limited by the fact that its operators produce increasingly large individuals, leading to overly complex models. The slim addresses this issue by introducing a deflate geometric semantic mutation capable of producing offspring smaller than their parents. Preliminary studies show that slim can maintain accuracy comparable to traditional GSGP while generating much smaller models. However, a thorough analysis of this mutation remains lacking. This work fills that gap by conducting a detailed study of the deflate mutation, focusing on its behavior and practical value. Our results show that, when applied at the right stage of evolution, deflate mutation mitigates overfitting and yields compact, accurate models. This is also the first study to explore the timing and interaction of inflate and deflate mutations in slim, demonstrating how deflation enhances generalization and reduces overfitting. We support our conclusions with a comprehensive experimental approach, including comparisons between exclusive use of inflate mutation and alternating it with deflation. We also evaluate numerical indicators such as improvement rate and training effectiveness. The consistency across these methods reinforces our findings and highlights the deflate mutation as a robust regularization strategy. Finally, when compared to established non-evolutionary machine learning methods, SLIM shows competitive performance. Overall, this study confirms SLIM as a promising direction for GP and lays the foundation for future research.},
  archive      = {J_TEVC},
  author       = {Davide Farinati and Gloria Pietropolli and Leonardo Vanneschi},
  doi          = {10.1109/TEVC.2025.3611226},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {A study on the dynamics and effectiveness of the deflate geometric semantic mutation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A generative adversarial network based prediction strategy for evolutionary dynamic multiobjective optimization. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3611014'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, dynamic multi-objective evolutionary optimization has attracted much research attention, and the prediction based strategy has been proved to be an effective way for dynamic multi-objective optimization evolutionary algorithms (DMOEAs) to track the changing Pareto set (PS). Recently, some strategies have been proposed to use the information from both the historical and new environments to predict the population in the new environment. However, their performance in complex new environments is limited, as they partially utilize information from the new environment. Generative adversarial networks (GANs) have been proven to be an efficient generative model, which can capture the relationships between high-quality solutions in the new environment and estimate the data distribution more accurately. In this paper, we propose a GAN based prediction strategy (GANPS) to generate more high-quality solutions that can adapt to the new environment. In GANPS, a PS estimation method is employed to improve the quality of samples (solutions) for GAN training at first, which helps the GAN better extract the information from historical and new environment. Then GANPS utilizes the trained generator to re-initialize the population, and a model reuse strategy is further designed to reduce training costs and the algorithm’s response time. Experimental results on a set of benchmark test suites show that GANPS could outperform other state-of-the-art reaction strategies in most cases and has better change tracking capability in dynamic environments.},
  archive      = {J_TEVC},
  author       = {Feng Wang and Jinsong Xie and Fanshu Liao and Yixuan Li and Yinan Guo and Shengxiang Yang and Aimin Zhou},
  doi          = {10.1109/TEVC.2025.3611014},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {A generative adversarial network based prediction strategy for evolutionary dynamic multiobjective optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discovering infinite recursive conjectures through genetic programming. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3611312'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mathematics is filled with conjectures that involve infinite recursive structure, representing complex structures and underlying deep relationships. Discovering such conjectures is crucial for advancing our understanding of fundamental mathematical principles, as they reveal unexpected patterns and connections across different areas of mathematics. Due to their inherent complexity and infinite nature, these conjectures are challenging to uncover using traditional methods such as manual derivation and numerical calculations. A core task in studying such conjectures is identifying recursive relationships that describe potentially unknown patterns and structures. This task can be framed as a symbolic regression problem, as it involves searching for a suitable mathematical form to represent complex relationships. To address this symbolic regression problem, we propose a gene programming-based algorithm named infinite conjecture explorer (ICE) with a dual-chromosome encoding (DCE) and a two-sided matching operator (TMO). DCE encodes the two sides of a conjectured as separate two chromosomes, providing a clear representation of the underlying structure of an equation. Unlike other encoding methods, DCE improves the efficiency of discovering meaningful conjectures. Since the use of DCE results in two corresponding large and complex search spaces, TMO is designed to efficiently identify expressions that match on both sides of the equation across two spaces. The experimental results show that the proposed ICE is effective in generating promising conjectures with diverse forms.},
  archive      = {J_TEVC},
  author       = {Min-Yi Zheng and Yueheng Wang and Jinghui Zhong and Jun Zhang},
  doi          = {10.1109/TEVC.2025.3611312},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Discovering infinite recursive conjectures through genetic programming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MORA-LLM: Enhancing multi-objective optimization recommendation algorithm by integrating large language models. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3612284'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-objective evolutionary algorithms (MOEAs) have achieved notable success in recommendation systems (RSs) by meeting diverse user needs. However, existing MOEAs lack effective methods to coordinate the challenges of cold start, low convergence of multiple objectives and lack of explainable recommendation reasons. Therefore, we propose an enhancing multi-objective optimization recommendation algorithm by integrating large language models (named as MORA-LLM). MORA-LLM uses the vast knowledge, reasoning ability and natural language generation (NLG) ability of large language models (LLMs) to compensate for the shortcomings of MOEA-based RSs in semantic understanding. Firstly, an LLM-enhancing prediction score (LEPS) strategy is proposed to alleviate the cold start problem. LEPS obtains the user embedding vector by vast knowledge of LLM and extracts interaction information of similar users to improve the accuracy of prediction scores. Secondly, an LLM-enhancing search (LES) strategy is proposed to improve the convergence of the multi-objective. LES strategy combines the reasoning ability of LLM with the competitive idea of competitive swarm optimization to achieve efficient search and balance multiple objectives. Finally, the prediction scores are further corrected based on the LLM output results and MORA-LLM offers recommendation reasons to help users better understand the recommendation results. Experimental results on real-world datasets demonstrate that MORA-LLM significantly outperforms existing algorithms in terms of recommendation accuracy and convergence.},
  archive      = {J_TEVC},
  author       = {Yuanyuan Ge and Likang Wu and Haipeng Yang and Fan Cheng and Hongke Zhao and Lei Zhang},
  doi          = {10.1109/TEVC.2025.3612284},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {MORA-LLM: Enhancing multi-objective optimization recommendation algorithm by integrating large language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controlling functional complexity for overfitting reduction and improved interpretability in GP. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3614086'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Like other machine learning methods, Genetic Programming (GP) frequently faces the issue of overfitting when applied to supervised learning tasks. Traditional regularization techniques, though well-studied, are challenging to apply to GP due to the free-form nature of the evolved models. This work proposes a novel approach that prevents overfitting while inherently improving the interpretability of GP models. It involves a dual optimization process that minimizes loss while penalizing functional complexity using multi-objective selection mechanisms. The improved complexity measure used in this study approximates the mathematical curvature of a function in linear time. While loss minimization is common in GP, penalizing functional complexity is an additional step aimed at evolving robust and smooth functions, less prone to overfitting and potentially more interpretable. Experimental results demonstrate the effectiveness of the two variants of our method, benchmarked against standard GP and two of the seemingly best overfitting-reduction methods found in the literature. By focusing on both loss and complexity, our approach achieves state-of-the-art generalization on difficult problems and a strong feature selection that improves interpretability, making it a unified improvement of GP.},
  archive      = {J_TEVC},
  author       = {Sara Silva and Inês Magessi and Leonardo Vanneschi},
  doi          = {10.1109/TEVC.2025.3614086},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Controlling functional complexity for overfitting reduction and improved interpretability in GP},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). It’s morphing time: Unleashing the potential of multiple LLMs via multi-objective optimization. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3613937'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel approach for addressing the multi-objective optimization problem in large language model merging via black-box multi-objective optimization algorithms. The goal of model merging is to combine multiple models, each excelling in different tasks, into a single model that outperforms any of the individual source models. However, the effectiveness of conventional model merging methods is constrained by human intuition or domain knowledge. While existing optimization-based model merging methods can automatically search for model merging parameter configurations, they often struggle to find a satisfactory configuration within a limited evaluation budget. To address this challenge, we propose a novel and sample-efficient automated model merging method, named MM-MO. This method leverages multi-objective Bayesian optimization algorithms to autonomously search for great merging configurations across various tasks. In MMMO, we proposed an enhanced acquisition strategy and an auxiliary optimization objective to improve the search process. Our enhanced acquisition strategy integrates a weak-to-strong method to refine the acquisition function, enabling previously evaluated superior configurations to guide the search for new ones. Meanwhile, Fisher information is utilized to further filter these configurations, increasing the possibility of finding high-quality merging configurations. Additionally, we design a sparsity metric as an auxiliary optimization objective, further enhance the models generalization performance across different tasks. We conducted comprehensive experiments with other mainstream model merging methods, demonstrating that the proposed MMMO algorithm is competitive and effective in achieving high-quality model merging.},
  archive      = {J_TEVC},
  author       = {Bingdong Li and Zixiang Di and Yanting Yang and Hong Qian and Peng Yang and Hao Hao and Ke Tang and Aimin Zhou},
  doi          = {10.1109/TEVC.2025.3613937},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {It’s morphing time: Unleashing the potential of multiple LLMs via multi-objective optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient evolutionary neural architecture search with hierarchical parameter mapping for monocular depth estimation. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3614261'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular Depth Estimation (MDE) plays a crucial role in various real-world applications, including autonomous driving and augmented reality. However, automating the design of efficient MDE models via neural architecture search (NAS) remains challenging due to the high computational cost associated with (1) pretraining numerous candidate encoders on large-scale datasets such as ImageNet and (2) the substantial memory demands of high-resolution depth estimation models. To address these issues, this paper introduces PTF-EvoMDE, an evolutionary NAS framework that eliminates the need for per-candidate pretraining. PTF-EvoMDE incorporates a hierarchical parameter mapping (HPM) strategy that transfers weights from a single pretrained template network (MobileNetV2) to candidate architectures with varying depths, widths, and kernel sizes, significantly reducing computational overhead. Additionally, a feature-aligned conditional random fields (Fa-CRFs) decoder is proposed, leveraging deformable convolutions to dynamically align features and mitigate spatial misalignment, thereby enhancing depth prediction accuracy. Experiments on the KITTI benchmark demonstrate that PTF-EvoMDE achieves an absolute relative error (Abs Rel) of 0.061 and a root mean squared error (RMSE) of 2.470, comparable to Dense Prediction Transformer (DPT) model (Abs Rel: 0.062, RMSE: 2.573) while requiring only 5% of the parameters. Moreover, PTF-EvoMDE reduces the computational cost of NAS by more than 75% compared to conventional evolutionary approaches that rely on per-candidate pretraining. The resulting lightweight encoder exhibits strong transferability, achieving competitive performance on COCO object detection (42.1% mAP) and Cityscapes semantic segmentation (76.4% mIoU) with minimal fine-tuning.},
  archive      = {J_TEVC},
  author       = {Haoyu Zhang and Zhihao Yu and Yaochu Jin and Xiufeng Liu and Weiguo Sheng and Ruyu Liu and Xiumei Li and Qiqi Liu and Ran Cheng},
  doi          = {10.1109/TEVC.2025.3614261},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {9},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Efficient evolutionary neural architecture search with hierarchical parameter mapping for monocular depth estimation},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
