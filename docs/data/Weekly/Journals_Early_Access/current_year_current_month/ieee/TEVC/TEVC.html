<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TEVC</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tevc">TEVC - 8</h2>
<ul>
<li><details>
<summary>
(2025). Enhanced evolution of parallel algorithm portfolio for vehicle routing problem via transfer optimization. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3616385'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel Algorithm Portfolio (PAP), comprising several component solvers with complementary capabilities, emerges as a cutting-edge computational technique for addressing computationally hard problems. Automatic construction of PAPs can develop high-performance PAPs without human intervention. It is natural to believe that the component solvers share common useful building blocks, thus knowledge transfer among them could be beneficial during the evolution. However, this has been neglected by existing studies. To fill this gap, we propose a transfer optimization framework for automatically co-evolving high-performance PAPs. Specifically, we develop a novel performance-oriented dynamic instance grouping strategy to divide problem instances into groups, each of which is associated with a subpopulation of individuals tasked with evolving a component solver. Additionally, the framework incorporates an adaptive knowledge transfer strategy that automatically identifies when and how to transfer knowledge among instance groups. We conducted extensive experiments on the well-known Vehicle Routing Problem (VRP), a famously challenging NP-hard combinatorial optimization problem. The comprehensive experimental results from three public benchmarks demonstrate that our proposed framework significantly outperforms existing state-of-the-art VRP solvers and automatic PAP construction methods.},
  archive      = {J_TEVC},
  author       = {Tong Guo and Yi Mei and Mengjie Zhang and Ke Tang and Kaiquan Cai and Wenbo Du},
  doi          = {10.1109/TEVC.2025.3616385},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Enhanced evolution of parallel algorithm portfolio for vehicle routing problem via transfer optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta-learning inspired single-step generative model for expensive multitask optimization problems. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3617343'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In expensive multitask optimization problems (ExMTOPs), multiple complex tasks must be optimized simultaneously under limited computational budgets. Existing approaches, often based on surrogate models, aim to approximate objective functions but struggle to generalize across heterogeneous tasks, depend on task-specific sampling, and require frequent retraining. To address these challenges, we propose the Multifactorial Evolutionary Algorithm–Single Step Generative Model (MFEA-SSG), a meta-learning-inspired framework that learns to generate high-quality solutions across tasks. Inspired by meta-learning, we treat each random shuffle of the decision variables as a unique pseudo-task, training the model on a distribution of these tasks to learn a task-agnostic prior about the structure of elite solutions. This process disrupts task-specific dependencies, allowing the model to learn transferable structures from recomposed samples. We then adopt a diffusion-based generative model to learn the distribution of optimal solutions, enabling knowledge transfer across tasks without directly approximating objective functions. To reduce inference cost, we introduce a student model distilled from the diffusion process. Unlike conventional diffusion models that denoise iteratively, the student generates solutions in a single forward pass, significantly reducing inference time. Comprehensive experiments on both general multitask benchmarks and a real-world protein mutation prediction scenario demonstrate that MFEA-SSG achieves high-quality solutions with fast convergence and low computational cost under limited evaluation budgets, outperforming state-of-the-art general and ExMTOPs algorithms.},
  archive      = {J_TEVC},
  author       = {Ruilin Wang and Xiang Feng and Huiqun Yu and Yang Tan and Edmund M-K Lai},
  doi          = {10.1109/TEVC.2025.3617343},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Meta-learning inspired single-step generative model for expensive multitask optimization problems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-light: Automatic lightweight you-only-look-once generation in different scenarios through NeuroEvolution. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3617095'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {You-Only-Look-Once (YOLO) represents the state-of-the-art in object detection models. With the emergence of various applications utilizing small domain-specific datasets and limited computing resources for extensive model training and deployment, there is an increasing demand for customized lightweight YOLO architectures. In this paper, we propose a general NeuroEvolution-based method, termed YOLO-Light, designed to automatically create lightweight variants of YOLO architectures tailored to object detection tasks across diverse scenarios. For a given task, YOLO-Light first initializes a population of minimal YOLO architectures and subsequently evolves these models within a novel parallel-chain evolutionary space. This process employs a diversity-protecting evolutionary search strategy until some architectures meet the expected performance standards. During evolution, YOLO-Light incorporates a dynamic evolution regulation mechanism to adjust the evolutionary configuration, thereby enhancing efficiency based on the current evolutionary state. We applied YOLO-Light to generate lightweight YOLOv5, YOLOv8, and YOLOv10 architectures for object detection on the Roboflow 100 small dataset collection, which comprises 100 diverse datasets spanning 7 distinct imagery domains, with a total of 224,714 images and 829 classes. Our experiments focused on 20 datasets ranging from 105 to 8,992 images and 1 to 53 classes. The experimental results show that YOLO-Light reduced the number of parameters by 54–95%, while maintaining or improving mean Average Precision (mAP) compared to standard YOLO architectures. These results demonstrate the effectiveness of YOLO-Light in generating lightweight, task-specific YOLO architectures for resource-constrained object detection tasks. The code repository of YOLO-Light is available on GitHub at https://github.com/BruceShine/YOLO-Light.},
  archive      = {J_TEVC},
  author       = {Zhenhao Shuai and Tao Yu and Bo Zhang and Chufan Ren and Liansheng Wang and Xiaoming Jiang and Linjin Li and Jianwei Shuai},
  doi          = {10.1109/TEVC.2025.3617095},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {YOLO-light: Automatic lightweight you-only-look-once generation in different scenarios through NeuroEvolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A size-imbalanced dual population with complementary search for sparse large-scale multi-modal multi-objective optimization. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3618953'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal multi-objective optimization problems (MMOPs), which contain multiple distinct Pareto sets, are found in many real-world applications. Their multi-modality characteristics require algorithms to be specifically designed to enhance the diversity in the decision space. However, existing methods often struggle to achieve either desired diversity or satisfactory convergence speed, particularly in large-scale scenarios. To address this issue, we propose an evolutionary framework that employs a size-imbalanced dual population to balance the convergence and diversity in the decision space. Specifically, a large main population, guided by non-zero mask vectors, searches promising regions of the decision space for fast convergence. On the other hand, a small complementary population is guided by a mask vector that is bitwise complementary to those of the main population, enabling it to focus on the unexplored areas and ensure comprehensive coverage of the search space. Periodically, the two populations and their corresponding mask vectors are updated by inter-and intra-population information exchanges, allowing them to collaboratively adjust their search directions and effectively address multi-modality challenges. Additionally, to further enhance the search efficiency, a clustering-enhanced genetic operator is designed to update different populations by adaptively applying either exploration or exploitation search phases. Last, we introduce a new evaluation metric, termed the multi-objective multi-modal index, to evaluate the multi-modality performance of algorithms in real-world MMOPs without reliance on groundtruth data. Experimental results show that the proposed method significantly outperforms five state-of-the-art algorithms on (non-) multi-modal benchmarks and real-world applications with substantially reduced computational time. (The source code is available at https://github.com/XWang1216/MMEA-IDC.)},
  archive      = {J_TEVC},
  author       = {Xiangyu Wang and Yaochu Jin},
  doi          = {10.1109/TEVC.2025.3618953},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {A size-imbalanced dual population with complementary search for sparse large-scale multi-modal multi-objective optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fitness spatially informed evolutionary algorithm for deceptive multi-modal multi-objective optimization. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3618608'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evolutionary Algorithms (EAs) are effective for solving Multi-Modal Multi-Objective Optimization Problems which optimal solutions subsets distributed regularly in the decision space. Existing EAs, however, are hard to achieve good diversity in the decision space due to their lack of thinking-in-space when solving deceptive multi-modal multi-objective problems, and one manifestation of the deceptiveness is that global optima are composed of unconnected small optimal solution subsets that randomly distributed in the decision space with many local optima subsets mixed among. In this study, for enabling EAs to continuously search without trapping, a solving pathway is proposed that identifies information-rich historical data and constructs structured spatial representation based on high-quality evolutionary data. Furthermore, a moderate region-oriented solution set update methodology is proposed for the detection of ultra-small optimal modalities, and two novel environmental selection and archive update mechanisms are designed. A Memory-aiding Search Multi-Modal Multi-Objective Optimization EA, i.e., MS-MMOEA, is instantiated. The experimental results on 27 deceptive instances and a real-world problem demonstrate MS-MMOEA’s competitiveness over the state-of-the-art.},
  archive      = {J_TEVC},
  author       = {Yaqi Ti and Changhe Li and Xinye Cai and Shengxiang Yang},
  doi          = {10.1109/TEVC.2025.3618608},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {A fitness spatially informed evolutionary algorithm for deceptive multi-modal multi-objective optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lamarckian inheritance improves robot evolution in dynamic environments. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3619278'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nature-inspired methods, such as evolutionary computing and lifetime learning, have shown great promise in advancing autonomous robot design. However, the integration of evolution and learning for the joint optimization of robot morphologies and controllers remains underexplored, particularly in dynamic environments. This paper addresses this gap by investigating the effectiveness of Lamarckian inheritance (a mechanism that allows learned traits to be encoded into the genotype and passed to offspring) in improving robot evolution in nonstationary environments. We compare a Lamarckian system with a traditional Darwinian system, where learned traits are not inherited. Using simulated modular robots in six distinct environmental setups, we analyze the fitness progression, learning ability, and parent-offspring similarity within both systems. Our results demonstrate that the Lamarckian system consistently outperforms the Darwinian system, achieving up to 33% higher fitness in the most challenging conditions. The Lamarckian system also recovers more quickly from environmental changes, showing immediate fitness gains when shifting to complex terrains, whereas the Darwinian system adapts more slowly. Real-world tests validate the robustness of the Lamarckian approach, as the top-performing robots evolved in the most challenging environment exhibit the smallest reality gap. These findings highlight the potential of Lamarckian inheritance as a powerful tool for engineering adaptive robotic systems capable of maintaining high performance in dynamic environments.},
  archive      = {J_TEVC},
  author       = {Jie Luo and Karine Miras and Carlo Longhi and Oliver Weissl and Agoston E. Eiben},
  doi          = {10.1109/TEVC.2025.3619278},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Lamarckian inheritance improves robot evolution in dynamic environments},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional-surrogate-assisted particle swarm optimization for large-scale robust optimization over time. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3619090'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In dynamic environments, optimization algorithms tend to track the changing optima. However, frequent changes to decision solutions often cause high switching costs and system instability. Decision solutions are expected to be robust to changes. Such problems bring new challenges to existing algorithms, i.e., solution evaluation in future environments and solution selection. Though multiple surrogate models are developed for fitness evaluation, they mainly focus on predicting absolute fitness in static environments. Thus, this paper proposes conditional-surrogate-assisted particle swarm optimization (CSPSO), which adopts a conditional surrogate model to predict the relative fitness of solutions in future for robust solution selection. In CSPSO, a problem is decomposed into multiple subproblems, which are optimized by multiple swarms. Searching data of swarms is collected to learn network models for predicting the future fitness of solutions. In order to coordinate with the multi-modal, decoupled, and dynamic property of the problem, networks are conditioned on some extra information, i.e., time, subproblems, and peaks. Particularly, neural networks are first trained to predict new optimal solutions and fitness conditioned on time and subproblems, and a surrogate model is then trained to predict the relative fitness of solutions conditioned on the closest peak, corresponding subproblem, and time. By combining the current fitness with future ones, solutions with the highest fitness are selected for decision making. Experimental results show that the proposed algorithm outperforms state-of-the-art algorithms on problem instances up to 1000-D in terms of solution optimality. The proposed conditional surrogate model can well predict future fitness to assist decision making.},
  archive      = {J_TEVC},
  author       = {Xiao-Fang Liu and Tian-Hong Wang and Zhi-Hui Zhan and Jun Zhang},
  doi          = {10.1109/TEVC.2025.3619090},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Conditional-surrogate-assisted particle swarm optimization for large-scale robust optimization over time},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multitree genetic programming for multimodal learning in multimodal medical image classification. <em>TEVC</em>, 1. (<a href='https://doi.org/10.1109/TEVC.2025.3619532'>www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As medical images from multiple modalities provide complementary diagnostic information, multimodal medical image classification (MMIC) leverages their integration to enhance disease diagnosis. Genetic programming (GP), especially multitree GP, offers a flexible framework for evolving multimodal features and holds the potential for interpretability—an essential factor in medical image analysis, where it is crucial to understand the underlying decision-making process. However, current multitree GP methods remain underexplored in MMIC tasks. This article attempts to fill that gap by proposing a novel multitree representation method that enables automatic multimodal feature learning and fusion from different modalities. Based on this new representation, we extend the function and terminal sets to support various types of medical imaging modalities. Additionally, a slice selection factor is proposed to process 3D medical image modalities effectively. Moreover, to fully utilize both modality-specific features and fused multimodal features, fusion is performed at both the feature and decision levels. The accuracy of the proposed method is assessed on eight MMIC tasks, representing different medical scenarios. The new method is compared with 15 competitive algorithms. Experimental results demonstrate that the new method achieves better classification performance than most benchmark methods. Specifically, it achieves the highest accuracy on four out of the eight tasks, GAMMA-Binary, GAMMA-Multi, BraTS2018 and Derm7pt, with improvements of 3.38%, 2.31%, 1.36% and 4.11% respectively over the second-best method. Further analysis reveals that the evolved models show potential for interpretability, owing to their ability to effectively capture the distinctive features of each modality, which may aid in biomarker identification.},
  archive      = {J_TEVC},
  author       = {Zhicheng Wu and Bing Xue and Mengjie Zhang},
  doi          = {10.1109/TEVC.2025.3619532},
  journal      = {IEEE Transactions on Evolutionary Computation},
  month        = {10},
  pages        = {1},
  shortjournal = {IEEE Trans. Evol. Comput.},
  title        = {Multitree genetic programming for multimodal learning in multimodal medical image classification},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
